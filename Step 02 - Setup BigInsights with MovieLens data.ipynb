{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "In this notebook, the cluster is loaded with the movielens ml-1m dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the cluster connection information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets get our previously saved credentials for the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('credentials', 'r') as f:\n",
    "    (hostname, username, password) = f.readline().split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ssh library for running commands on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now setup a python ssh library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade --force --quiet git+https://github.com/snowch/nb_utils\n",
    "\n",
    "# note to install a specific commit sha\n",
    "# !pip install --user --upgrade --force --quiet git+https://github.com/snowch/nb_utils@commit_sha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print version of installed nb_utils for traceability purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'04d1ae56e6093a2ac52818661bd8679df2a36c9b'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "requests.get('https://api.github.com/repos/snowch/nb_utils/git/refs/heads/master').json()['object']['sha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the ssh utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ssh_utils import ssh_utils\n",
    "ssh = ssh_utils.SshUtil(hostname, username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the ml-1m dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat\n",
      "  inflating: ml-1m/ratings.dat\n",
      "  inflating: ml-1m/README\n",
      "  inflating: ml-1m/users.dat\n"
     ]
    }
   ],
   "source": [
    "# make sure we don't have any data hanging around from previous runs\n",
    "ssh.cmd('rm -rf ml-1m ml-1m.zip movies.dat users.dat ratings.dat')\n",
    "\n",
    "# retrieve the data to BigInsights local filesystem\n",
    "ssh.cmd('wget --quiet http://files.grouplens.org/datasets/movielens/ml-1m.zip')\n",
    "\n",
    "# unzip the data\n",
    "ssh.cmd('unzip ml-1m.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to WebHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ratings.dat\n"
     ]
    }
   ],
   "source": [
    "# make sure we don't have any data hanging around from previous runs\n",
    "ssh.cmd('''\n",
    "    hdfs dfs -rm -f -skipTrash ./ratings.dat\n",
    "    hdfs dfs -rm -r -f -skipTrash ./rating\n",
    "    hdfs dfs -rm -r -f -skipTrash ./recommender_model\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data from the BigInsights local file system to HDDS and verify that it was copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - demouser hdfs          0 2016-11-02 10:37 .sparkStaging\n",
      "-rw-r--r--   3 demouser hdfs   24594131 2016-11-02 15:08 ratings.dat\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('''\n",
    "    hdfs dfs -put ./ml-1m/ratings.dat ./ratings.dat\n",
    "    hdfs dfs -ls ./\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's remove the data we downloaded to the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssh.cmd('rm -rf ml-1m ml-1m.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}