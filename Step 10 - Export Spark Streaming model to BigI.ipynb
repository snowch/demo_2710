{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "version": "2.7.11", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2"}, "kernelspec": {"display_name": "Python 2 with Spark 1.6", "name": "python2", "language": "python"}}, "nbformat_minor": 0, "cells": [{"metadata": {}, "source": "## Overview\n\nThis is a work-in-progress ...", "cell_type": "markdown"}, {"metadata": {}, "source": "### Setup SSH\n\nRead cluster credentials", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "with open('credentials', 'r') as f:\n    (hostname, username, password) = f.readline().split(',')", "cell_type": "code", "execution_count": 1}, {"metadata": {}, "source": "Install required ssh and scp libraries", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "!pip install --user --upgrade --force --quiet git+https://github.com/snowch/nb_utils", "cell_type": "code", "execution_count": 2}, {"metadata": {}, "source": "print version of installed nb_utils for traceability purposes", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": "{u'object': {u'sha': u'04d1ae56e6093a2ac52818661bd8679df2a36c9b',\n  u'type': u'commit',\n  u'url': u'https://api.github.com/repos/snowch/nb_utils/git/commits/04d1ae56e6093a2ac52818661bd8679df2a36c9b'},\n u'ref': u'refs/heads/master',\n u'url': u'https://api.github.com/repos/snowch/nb_utils/git/refs/heads/master'}"}, "metadata": {}, "output_type": "execute_result", "execution_count": 3}], "source": "import requests, json\nrequests.get('https://api.github.com/repos/snowch/nb_utils/git/refs/heads/master').json()", "cell_type": "code", "execution_count": 3}, {"metadata": {}, "source": "Setup a utility method to make scp commands easier", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "from ssh_utils import ssh_utils\nssh = ssh_utils.SshUtil(hostname, username, password)", "cell_type": "code", "execution_count": 4}, {"metadata": {}, "source": "Let's verify ssh is working by listing the root folder contents of hdfs", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 9 items\ndrwxrwxr-x   - ams      hdfs             0 2016-11-02 03:03 /amshbase\ndrwxrwxrwx   - yarn     hadoop           0 2016-11-02 00:33 /app-logs\ndrwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /apps\ndrwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /iop\ndrwxr-xr-x   - mapred   hdfs             0 2016-07-05 07:05 /mapred\ndrwxrwxrwx   - mapred   hadoop           0 2016-07-05 07:05 /mr-history\ndrwx------   - demouser biusers          0 2016-11-02 00:30 /securedir\ndrwxrwxrwx   - hdfs     hdfs             0 2016-11-02 00:29 /tmp\ndrwxr-xr-x   - hdfs     hdfs             0 2016-11-02 00:29 /user\n"}], "source": "ssh.cmd('hdfs dfs -ls /')", "cell_type": "code", "execution_count": 5}, {"metadata": {}, "source": "The next command we are running on DSX.  We create a tar archive containing our model.", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": "!rm -f recommender_model.tgz\n!tar czf recommender_model.tgz recommender_model/", "cell_type": "code", "execution_count": 6}, {"metadata": {}, "source": "On BigInsights, use delete any models that were copied across to BigInsights on previous runs on the notebook.", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "ssh.cmd('rm -rf ./recommender_model.tgz ./recommender_model')", "cell_type": "code", "execution_count": 7}, {"metadata": {}, "source": "Copy over our new model", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "ssh.put('recommender_model.tgz')", "cell_type": "code", "execution_count": 8}, {"metadata": {}, "source": "Verify that it was copied ok", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-rw-r--r--. 1 demouser biusers 2129960 Nov  2 15:56 ./recommender_model.tgz\n"}], "source": "ssh.cmd('ls -l ./recommender_model.tgz')", "cell_type": "code", "execution_count": 9}, {"metadata": {}, "source": "Unzip the model archive", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": "ssh.cmd('tar xzf ./recommender_model.tgz')", "cell_type": "code", "execution_count": 10}, {"metadata": {}, "source": "Verify the unzipped model folders", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "./recommender_model\n./recommender_model/data\n./recommender_model/data/user\n./recommender_model/data/user/.part-r-00000-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet.crc\n./recommender_model/data/user/part-r-00001-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet\n./recommender_model/data/user/._SUCCESS.crc\n./recommender_model/data/user/part-r-00000-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet\n./recommender_model/data/user/_SUCCESS\n./recommender_model/data/user/_common_metadata\n./recommender_model/data/user/.part-r-00001-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet.crc\n./recommender_model/data/user/._common_metadata.crc\n./recommender_model/data/user/_metadata\n./recommender_model/data/user/._metadata.crc\n./recommender_model/data/product\n./recommender_model/data/product/part-r-00000-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet\n./recommender_model/data/product/.part-r-00000-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet.crc\n./recommender_model/data/product/._SUCCESS.crc\n./recommender_model/data/product/_SUCCESS\n./recommender_model/data/product/_common_metadata\n./recommender_model/data/product/.part-r-00001-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet.crc\n./recommender_model/data/product/part-r-00001-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet\n./recommender_model/data/product/._common_metadata.crc\n./recommender_model/data/product/_metadata\n./recommender_model/data/product/._metadata.crc\n./recommender_model/metadata\n./recommender_model/metadata/._SUCCESS.crc\n./recommender_model/metadata/.part-00000.crc\n./recommender_model/metadata/_SUCCESS\n./recommender_model/metadata/part-00000\n"}], "source": "ssh.cmd('find ./recommender_model')", "cell_type": "code", "execution_count": 11}, {"metadata": {}, "source": "Remove any models that were copied to BigInsights HDFS on previous runs on the notebook. And then copy the model from the BigInsights local file system to HDFS.", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": "model_path = 'hdfs:///user/{0}/recommender_model'.format(username)", "cell_type": "code", "execution_count": 12}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "hdfs:///user/demouser/recommender_model\n"}], "source": "print(model_path)", "cell_type": "code", "execution_count": 51}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted hdfs:///user/demouser/recommender_model\n"}], "source": "ssh.cmd('hdfs dfs -rm -r -skipTrash {0}'.format(model_path)) # it's ok if this fails\nssh.cmd('hdfs dfs -copyFromLocal ./recommender_model {0}'.format(model_path))", "cell_type": "code", "execution_count": 52}, {"metadata": {}, "source": "Verify the model exists in HDFS", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\ndrwxr-xr-x   - demouser hdfs          0 2016-11-02 15:56 hdfs:///user/demouser/recommender_model/data\ndrwxr-xr-x   - demouser hdfs          0 2016-11-02 15:56 hdfs:///user/demouser/recommender_model/metadata\n"}], "source": "ssh.cmd('hdfs dfs -ls {0}'.format(model_path))", "cell_type": "code", "execution_count": 15}, {"metadata": {}, "source": "Copy a scala spark class to the cluster for doing the predictions<br/>\nSee https://github.com/snowch/demo_2710/blob/master/scala_streaming_predictor/src/main/scala/MovieRating.scala", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-rw-r--r--. 1 demouser biusers 9432739 Nov  2 16:06 movie-rating.jar\n"}], "source": "ssh.cmd('rm -f movie-rating.jar')\n\n# note we are now using the scala_streaming_predictor project\nssh.cmd('wget -q -O movie-rating.jar https://github.com/snowch/demo_2710/blob/master/scala_streaming_predictor/movie-rating_2.10-1.0.jar?raw=true')\nssh.cmd('ls -l movie-rating.jar')", "cell_type": "code", "execution_count": 26}, {"metadata": {}, "source": "The user_id and movie_id we want predictions for", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "ssh.put(\"messagehub.properties\")\n\nssh.cmd(\"\"\"\n    # spark requires properties to be prefixed with 'spark.'\n    sed -i -e 's/^/spark./' messagehub.properties\n    \n    # spark requires property name value pairs to be separated by a space\n    sed -i -e 's/=/ /' messagehub.properties\n    \n    # add the model path to the properties\n    echo \"\\nspark.model_path {0}\" >> messagehub.properties\n    \n    # rename the properties file inline with spark conventions\n    mv messagehub.properties messagehub.conf\n    \n    # add the cluster's spark settings to the configuration\n    cat /usr/iop/current/spark-client/conf/spark-defaults.conf >> messagehub.conf\n    \n\"\"\".format(model_path)\n)\n\n# uncomment to debug\n# ssh.cmd(\"cat messagehub.conf\")", "cell_type": "code", "execution_count": 72}, {"metadata": {}, "source": "First check we don't have any existing spark submit jobs running", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):0\n                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n16/11/02 16:34:01 INFO impl.TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n16/11/02 16:34:01 INFO client.RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4194.bi.services.us-south.bluemix.net/172.16.237.1:8050\n"}], "source": "ssh.cmd('yarn application -list')", "cell_type": "code", "execution_count": 73}, {"metadata": {}, "source": "If we have some existing yarn jobs for 'Movie Ratings', kill them", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [], "source": "# ssh.cmd('yarn application -kill application_1478046523919_0017')", "cell_type": "code", "execution_count": 74}, {"metadata": {}, "source": "Execute the spark class", "cell_type": "markdown"}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "application_1478046523919_0026\t         MovieRating\t               SPARK\t  demouser\t   default\t          ACCEPTED\t         UNDEFINED\t             0%\t                                N/A\n16/11/02 16:34:11 INFO impl.TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n16/11/02 16:34:12 INFO client.RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4194.bi.services.us-south.bluemix.net/172.16.237.1:8050\n"}], "source": "ssh.cmd('spark-submit --class \"MovieRating\" --master yarn-cluster --properties-file messagehub.conf ./movie-rating.jar > /dev/null 2>&1 &')\n\n# get the currently running yarn applications\nssh.cmd('sleep 5 && yarn application -list | grep \"^application_\"')", "cell_type": "code", "execution_count": 75}, {"metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):1\n                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\napplication_1478046523919_0026\t         MovieRating\t               SPARK\t  demouser\t   default\t           RUNNING\t         UNDEFINED\t            10%\t          http://172.16.237.2:35227\n16/11/02 16:34:29 INFO impl.TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n16/11/02 16:34:29 INFO client.RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4194.bi.services.us-south.bluemix.net/172.16.237.1:8050\n/app-logs/demouser/logs/application_1478046523919_0026 does not have any log files.\n16/11/02 16:34:33 INFO impl.TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n16/11/02 16:34:33 INFO client.RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4194.bi.services.us-south.bluemix.net/172.16.237.1:8050\n"}], "source": "ssh.cmd('yarn application -list')\nssh.cmd('yarn logs -applicationId application_1478046523919_0026')", "cell_type": "code", "execution_count": 76}, {"metadata": {"scrolled": true, "collapsed": false}, "outputs": [], "source": "# some commands for debugging\n\n# ssh.cmd('yarn application -list -appStates ALL')\n\n# get the logs if there is an issue\n# ssh.cmd('yarn logs -applicationId application_1478046523919_0025')\n\n# kill the application\n# ssh.cmd('yarn application -kill applicationId')", "cell_type": "code", "execution_count": 78}, {"metadata": {}, "source": "After running the code above and seeing your application running (Application-Name = MovieRating),\n\n 0. ensure the notebook kernel is stopped for step 09\n 1. open another window with the notebook for step 08 \n 2. some messages to MessageHub: **STEP 08 (A) - Produce Prediction Requests**\n 3. consume the responses: **STEP 08 (B) - Consume Prediction Responses** ", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": "", "cell_type": "code", "execution_count": null}], "nbformat": 4}