{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we look at how the movie recommendation model that was built on DSX can be exported to BigInsights.\n",
    "We have a lot of flexibility on BigInsights for using the model, e.g.\n",
    "\n",
    "- from Oozie to make batch predictions\n",
    "- from Spark Streaming to make realtime predictions\n",
    "\n",
    "We will use ssh and scp python libraries to move data and code between BigInsights and DSX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup SSH\n",
    "\n",
    "Read cluster credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('credentials', 'r') as f:\n",
    "    (hostname, username, password) = f.readline().split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required ssh and scp libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade --force --quiet git+https://github.com/snowch/nb_utils\n",
    "    \n",
    "# note to install a specific commit sha\n",
    "# !pip install --user --upgrade --force --quiet git+https://github.com/snowch/nb_utils@commit_sha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print version (commit sha) of installed nb_utils for traceability purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'04d1ae56e6093a2ac52818661bd8679df2a36c9b'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('https://api.github.com/repos/snowch/nb_utils/git/refs/heads/master').json()['object']['sha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the ssh utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ssh_utils import ssh_utils\n",
    "ssh = ssh_utils.SshUtil(hostname, username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify ssh is working by listing the root folder contents of hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "drwxrwxr-x   - ams      hdfs             0 2016-11-02 03:03 /amshbase\n",
      "drwxrwxrwx   - yarn     hadoop           0 2016-11-02 00:33 /app-logs\n",
      "drwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /apps\n",
      "drwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /iop\n",
      "drwxr-xr-x   - mapred   hdfs             0 2016-07-05 07:05 /mapred\n",
      "drwxrwxrwx   - mapred   hadoop           0 2016-07-05 07:05 /mr-history\n",
      "drwx------   - demouser biusers          0 2016-11-02 00:30 /securedir\n",
      "drwxrwxrwx   - hdfs     hdfs             0 2016-11-02 00:29 /tmp\n",
      "drwxr-xr-x   - hdfs     hdfs             0 2016-11-02 00:29 /user\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('hdfs dfs -ls /')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command we are running on DSX.  We create a tar archive containing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -f recommender_model.tgz\n",
    "!tar czf recommender_model.tgz recommender_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On BigInsights, use delete any models that were copied across to BigInsights on previous runs on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssh.cmd('rm -rf ./recommender_model.tgz ./recommender_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy over our new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssh.put('recommender_model.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that it was copied ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 demouser biusers 2129960 Nov  2 15:28 ./recommender_model.tgz\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('ls -l ./recommender_model.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the model archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssh.cmd('tar xzf ./recommender_model.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the unzipped model folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./recommender_model\n",
      "./recommender_model/data\n",
      "./recommender_model/data/user\n",
      "./recommender_model/data/user/.part-r-00000-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet.crc\n",
      "./recommender_model/data/user/part-r-00001-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet\n",
      "./recommender_model/data/user/._SUCCESS.crc\n",
      "./recommender_model/data/user/part-r-00000-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet\n",
      "./recommender_model/data/user/_SUCCESS\n",
      "./recommender_model/data/user/_common_metadata\n",
      "./recommender_model/data/user/.part-r-00001-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet.crc\n",
      "./recommender_model/data/user/._common_metadata.crc\n",
      "./recommender_model/data/user/_metadata\n",
      "./recommender_model/data/user/._metadata.crc\n",
      "./recommender_model/data/product\n",
      "./recommender_model/data/product/part-r-00000-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet\n",
      "./recommender_model/data/product/.part-r-00000-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet.crc\n",
      "./recommender_model/data/product/._SUCCESS.crc\n",
      "./recommender_model/data/product/_SUCCESS\n",
      "./recommender_model/data/product/_common_metadata\n",
      "./recommender_model/data/product/.part-r-00001-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet.crc\n",
      "./recommender_model/data/product/part-r-00001-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet\n",
      "./recommender_model/data/product/._common_metadata.crc\n",
      "./recommender_model/data/product/_metadata\n",
      "./recommender_model/data/product/._metadata.crc\n",
      "./recommender_model/metadata\n",
      "./recommender_model/metadata/._SUCCESS.crc\n",
      "./recommender_model/metadata/.part-00000.crc\n",
      "./recommender_model/metadata/_SUCCESS\n",
      "./recommender_model/metadata/part-00000\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('find ./recommender_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any models that were copied to BigInsights HDFS on previous runs on the notebook. And then copy the model from the BigInsights local file system to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'hdfs:///user/{0}/recommender_model'.format(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs:///user/demouser/recommender_model\n"
     ]
    }
   ],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/demouser/recommender_model\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('hdfs dfs -rm -r -skipTrash {0}'.format(model_path)) # it's ok if this fails\n",
    "ssh.cmd('hdfs dfs -copyFromLocal ./recommender_model {0}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the model exists in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - demouser hdfs          0 2016-11-02 15:29 hdfs:///user/demouser/recommender_model/data\n",
      "drwxr-xr-x   - demouser hdfs          0 2016-11-02 15:29 hdfs:///user/demouser/recommender_model/metadata\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('hdfs dfs -ls {0}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy a scala spark class to the cluster for doing the predictions<br/>\n",
    "See https://github.com/snowch/demo_2710/blob/master/scala_predictor/MovieRating.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 demouser biusers 2472 Nov  2 15:29 movie-rating.jar\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('rm -f movie-rating.jar')\n",
    "ssh.cmd('wget -q -O movie-rating.jar https://github.com/snowch/demo_2710/blob/master/scala_predictor/movie-rating_2.10-1.0.jar?raw=true')\n",
    "ssh.cmd('ls -l movie-rating.jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user_id and movie_id we want predictions for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "movie_id = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the spark class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs:///user/demouser/rating\n"
     ]
    }
   ],
   "source": [
    "output_path = 'hdfs:///user/{0}/rating'.format(username)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/demouser/rating\n",
      "16/11/02 15:29:13 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n",
      "16/11/02 15:29:13 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n",
      "16/11/02 15:29:14 INFO TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n",
      "16/11/02 15:29:15 INFO RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4194.bi.services.us-south.bluemix.net/172.16.237.1:8050\n",
      "16/11/02 15:29:16 INFO Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "16/11/02 15:29:16 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "16/11/02 15:29:16 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "16/11/02 15:29:16 INFO Client: Setting up container launch context for our AM\n",
      "16/11/02 15:29:16 INFO Client: Setting up the launch environment for our AM container\n",
      "16/11/02 15:29:16 INFO Client: Preparing resources for our AM container\n",
      "16/11/02 15:29:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8020/iop/apps/4.2.0.0/spark/jars/spark-assembly.jar\n",
      "16/11/02 15:29:17 INFO Client: Uploading resource file:/home/demouser/movie-rating.jar -> hdfs://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8020/user/demouser/.sparkStaging/application_1478046523919_0019/movie-rating.jar\n",
      "16/11/02 15:29:18 INFO Client: Uploading resource file:/tmp/spark-f576c88d-5528-4b99-a00f-eb23a1870487/__spark_conf__1791630809560458183.zip -> hdfs://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8020/user/demouser/.sparkStaging/application_1478046523919_0019/__spark_conf__1791630809560458183.zip\n",
      "16/11/02 15:29:18 WARN Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode\n",
      "16/11/02 15:29:18 INFO SecurityManager: Changing view acls to: demouser\n",
      "16/11/02 15:29:18 INFO SecurityManager: Changing modify acls to: demouser\n",
      "16/11/02 15:29:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(demouser); users with modify permissions: Set(demouser)\n",
      "16/11/02 15:29:18 INFO Client: Submitting application 19 to ResourceManager\n",
      "16/11/02 15:29:18 INFO YarnClientImpl: Submitted application application_1478046523919_0019\n",
      "16/11/02 15:29:19 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n",
      "16/11/02 15:29:19 INFO Client:\n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1478100558571\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8088/proxy/application_1478046523919_0019/\n",
      "\t user: demouser\n",
      "16/11/02 15:29:20 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n",
      "16/11/02 15:29:21 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n",
      "16/11/02 15:29:22 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n",
      "16/11/02 15:29:23 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:23 INFO Client:\n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.237.2\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1478100558571\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8088/proxy/application_1478046523919_0019/\n",
      "\t user: demouser\n",
      "16/11/02 15:29:24 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:25 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:26 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:27 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:28 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:29 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:30 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:31 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:32 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:33 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:34 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:35 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:36 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:37 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:38 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:39 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:40 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:41 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n",
      "16/11/02 15:29:42 INFO Client: Application report for application_1478046523919_0019 (state: FINISHED)\n",
      "16/11/02 15:29:42 INFO Client:\n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.237.2\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1478100558571\n",
      "\t final status: SUCCEEDED\n",
      "\t tracking URL: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8088/proxy/application_1478046523919_0019/\n",
      "\t user: demouser\n",
      "16/11/02 15:29:42 INFO ShutdownHookManager: Shutdown hook called\n",
      "16/11/02 15:29:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-f576c88d-5528-4b99-a00f-eb23a1870487\n"
     ]
    }
   ],
   "source": [
    "# ensure the output folder is clean\n",
    "ssh.cmd('hdfs dfs -rm -f -r -skipTrash {0}'.format(output_path))\n",
    "ssh.cmd('spark-submit --class \"MovieRating\" --master yarn-cluster ./movie-rating.jar {0} {1} {2} {3}'.format(model_path, user_id, movie_id, output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify we have some ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 demouser hdfs          0 2016-11-02 15:29 hdfs:///user/demouser/rating/_SUCCESS\n",
      "-rw-r--r--   3 demouser hdfs         24 2016-11-02 15:29 hdfs:///user/demouser/rating/part-00000\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('hdfs dfs -ls {0}'.format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the predicted rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,500,5.518930176128497\n"
     ]
    }
   ],
   "source": [
    "ssh.cmd('hdfs dfs -cat {0}/*'.format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}