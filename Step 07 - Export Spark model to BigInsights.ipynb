{"nbformat_minor": 0, "metadata": {"language_info": {"version": "2.7.11", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "name": "python2", "display_name": "Python 2 with Spark 1.6"}}, "nbformat": 4, "cells": [{"source": "## Overview\n\nIn this notebook, we look at how the movie recommendation model that was built on DSX can be exported to BigInsights.\nWe have a lot of flexibility on BigInsights for using the model, e.g.\n\n- from Oozie to make batch predictions\n- from Spark Streaming to make realtime predictions\n\nWe will use ssh and scp python libraries to move data and code between BigInsights and DSX", "metadata": {}, "cell_type": "markdown"}, {"source": "### Setup SSH\n\nRead cluster credentials", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 23, "outputs": [], "source": "with open('credentials', 'r') as f:\n    (hostname, username, password) = f.readline().split(',')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Install required ssh and scp libraries", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 24, "outputs": [], "source": "!pip install --user --upgrade --force --quiet git+https://github.com/snowch/nb_utils", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "print version of installed nb_utils for traceability purposes", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 25, "outputs": [{"execution_count": 25, "output_type": "execute_result", "data": {"text/plain": "{u'object': {u'sha': u'04d1ae56e6093a2ac52818661bd8679df2a36c9b',\n  u'type': u'commit',\n  u'url': u'https://api.github.com/repos/snowch/nb_utils/git/commits/04d1ae56e6093a2ac52818661bd8679df2a36c9b'},\n u'ref': u'refs/heads/master',\n u'url': u'https://api.github.com/repos/snowch/nb_utils/git/refs/heads/master'}"}, "metadata": {}}], "source": "import requests\nrequests.get('https://api.github.com/repos/snowch/nb_utils/git/refs/heads/master').json()", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "load the ssh utilities", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 26, "outputs": [], "source": "from ssh_utils import ssh_utils\nssh = ssh_utils.SshUtil(hostname, username, password)", "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "Let's verify ssh is working by listing the root folder contents of hdfs", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 27, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 9 items\ndrwxrwxr-x   - ams      hdfs             0 2016-11-02 03:03 /amshbase\ndrwxrwxrwx   - yarn     hadoop           0 2016-11-02 00:33 /app-logs\ndrwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /apps\ndrwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /iop\ndrwxr-xr-x   - mapred   hdfs             0 2016-07-05 07:05 /mapred\ndrwxrwxrwx   - mapred   hadoop           0 2016-07-05 07:05 /mr-history\ndrwx------   - demouser biusers          0 2016-11-02 00:30 /securedir\ndrwxrwxrwx   - hdfs     hdfs             0 2016-11-02 00:29 /tmp\ndrwxr-xr-x   - hdfs     hdfs             0 2016-11-02 00:29 /user\n"}], "source": "ssh.cmd('hdfs dfs -ls /')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "The next command we are running on DSX.  We create a tar archive containing our model.", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 28, "outputs": [], "source": "!rm -f recommender_model.tgz\n!tar czf recommender_model.tgz recommender_model/", "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "On BigInsights, use delete any models that were copied across to BigInsights on previous runs on the notebook.", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 29, "outputs": [], "source": "ssh.cmd('rm -rf ./recommender_model.tgz ./recommender_model')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Copy over our new model", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 30, "outputs": [], "source": "ssh.put('recommender_model.tgz')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Verify that it was copied ok", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 31, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-rw-r--r--. 1 demouser biusers 2129960 Nov  2 15:28 ./recommender_model.tgz\n"}], "source": "ssh.cmd('ls -l ./recommender_model.tgz')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Unzip the model archive", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 32, "outputs": [], "source": "ssh.cmd('tar xzf ./recommender_model.tgz')", "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "Verify the unzipped model folders", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 33, "outputs": [{"name": "stdout", "output_type": "stream", "text": "./recommender_model\n./recommender_model/data\n./recommender_model/data/user\n./recommender_model/data/user/.part-r-00000-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet.crc\n./recommender_model/data/user/part-r-00001-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet\n./recommender_model/data/user/._SUCCESS.crc\n./recommender_model/data/user/part-r-00000-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet\n./recommender_model/data/user/_SUCCESS\n./recommender_model/data/user/_common_metadata\n./recommender_model/data/user/.part-r-00001-70815abd-bcd7-4945-9bc5-b0796c698570.gz.parquet.crc\n./recommender_model/data/user/._common_metadata.crc\n./recommender_model/data/user/_metadata\n./recommender_model/data/user/._metadata.crc\n./recommender_model/data/product\n./recommender_model/data/product/part-r-00000-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet\n./recommender_model/data/product/.part-r-00000-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet.crc\n./recommender_model/data/product/._SUCCESS.crc\n./recommender_model/data/product/_SUCCESS\n./recommender_model/data/product/_common_metadata\n./recommender_model/data/product/.part-r-00001-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet.crc\n./recommender_model/data/product/part-r-00001-a9529fe2-fc9e-4b18-95c7-6300fd989442.gz.parquet\n./recommender_model/data/product/._common_metadata.crc\n./recommender_model/data/product/_metadata\n./recommender_model/data/product/._metadata.crc\n./recommender_model/metadata\n./recommender_model/metadata/._SUCCESS.crc\n./recommender_model/metadata/.part-00000.crc\n./recommender_model/metadata/_SUCCESS\n./recommender_model/metadata/part-00000\n"}], "source": "ssh.cmd('find ./recommender_model')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Remove any models that were copied to BigInsights HDFS on previous runs on the notebook. And then copy the model from the BigInsights local file system to HDFS.", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 34, "outputs": [], "source": "model_path = 'hdfs:///user/{0}/recommender_model'.format(username)", "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": 35, "outputs": [{"name": "stdout", "output_type": "stream", "text": "hdfs:///user/demouser/recommender_model\n"}], "source": "print(model_path)", "metadata": {"collapsed": false}, "cell_type": "code"}, {"execution_count": 36, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted hdfs:///user/demouser/recommender_model\n"}], "source": "ssh.cmd('hdfs dfs -rm -r -skipTrash {0}'.format(model_path)) # it's ok if this fails\nssh.cmd('hdfs dfs -copyFromLocal ./recommender_model {0}'.format(model_path))", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Verify the model exists in HDFS", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 37, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\ndrwxr-xr-x   - demouser hdfs          0 2016-11-02 15:29 hdfs:///user/demouser/recommender_model/data\ndrwxr-xr-x   - demouser hdfs          0 2016-11-02 15:29 hdfs:///user/demouser/recommender_model/metadata\n"}], "source": "ssh.cmd('hdfs dfs -ls {0}'.format(model_path))", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Copy a scala spark class to the cluster for doing the predictions<br/>\nSee https://github.com/snowch/demo_2710/blob/master/scala_predictor/MovieRating.scala", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 38, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-rw-r--r--. 1 demouser biusers 2472 Nov  2 15:29 movie-rating.jar\n"}], "source": "ssh.cmd('rm -f movie-rating.jar')\nssh.cmd('wget -q -O movie-rating.jar https://github.com/snowch/demo_2710/blob/master/scala_predictor/movie-rating_2.10-1.0.jar?raw=true')\nssh.cmd('ls -l movie-rating.jar')", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "The user_id and movie_id we want predictions for", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 39, "outputs": [], "source": "user_id = 0\nmovie_id = 500", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Execute the spark class", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 40, "outputs": [{"name": "stdout", "output_type": "stream", "text": "hdfs:///user/demouser/rating\n"}], "source": "output_path = 'hdfs:///user/{0}/rating'.format(username)\nprint(output_path)", "metadata": {"collapsed": false}, "cell_type": "code"}, {"execution_count": 41, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted hdfs:///user/demouser/rating\n16/11/02 15:29:13 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n16/11/02 15:29:13 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n16/11/02 15:29:14 INFO TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n16/11/02 15:29:15 INFO RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4194.bi.services.us-south.bluemix.net/172.16.237.1:8050\n16/11/02 15:29:16 INFO Client: Requesting a new application from cluster with 1 NodeManagers\n16/11/02 15:29:16 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n16/11/02 15:29:16 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n16/11/02 15:29:16 INFO Client: Setting up container launch context for our AM\n16/11/02 15:29:16 INFO Client: Setting up the launch environment for our AM container\n16/11/02 15:29:16 INFO Client: Preparing resources for our AM container\n16/11/02 15:29:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8020/iop/apps/4.2.0.0/spark/jars/spark-assembly.jar\n16/11/02 15:29:17 INFO Client: Uploading resource file:/home/demouser/movie-rating.jar -> hdfs://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8020/user/demouser/.sparkStaging/application_1478046523919_0019/movie-rating.jar\n16/11/02 15:29:18 INFO Client: Uploading resource file:/tmp/spark-f576c88d-5528-4b99-a00f-eb23a1870487/__spark_conf__1791630809560458183.zip -> hdfs://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8020/user/demouser/.sparkStaging/application_1478046523919_0019/__spark_conf__1791630809560458183.zip\n16/11/02 15:29:18 WARN Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode\n16/11/02 15:29:18 INFO SecurityManager: Changing view acls to: demouser\n16/11/02 15:29:18 INFO SecurityManager: Changing modify acls to: demouser\n16/11/02 15:29:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(demouser); users with modify permissions: Set(demouser)\n16/11/02 15:29:18 INFO Client: Submitting application 19 to ResourceManager\n16/11/02 15:29:18 INFO YarnClientImpl: Submitted application application_1478046523919_0019\n16/11/02 15:29:19 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n16/11/02 15:29:19 INFO Client:\n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1478100558571\n\t final status: UNDEFINED\n\t tracking URL: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8088/proxy/application_1478046523919_0019/\n\t user: demouser\n16/11/02 15:29:20 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n16/11/02 15:29:21 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n16/11/02 15:29:22 INFO Client: Application report for application_1478046523919_0019 (state: ACCEPTED)\n16/11/02 15:29:23 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:23 INFO Client:\n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.16.237.2\n\t ApplicationMaster RPC port: 0\n\t queue: default\n\t start time: 1478100558571\n\t final status: UNDEFINED\n\t tracking URL: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8088/proxy/application_1478046523919_0019/\n\t user: demouser\n16/11/02 15:29:24 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:25 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:26 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:27 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:28 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:29 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:30 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:31 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:32 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:33 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:34 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:35 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:36 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:37 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:38 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:39 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:40 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:41 INFO Client: Application report for application_1478046523919_0019 (state: RUNNING)\n16/11/02 15:29:42 INFO Client: Application report for application_1478046523919_0019 (state: FINISHED)\n16/11/02 15:29:42 INFO Client:\n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.16.237.2\n\t ApplicationMaster RPC port: 0\n\t queue: default\n\t start time: 1478100558571\n\t final status: SUCCEEDED\n\t tracking URL: http://bi-hadoop-prod-4194.bi.services.us-south.bluemix.net:8088/proxy/application_1478046523919_0019/\n\t user: demouser\n16/11/02 15:29:42 INFO ShutdownHookManager: Shutdown hook called\n16/11/02 15:29:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-f576c88d-5528-4b99-a00f-eb23a1870487\n"}], "source": "# ensure the output folder is clean\nssh.cmd('hdfs dfs -rm -f -r -skipTrash {0}'.format(output_path))\nssh.cmd('spark-submit --class \"MovieRating\" --master yarn-cluster ./movie-rating.jar {0} {1} {2} {3}'.format(model_path, user_id, movie_id, output_path))", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Verify we have some ratings", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 42, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\n-rw-r--r--   3 demouser hdfs          0 2016-11-02 15:29 hdfs:///user/demouser/rating/_SUCCESS\n-rw-r--r--   3 demouser hdfs         24 2016-11-02 15:29 hdfs:///user/demouser/rating/part-00000\n"}], "source": "ssh.cmd('hdfs dfs -ls {0}'.format(output_path))", "metadata": {"collapsed": false}, "cell_type": "code"}, {"source": "Let's check the predicted rating", "metadata": {}, "cell_type": "markdown"}, {"execution_count": 43, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0,500,5.518930176128497\n"}], "source": "ssh.cmd('hdfs dfs -cat {0}/*'.format(output_path))", "metadata": {"collapsed": false}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": "", "metadata": {"collapsed": true}, "cell_type": "code"}]}