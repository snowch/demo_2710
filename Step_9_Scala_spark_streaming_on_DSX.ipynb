{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads prediction requests from a MessageHub (kafka) topic and makes predictions.<br>\n",
    "In a real world application these requests could be put on the topic by a web application that a user is interacting with.<br>\n",
    "<br>\n",
    "This notebook prints the predictions to the console.<br>\n",
    "A future notebook will put the predictions on another MessageHub topic where it can be read by the web application to make recommendations to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka-clients/0.9.0.0/kafka-clients-0.9.0.0.jar\n",
    "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.9.0.0/kafka_2.10-0.9.0.0.jar\n",
    "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka-log4j-appender/0.9.0.0/kafka-log4j-appender-0.9.0.0.jar\n",
    "%Addjar https://github.com/ibm-messaging/message-hub-samples/raw/master/java/message-hub-login-library/messagehub.login-1.0.0.jar\n",
    "%Addjar https://github.com/ibm-messaging/iot-messgehub-spark-samples/releases/download/v0.1/streaming-kafka.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the MessageHub properties that were saved by the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.util.Properties\n",
    "import java.io.FileInputStream\n",
    "\n",
    "val prop = new Properties()\n",
    "prop.load(new FileInputStream(\"messagehub.properties\"))\n",
    "\n",
    "val bootstrap_servers     = prop.getProperty(\"bootstrap_servers\")\n",
    "val sasl_username         = prop.getProperty(\"sasl_username\")\n",
    "val sasl_password         = prop.getProperty(\"sasl_password\")\n",
    "val messagehub_topic_name = prop.getProperty(\"messagehub_topic_name\")\n",
    "val api_key               = prop.getProperty(\"api_key\")\n",
    "val kafka_rest_url        = prop.getProperty(\"kafka_rest_url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spark streaming to retrieve the prediction requests and make predictions.<br>\n",
    "You will need to stop the notebook kernel to quit the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for user=1, movie=500 is 3.685349066437224\n",
      "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n",
      "com/ibm/cds/spark/samples/config/jaas.conf\n",
      "Registering JaasConfiguration: /gpfs/fs01/user/s85d-88ebffb000cc3e-39ca506ba762/notebook/tmp/p5uQW3nsiHMczWE9/jaas.conf\n",
      "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n",
      "1, 500, 3.685349066437224\n",
      "1, 501, 3.2725849355746823\n",
      "1, 502, 1.9849899602592473\n",
      "1, 503, 2.5721063844976557\n",
      "100000, 503, -1\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import com.ibm.cds.spark.samples.config.MessageHubConfig\n",
    "import com.ibm.cds.spark.samples.dstream.KafkaStreaming.KafkaStreamingContextAdapter\n",
    "import org.apache.kafka.common.serialization.Deserializer\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import java.util.UUID\n",
    "import scala.util.Try\n",
    "\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "\n",
    "// load the saved model\n",
    "val model = MatrixFactorizationModel.load(sc, \"./recommender_model/\")\n",
    "\n",
    "// test the model\n",
    "println( \"Prediction for user=1, movie=500 is \" + model.predict(1, 500) ) \n",
    "\n",
    "val kafkaProps = new MessageHubConfig\n",
    "\n",
    "kafkaProps.setConfig(\"bootstrap.servers\",   bootstrap_servers)\n",
    "kafkaProps.setConfig(\"kafka.user.name\",     sasl_username)\n",
    "kafkaProps.setConfig(\"kafka.user.password\", sasl_password)\n",
    "kafkaProps.setConfig(\"kafka.topic\",         messagehub_topic_name)\n",
    "kafkaProps.setConfig(\"api_key\",             api_key)\n",
    "kafkaProps.setConfig(\"kafka_rest_url\",      kafka_rest_url)\n",
    "kafkaProps.setConfig(\"auto.offset.reset\",   \"earliest\") // should this be \"smallest\"?\n",
    "kafkaProps.setConfig(\"group.id\",            UUID.randomUUID().toString())\n",
    "\n",
    "kafkaProps.createConfiguration()\n",
    "\n",
    "// kafkaProps.toImmutableMap.foreach { keyVal => println(keyVal._1 + \"=\" + keyVal._2) }\n",
    "// println(\"group.id=\" + kafkaProps.getConfig(\"group.id\"))\n",
    "\n",
    "val ssc = new StreamingContext( sc, Seconds(2) )\n",
    "\n",
    "val stream = ssc.createKafkaStream[String, String, StringDeserializer, StringDeserializer](\n",
    "                     kafkaProps,\n",
    "                     List(kafkaProps.getConfig(\"kafka.topic\"))\n",
    "                     )\n",
    "\n",
    "// let's wrap the predict function with a try catch block\n",
    "def predict(userId: Int, movieId: Int): Try[Any] = {\n",
    "    Try(model.predict(userId, movieId))\n",
    "}\n",
    "\n",
    "val moviesToRate = stream.map(_._2.split(\",\"))\n",
    "\n",
    "moviesToRate.foreachRDD( rdd => {\n",
    "    for(item <- rdd.collect().toArray) {\n",
    "        val userId = item(0).toInt\n",
    "        val movieId = item(1).toInt     \n",
    "        val prediction = predict(userId, movieId).getOrElse(-1)\n",
    "        \n",
    "        // TODO: the predictions should be put on another topic to be consumed\n",
    "        // by the client application (e.g. a web app)\n",
    "        \n",
    "        println(s\"$userId, $movieId, $prediction\")\n",
    "    }\n",
    "})\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "// if we didn't want to wait indefinitely, we could replace\n",
    "// the above statement with:\n",
    "//\n",
    "//   ssc.awaitTerminationOrTimeout(30000)\n",
    "ssc.stop(stopSparkContext=false, stopGracefully=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}