{"nbformat": 4, "cells": [{"source": "This notebook reads prediction requests from a MessageHub (kafka) topic and makes predictions.<br>\nIn a real world application these requests could be put on the topic by a web application that a user is interacting with.<br>\n<br>\nThis notebook prints the predictions to the console.<br>\nA future notebook will put the predictions on another MessageHub topic where it can be read by the web application to make recommendations to the user.", "cell_type": "markdown", "metadata": {}}, {"source": "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka-clients/0.9.0.0/kafka-clients-0.9.0.0.jar\n%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.9.0.0/kafka_2.10-0.9.0.0.jar\n%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka-log4j-appender/0.9.0.0/kafka-log4j-appender-0.9.0.0.jar\n%Addjar https://github.com/ibm-messaging/message-hub-samples/raw/master/java/message-hub-login-library/messagehub.login-1.0.0.jar\n%Addjar https://github.com/ibm-messaging/iot-messgehub-spark-samples/releases/download/v0.1/streaming-kafka.jar", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting download from http://central.maven.org/maven2/org/apache/kafka/kafka-clients/0.9.0.0/kafka-clients-0.9.0.0.jar\nFinished download of kafka-clients-0.9.0.0.jar\nStarting download from http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.9.0.0/kafka_2.10-0.9.0.0.jar\nFinished download of kafka_2.10-0.9.0.0.jar\nStarting download from http://central.maven.org/maven2/org/apache/kafka/kafka-log4j-appender/0.9.0.0/kafka-log4j-appender-0.9.0.0.jar\nFinished download of kafka-log4j-appender-0.9.0.0.jar\nStarting download from https://github.com/ibm-messaging/message-hub-samples/raw/master/java/message-hub-login-library/messagehub.login-1.0.0.jar\nFinished download of messagehub.login-1.0.0.jar\nStarting download from https://github.com/ibm-messaging/iot-messgehub-spark-samples/releases/download/v0.1/streaming-kafka.jar\nFinished download of streaming-kafka.jar\n"}]}, {"source": "**IMPORTANT:** Restart your kernel after running the above cell for the first time.", "cell_type": "markdown", "metadata": {}}, {"source": "Read the MessageHub properties that were saved by the previous step.", "cell_type": "markdown", "metadata": {}}, {"source": "import java.util.Properties\nimport java.io.FileInputStream\n\nval prop = new Properties()\nprop.load(new FileInputStream(\"messagehub.properties\"))\n\nval bootstrap_servers     = prop.getProperty(\"bootstrap_servers\")\nval sasl_username         = prop.getProperty(\"sasl_username\")\nval sasl_password         = prop.getProperty(\"sasl_password\")\nval messagehub_topic_name = prop.getProperty(\"messagehub_topic_name\")\nval api_key               = prop.getProperty(\"api_key\")\nval kafka_rest_url        = prop.getProperty(\"kafka_rest_url\")\n\nprintln (bootstrap_servers)\nprintln (sasl_username)\nprintln (sasl_password)\nprintln (messagehub_topic_name)\nprintln (api_key)\nprintln (kafka_rest_url)", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 7, "outputs": [{"output_type": "stream", "name": "stdout", "text": "kafka01-prod01.messagehub.services.us-south.bluemix.net:9093,kafka02-prod01.messagehub.services.us-south.bluemix.net:9093,kafka03-prod01.messagehub.services.us-south.bluemix.net:9093,kafka04-prod01.messagehub.services.us-south.bluemix.net:9093,kafka05-prod01.messagehub.services.us-south.bluemix.net:9093\n0fa6cpzwWnrLoYQI\nhy0ByQF98ylVDy5LC6vlYWeods2ppScb\nmy_topic\n0fa6cpzwWnrLoYQIhy0ByQF98ylVDy5LC6vlYWeods2ppScb\nhttps://kafka-rest-prod01.messagehub.services.us-south.bluemix.net:443\n"}]}, {"source": "Load the model and create a properties object for spark streaming", "cell_type": "markdown", "metadata": {}}, {"source": "import scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nimport com.ibm.cds.spark.samples.config.MessageHubConfig\nimport com.ibm.cds.spark.samples.dstream.KafkaStreaming.KafkaStreamingContextAdapter\nimport org.apache.kafka.common.serialization.Deserializer\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.kafka.common.serialization.StringSerializer\nimport org.apache.kafka.clients.producer.KafkaProducer\nimport org.apache.kafka.clients.producer.ProducerRecord\nimport java.util.UUID\nimport java.util.Properties\nimport scala.util.Try\n\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.mllib.recommendation.Rating\n\n// load the saved model\nval model = MatrixFactorizationModel.load(sc, \"./recommender_model/\")\n\n// test the model\nprintln( \"Prediction for user=1, movie=500 is \" + model.predict(1, 500) ) \n\nval kafkaProps = new MessageHubConfig\n\nkafkaProps.setConfig(\"bootstrap.servers\",   bootstrap_servers)\nkafkaProps.setConfig(\"kafka.user.name\",     sasl_username)\nkafkaProps.setConfig(\"kafka.user.password\", sasl_password)\nkafkaProps.setConfig(\"kafka.topic\",         messagehub_topic_name)\nkafkaProps.setConfig(\"api_key\",             api_key)\nkafkaProps.setConfig(\"kafka_rest_url\",      kafka_rest_url)\nkafkaProps.setConfig(\"auto.offset.reset\",   \"earliest\") // should this be \"smallest\"?\nkafkaProps.setConfig(\"group.id\",            UUID.randomUUID().toString())\n\n// the topic for responses\nval messagehub_response_topic_name = messagehub_topic_name + \"_responses\" \n\nkafkaProps.createConfiguration()\n\nval properties = new Properties()\nkafkaProps.toImmutableMap.foreach {\n    case (key, value) => properties.setProperty (key, value)\n}\nproperties.setProperty(\n    \"value.serializer\", \n    \"org.apache.kafka.common.serialization.StringSerializer\"\n)\n\n// create a producer for sending responses\nval kafkaProducer = new KafkaProducer[String, String]( properties )", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 9, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Prediction for user=1, movie=500 is 3.715163746095887\ndefault location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\ncom/ibm/cds/spark/samples/config/jaas.conf\nRegistering JaasConfiguration: /gpfs/fs01/user/s30f-65857bea3b733e-39ca506ba762/notebook/tmp/0fa6cpzwWnrLoYQI/jaas.conf\n"}, {"data": {"text/plain": "null"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}]}, {"source": "Use spark streaming to retrieve the prediction requests and make predictions.<br>\n**IMPORTANT** The following code will block - you will need to stop the notebook kernel to quit the code below.<br>\nAfter running the code below,\n\n 1. go back to the previous notebook to send some messages to MessageHub: **STEP 08 (A) - Produce Prediction Requests**\n 2. you should see some requests output to the console below\n 3. then go to the previous notebook and attempt to consume the responses: **STEP 08 (B) - Consume Prediction Responses** ", "cell_type": "markdown", "metadata": {}}, {"source": "val ssc = new StreamingContext( sc, Seconds(2) )\n\nval stream = ssc.createKafkaStream[String, String, StringDeserializer, StringDeserializer](\n                     kafkaProps,\n                     List(kafkaProps.getConfig(\"kafka.topic\"))\n                     )\n\n// let's wrap the predict function with a try catch block\ndef predict(userId: Int, movieId: Int): Try[Any] = {\n    Try(model.predict(userId, movieId))\n}\n\nval moviesToRate = stream.\n                    filter(_._2.contains(\",\")).\n                    map(_._2.split(\",\"))\n\nmoviesToRate.foreachRDD( rdd => {\n    for(item <- rdd.collect().toArray) {\n        val userId = item(0).toInt\n        val movieId = item(1).toInt     \n        val prediction = predict(userId, movieId).getOrElse(-1)\n        \n        // print the prediction responses to the console\n        println(s\"$userId, $movieId, $prediction\")\n        \n        val producerRecord = new ProducerRecord[String, String](messagehub_response_topic_name, s\"$userId, $movieId, $prediction\")\n        \n        // send the prediction responses to MessageHub\n        kafkaProducer.send( producerRecord );\n    }\n})\n\nssc.start()\nssc.awaitTermination() // you will need to restart the notebook kernel to quit", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 12, "outputs": [{"output_type": "stream", "name": "stdout", "text": "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n"}]}, {"source": "", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "", "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}], "metadata": {"language_info": {"name": "scala"}, "kernelspec": {"language": "scala", "display_name": "Scala 2.10 with Spark 1.6", "name": "scala"}}, "nbformat_minor": 0}