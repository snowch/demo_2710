{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads prediction requests from a MessageHub (kafka) topic and makes predictions.<br>\n",
    "In a real world application these requests could be put on the topic by a web application that a user is interacting with.<br>\n",
    "<br>\n",
    "This notebook prints the predictions to the console.<br>\n",
    "A future notebook will put the predictions on another MessageHub topic where it can be read by the web application to make recommendations to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://central.maven.org/maven2/org/apache/kafka/kafka-clients/0.9.0.0/kafka-clients-0.9.0.0.jar\n",
      "Finished download of kafka-clients-0.9.0.0.jar\n",
      "Starting download from http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.9.0.0/kafka_2.10-0.9.0.0.jar\n",
      "Finished download of kafka_2.10-0.9.0.0.jar\n",
      "Starting download from http://central.maven.org/maven2/org/apache/kafka/kafka-log4j-appender/0.9.0.0/kafka-log4j-appender-0.9.0.0.jar\n",
      "Finished download of kafka-log4j-appender-0.9.0.0.jar\n",
      "Starting download from https://github.com/ibm-messaging/message-hub-samples/raw/master/java/message-hub-login-library/messagehub.login-1.0.0.jar\n",
      "Finished download of messagehub.login-1.0.0.jar\n",
      "Starting download from https://github.com/ibm-messaging/iot-messgehub-spark-samples/releases/download/v0.1/streaming-kafka.jar\n",
      "Finished download of streaming-kafka.jar\n"
     ]
    }
   ],
   "source": [
    "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka-clients/0.9.0.0/kafka-clients-0.9.0.0.jar\n",
    "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.9.0.0/kafka_2.10-0.9.0.0.jar\n",
    "%Addjar http://central.maven.org/maven2/org/apache/kafka/kafka-log4j-appender/0.9.0.0/kafka-log4j-appender-0.9.0.0.jar\n",
    "%Addjar https://github.com/ibm-messaging/message-hub-samples/raw/master/java/message-hub-login-library/messagehub.login-1.0.0.jar\n",
    "%Addjar https://github.com/ibm-messaging/iot-messgehub-spark-samples/releases/download/v0.1/streaming-kafka.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Restart your kernel after running the above cell for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the MessageHub properties that were saved by the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.util.Properties\n",
    "import java.io.FileInputStream\n",
    "\n",
    "val prop = new Properties()\n",
    "prop.load(new FileInputStream(\"messagehub.properties\"))\n",
    "\n",
    "val bootstrap_servers     = prop.getProperty(\"bootstrap_servers\")\n",
    "val sasl_username         = prop.getProperty(\"sasl_username\")\n",
    "val sasl_password         = prop.getProperty(\"sasl_password\")\n",
    "val messagehub_topic_name = prop.getProperty(\"messagehub_topic_name\")\n",
    "val api_key               = prop.getProperty(\"api_key\")\n",
    "val kafka_rest_url        = prop.getProperty(\"kafka_rest_url\")\n",
    "\n",
    "// set to true to debug\n",
    "if (false) { \n",
    "    println (bootstrap_servers)\n",
    "    println (sasl_username)\n",
    "    println (sasl_password)\n",
    "    println (messagehub_topic_name)\n",
    "    println (api_key)\n",
    "    println (kafka_rest_url)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and create a properties object for spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for user=1, movie=500 is 3.715163746095887\n",
      "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n",
      "com/ibm/cds/spark/samples/config/jaas.conf\n",
      "Registering JaasConfiguration: /gpfs/fs01/user/s30f-65857bea3b733e-39ca506ba762/notebook/tmp/0fa6cpzwWnrLoYQI/jaas.conf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import com.ibm.cds.spark.samples.config.MessageHubConfig\n",
    "import com.ibm.cds.spark.samples.dstream.KafkaStreaming.KafkaStreamingContextAdapter\n",
    "import org.apache.kafka.common.serialization.Deserializer\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.kafka.common.serialization.StringSerializer\n",
    "import org.apache.kafka.clients.producer.KafkaProducer\n",
    "import org.apache.kafka.clients.producer.ProducerRecord\n",
    "import java.util.UUID\n",
    "import java.util.Properties\n",
    "import scala.util.Try\n",
    "\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "\n",
    "// load the saved model\n",
    "val model = MatrixFactorizationModel.load(sc, \"./recommender_model/\")\n",
    "\n",
    "// test the model\n",
    "println( \"Prediction for user=1, movie=500 is \" + model.predict(1, 500) ) \n",
    "\n",
    "val kafkaProps = new MessageHubConfig\n",
    "\n",
    "kafkaProps.setConfig(\"bootstrap.servers\",   bootstrap_servers)\n",
    "kafkaProps.setConfig(\"kafka.user.name\",     sasl_username)\n",
    "kafkaProps.setConfig(\"kafka.user.password\", sasl_password)\n",
    "kafkaProps.setConfig(\"kafka.topic\",         messagehub_topic_name)\n",
    "kafkaProps.setConfig(\"api_key\",             api_key)\n",
    "kafkaProps.setConfig(\"kafka_rest_url\",      kafka_rest_url)\n",
    "kafkaProps.setConfig(\"auto.offset.reset\",   \"earliest\") // should this be \"smallest\"?\n",
    "kafkaProps.setConfig(\"group.id\",            UUID.randomUUID().toString())\n",
    "\n",
    "// the topic for responses\n",
    "val messagehub_response_topic_name = messagehub_topic_name + \"_responses\" \n",
    "\n",
    "kafkaProps.createConfiguration()\n",
    "\n",
    "val properties = new Properties()\n",
    "kafkaProps.toImmutableMap.foreach {\n",
    "    case (key, value) => properties.setProperty (key, value)\n",
    "}\n",
    "properties.setProperty(\n",
    "    \"value.serializer\", \n",
    "    \"org.apache.kafka.common.serialization.StringSerializer\"\n",
    ")\n",
    "\n",
    "// create a producer for sending responses\n",
    "val kafkaProducer = new KafkaProducer[String, String]( properties )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spark streaming to retrieve the prediction requests and make predictions.<br>\n",
    "**IMPORTANT** The following code will block - you will need to stop the notebook kernel to quit the code below.<br>\n",
    "After running the code below,\n",
    "\n",
    " 1. go back to the previous notebook to send some messages to MessageHub: **STEP 08 (A) - Produce Prediction Requests**\n",
    " 2. you should see some requests output to the console below\n",
    " 3. then go to the previous notebook and attempt to consume the responses: **STEP 08 (B) - Consume Prediction Responses** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n"
     ]
    }
   ],
   "source": [
    "val ssc = new StreamingContext( sc, Seconds(2) )\n",
    "\n",
    "val stream = ssc.createKafkaStream[String, String, StringDeserializer, StringDeserializer](\n",
    "                     kafkaProps,\n",
    "                     List(kafkaProps.getConfig(\"kafka.topic\"))\n",
    "                     )\n",
    "\n",
    "// let's wrap the predict function with a try catch block\n",
    "def predict(userId: Int, movieId: Int): Try[Any] = {\n",
    "    Try(model.predict(userId, movieId))\n",
    "}\n",
    "\n",
    "val moviesToRate = stream.\n",
    "                    filter(_._2.contains(\",\")).\n",
    "                    map(_._2.split(\",\"))\n",
    "\n",
    "moviesToRate.foreachRDD( rdd => {\n",
    "    for(item <- rdd.collect().toArray) {\n",
    "        val userId = item(0).toInt\n",
    "        val movieId = item(1).toInt     \n",
    "        val prediction = predict(userId, movieId).getOrElse(-1)\n",
    "        \n",
    "        // print the prediction responses to the console\n",
    "        println(s\"$userId, $movieId, $prediction\")\n",
    "        \n",
    "        val producerRecord = new ProducerRecord[String, String](messagehub_response_topic_name, s\"$userId, $movieId, $prediction\")\n",
    "        \n",
    "        // send the prediction responses to MessageHub\n",
    "        kafkaProducer.send( producerRecord );\n",
    "    }\n",
    "})\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination() // you will need to restart the notebook kernel to quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}