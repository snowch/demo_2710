{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, ensure you have installed spark-cloudant 1.6.4 by running the notebook: **Install spark-cloudant 1.6.4 lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python -c 'import cloudant' || pip install cloudant --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility method for timestamps\n",
    "import time\n",
    "def ts():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility method for logging\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(\"CloudantRecommender\")\n",
    "\n",
    "def info(*args):\n",
    "    \n",
    "    # sends output to notebook\n",
    "    print(args)\n",
    "    \n",
    "    # sends output to kernel log file\n",
    "    LOGGER.info(args)\n",
    "    \n",
    "def error(*args):\n",
    "    \n",
    "    # sends output to notebook\n",
    "    print(args)\n",
    "    \n",
    "    # sends output to kernel log file\n",
    "    LOGGER.error(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility class for holding cloudant connection details\n",
    "import json\n",
    "\n",
    "def set_attr_if_exists(obj, data, k):\n",
    "    try:\n",
    "        setattr(obj, k, data[k])\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "class CloudantConfig:\n",
    "    def __init__(self, database, json_file=None, host=None, username=None, password=None):\n",
    "       \n",
    "        self.database = database\n",
    "        self.host = None\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "\n",
    "        with open(json_file) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "            \n",
    "            set_attr_if_exists(self, data, 'host')\n",
    "            set_attr_if_exists(self, data, 'username')\n",
    "            set_attr_if_exists(self, data, 'password')\n",
    "        \n",
    "        # override json attributes if provided\n",
    "        if host:     self.host = host\n",
    "        if username: self.username = username\n",
    "        if password: self.password = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9aefd1f0-d288-4666-a12f-abd93ee724fc-bluemix.cloudant.com\n"
     ]
    }
   ],
   "source": [
    "sourceDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"ratingdb\"\n",
    "                    )\n",
    "\n",
    "print(sourceDB.host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "import json\n",
    "\n",
    "# we use the cloudant python library to save the recommendations\n",
    "from cloudant.client import Cloudant\n",
    "from cloudant.adapters import Replay429Adapter\n",
    "\n",
    "class CloudantMovieRecommender:\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "    \n",
    "    def train(self, sourceDB):\n",
    "                      \n",
    "        info(\"Starting load from Cloudant: \", ts())\n",
    "\n",
    "        dfReader = sqlContext.read.format(\"com.cloudant.spark\")\n",
    "        dfReader.option(\"cloudant.host\", sourceDB.host)\n",
    "        \n",
    "        if sourceDB.username:\n",
    "            dfReader.option(\"cloudant.username\", sourceDB.username)\n",
    "            \n",
    "        if sourceDB.password:\n",
    "            dfReader.option(\"cloudant.password\", sourceDB.password)\n",
    "            \n",
    "        df = dfReader.load(sourceDB.database).cache()\n",
    "\n",
    "        info(\"Finished load from Cloudant: \", ts())\n",
    "        info(\"Found\", df.count(), \"records in Cloudant\")\n",
    "        \n",
    "        # convert cloudant docs into Rating objects\n",
    "        def make_rating(row):\n",
    "            (user_id, prod_id) = row[0].split('/')\n",
    "            user_id = int(user_id.replace('user_', ''))\n",
    "            prod_id = int(prod_id.replace('movie_', ''))\n",
    "\n",
    "            rating = float(row[2])\n",
    "            return Rating(user_id, prod_id, rating)\n",
    "        \n",
    "        ratings = df.map(make_rating)\n",
    "\n",
    "        rank = 50\n",
    "        numIterations = 20\n",
    "        lambdaParam = 0.1\n",
    "\n",
    "        info(\"Starting train model: \", ts())\n",
    "        self.model = ALS.train(ratings, rank, numIterations, lambdaParam)\n",
    "        info(\"Finished train model: \", ts())\n",
    "        \n",
    "    def get_top_recommendations(self):\n",
    "        info(\"Starting __get_top_recommendations: \", ts())\n",
    "        df = self.model.recommendProductsForUsers(10).toDF()\n",
    "        df.cache()\n",
    "        info(\"Finished __get_top_recommendations: \", ts())\n",
    "        return df\n",
    "    \n",
    "    def del_old_recommendationdbs(self, cloudant_client, db_name_prefix):\n",
    "        dbs_to_del = cloudant_client.all_dbs()\n",
    "\n",
    "        # only delete dbs we are using for recommendations\n",
    "        dbs_to_del = [db for db in dbs_to_del if db.startswith(db_name_prefix + '_') ]\n",
    "\n",
    "        # ensure the list is in timestamp order\n",
    "        dbs_to_del.sort()\n",
    "\n",
    "        # keeping the last 5 dbs and delete the rest\n",
    "        for db in dbs_to_del[:-5]:\n",
    "            cloudant_client.delete_database(db)\n",
    "            info(\"Deleted old recommendations db\", db)\n",
    "            \n",
    "    def update_meta_document(self, cloudant_client, meta_db_name, latest_db_name):\n",
    "        \n",
    "        meta_db = cloudant_client[meta_db_name]\n",
    "        \n",
    "        try:\n",
    "            # update doc if exists\n",
    "            meta_doc = meta_db['recommendation_metadata']\n",
    "            meta_doc['latest_db'] = latest_db_name\n",
    "            meta_doc['timestamp'] = ts()\n",
    "            meta_doc.save()\n",
    "            info(\"Updated recommendationdb metadata record with latest_db\", latest_db_name)\n",
    "        except KeyError:\n",
    "            # create a new doc\n",
    "            data = {\n",
    "                '_id': 'recommendation_metadata',\n",
    "                'latest_db': latest_db_name,\n",
    "                'timestamp': ts()\n",
    "                }\n",
    "            meta_doc = meta_db.create_document(data)\n",
    "            if meta_doc.exists():\n",
    "                info(\"Saved recommendationdb metadata record\", str(data))\n",
    "    \n",
    "    def create_recommendationdb(self, cloudant_client):\n",
    "        # create a database for recommendations\n",
    "        import time\n",
    "        db_name = destDB.database + '_' + str(int(time.time()))\n",
    "        \n",
    "        db = cloudant_client.create_database(db_name)\n",
    "        info(\"Created new recommendations db\", db_name)\n",
    "        return db\n",
    "        \n",
    "    def save_recommendations(self, destDB):\n",
    "        df = movieRecommender.get_top_recommendations()\n",
    "        \n",
    "        cloudant_client = Cloudant(\n",
    "                                destDB.username,\n",
    "                                destDB.password,\n",
    "                                account=destDB.username, \n",
    "                                adapter=Replay429Adapter(retries=10)\n",
    "                                )\n",
    "        cloudant_client.connect()\n",
    "        self.del_old_recommendationdbs(cloudant_client, destDB.database)\n",
    "        recommendations_db = self.create_recommendationdb(cloudant_client)\n",
    "\n",
    "        # reformat data for saving\n",
    "        docs = df.map(lambda x: {'_id':str(x[0]), 'recommendations':x[1]}).collect()\n",
    "        \n",
    "        # we could hit cloudant resource limits if trying to save entire doc\n",
    "        # so we save it in smaller sized chunks\n",
    "        \n",
    "        for i in range(0, len(docs), 100):\n",
    "            chunk = docs[i:i + 100]\n",
    "            recommendations_db.bulk_docs(chunk) # TODO check for errors saving the chunk\n",
    "            info(\"Saved recommendations chunk\", i, ts())\n",
    "        \n",
    "        self.update_meta_document(cloudant_client, destDB.database, recommendations_db.database_name)\n",
    "        \n",
    "        info(\"Saved recommendations to: \", recommendations_db.database_name, ts())\n",
    "\n",
    "        cloudant_client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your cloudant credentials below, change notebook format to 'Code' and run the cell to save your credentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF > cloudant_credentials.json\n",
    "{\n",
    "  \"username\": \"changeme\",\n",
    "  \"password\": \"changeme\",\n",
    "  \"host\": \"changeme\",\n",
    "  \"port\": 443,\n",
    "  \"url\": \"changeme\"\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Starting load from Cloudant: ', '2017-01-05 15:40:13 CST')\n",
      "('Finished load from Cloudant: ', '2017-01-05 15:41:16 CST')\n",
      "('Found', 1000009, 'records in Cloudant')\n",
      "('Starting train model: ', '2017-01-05 15:42:47 CST')\n",
      "('Finished train model: ', '2017-01-05 15:43:24 CST')\n",
      "('Starting __get_top_recommendations: ', '2017-01-05 15:43:24 CST')\n",
      "('Finished __get_top_recommendations: ', '2017-01-05 15:43:28 CST')\n",
      "('Created new recommendations db', 'recommendationdb_1483652608')\n",
      "('Saved recommendations chunk', 0, '2017-01-05 15:43:30 CST')\n",
      "('Saved recommendations chunk', 100, '2017-01-05 15:43:30 CST')\n",
      "('Saved recommendations chunk', 200, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 300, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 400, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 500, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 600, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 700, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 800, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 900, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 1000, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 1100, '2017-01-05 15:43:31 CST')\n",
      "('Saved recommendations chunk', 1200, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1300, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1400, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1500, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1600, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1700, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1800, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 1900, '2017-01-05 15:43:32 CST')\n",
      "('Saved recommendations chunk', 2000, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2100, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2200, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2300, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2400, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2500, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2600, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2700, '2017-01-05 15:43:33 CST')\n",
      "('Saved recommendations chunk', 2800, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 2900, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3000, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3100, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3200, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3300, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3400, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3500, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3600, '2017-01-05 15:43:34 CST')\n",
      "('Saved recommendations chunk', 3700, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 3800, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 3900, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 4000, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 4100, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 4200, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 4300, '2017-01-05 15:43:35 CST')\n",
      "('Saved recommendations chunk', 4400, '2017-01-05 15:43:36 CST')\n",
      "('Saved recommendations chunk', 4500, '2017-01-05 15:43:36 CST')\n",
      "('Saved recommendations chunk', 4600, '2017-01-05 15:43:36 CST')\n",
      "('Saved recommendations chunk', 4700, '2017-01-05 15:43:36 CST')\n",
      "('Saved recommendations chunk', 4800, '2017-01-05 15:43:36 CST')\n",
      "('Saved recommendations chunk', 4900, '2017-01-05 15:43:36 CST')\n",
      "('Saved recommendations chunk', 5000, '2017-01-05 15:43:37 CST')\n",
      "('Saved recommendations chunk', 5100, '2017-01-05 15:43:37 CST')\n",
      "('Saved recommendations chunk', 5200, '2017-01-05 15:43:37 CST')\n",
      "('Saved recommendations chunk', 5300, '2017-01-05 15:43:37 CST')\n",
      "('Saved recommendations chunk', 5400, '2017-01-05 15:43:37 CST')\n",
      "('Saved recommendations chunk', 5500, '2017-01-05 15:43:37 CST')\n",
      "('Saved recommendations chunk', 5600, '2017-01-05 15:43:38 CST')\n",
      "('Saved recommendations chunk', 5700, '2017-01-05 15:43:38 CST')\n",
      "('Saved recommendations chunk', 5800, '2017-01-05 15:43:38 CST')\n",
      "('Saved recommendations chunk', 5900, '2017-01-05 15:43:38 CST')\n",
      "('Saved recommendations chunk', 6000, '2017-01-05 15:43:38 CST')\n",
      "('Updated recommendationdb metadata record with latest_db', 'recommendationdb_1483652608')\n",
      "('Saved recommendations to: ', 'recommendationdb_1483652608', '2017-01-05 15:43:38 CST')\n"
     ]
    }
   ],
   "source": [
    "sourceDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"ratingdb\"\n",
    "                    )\n",
    "\n",
    "destDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"recommendationdb\", \n",
    "                    )\n",
    "\n",
    "import traceback\n",
    "try:\n",
    "    movieRecommender = CloudantMovieRecommender(sc)\n",
    "    movieRecommender.train(sourceDB)\n",
    "    movieRecommender.save_recommendations(destDB)\n",
    "except Exception as e:\n",
    "    error(str(e), traceback.format_exc(), ts())\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For debugging issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== 20170105_212445 ==========================\n",
      "/usr/local/src/bluemix_jupyter_bundle.v31/provision/pyspark_kernel_wrapper.sh /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/jupyter-rt/kernel-ca960e98-2971-44f9-9e2c-25f125822604.json spark160master\n",
      "no extra config\n",
      "load default config from : /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/spark-config/spark160master\n",
      "-------- Environment for PySpark --------\n",
      "APP_ENV_BM_DOMAIN=ng.bluemix.net\n",
      "APP_ENV_CDSX_NOTEBOOKS_API=cdsx-notebooks-api.ng.bluemix.net\n",
      "APP_ENV_ENVIRONMENT=prod\n",
      "APP_ENV_IBM_ONLY_AUTH=false\n",
      "APP_ENV_JUPYTER_TENANTS_API=cdsx-tenants-api.ng.bluemix.net\n",
      "APP_ENV_NOTEBOOKS_JOB_MANAGER=cdsx-notebooks-job-manager.ng.bluemix.net\n",
      "ATLAS_VERSION=3.10.2\n",
      "_=/bin/printenv\n",
      "BLUEMIX_RES_PLAN=s\n",
      "BRUNEL_CONFIG=locjavascript=/data/jupyter2/23fd2775-6db5-4d40-b15a-8ea34840daaa/nbextensions/brunel_ext\n",
      "CC_DISABLE_BIG_BUFFER_API=true\n",
      "CDSX_APP_ENV_NOTEBOOKS_API_URL=https://cdsx-notebooks-api.ng.bluemix.net/v1/notebooks/\n",
      "C_INCLUDE_PATH=/usr/local/src/bluemix_jupyter_bundle.v31/notebook/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/include:\n",
      "DEPLOY_HOME=/usr/local/src/bluemix_jupyter_bundle.v31\n",
      "DS_RESOURCE_LOCATION=/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/res\n",
      "DW_CONN_HOME=/usr/local/src/dataconnector-dw/spark-1.6.0/Server\n",
      "EGO_ACTIVITY_PID=-1\n",
      "EGO_CONFDIR=/disk1/ego/kernel/conf\n",
      "EGO_CONTAINER_ID=465807\n",
      "EGO_EXPORT_OS_USER_ENV=N\n",
      "EGO_KD_PORT=7870\n",
      "EGO_LIBDIR=/disk1/ego/3.4/linux-x86_64/lib\n",
      "EGO_LOAD_BASHRC_SRC=N\n",
      "EGO_MASTER_LIST_PEM=yp-spark-dal09-env5-0001.bluemix.net yp-spark-dal09-env5-0003\n",
      "EGOSC_INSTANCE_HOST=yp-spark-dal09-env5-0038\n",
      "EGOSC_INSTANCE_INDEX_PER_HOST=1\n",
      "EGOSC_INSTANCE_RESOURCE_GROUP=ComputeHosts\n",
      "EGOSC_INSTANCE_SEQNO=1\n",
      "EGOSC_INSTANCE_START_REASON=Restart\n",
      "EGOSC_SERVICE_NAME=s15a-8ea34840daaa3e-39ca506ba762\n",
      "HOME=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762\n",
      "IBM_JAVA_OPTIONS=-Dderby.system.home=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/jupyter-rt/kernel-ca960e98-2971-44f9-9e2c-25f125822604-20170105_212445\n",
      "JAR_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/downloads\n",
      "JAVA_HOME=/usr/local/src/spark160master/ibm-java-x86_64-80\n",
      "JPY_PARENT_PID=28857\n",
      "JUPYTER_CONFIG_DIR=/usr/local/src/bluemix_jupyter_bundle.v31/provision/jupyter-ax-ext\n",
      "JUPYTER_DATA_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/jupyter-data\n",
      "JUPYTER_RUNTIME_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/jupyter-rt\n",
      "KERNEL_ACTIVITY_LOG=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/process-info/kernel_activity.status\n",
      "LANG=en_US.UTF-8\n",
      "LD_LIBRARY_PATH=/usr/local/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-sca:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-xml:/usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/amd64/compressedrefs:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:\n",
      "LIBRARY_PATH=/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:\n",
      "LOG_FILE=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/logs/kernel-pyspark-20170105_212445.log\n",
      "MASTER=spark://yp-spark-dal09-env5-0038:7082\n",
      "NGWB_TAM_FILE_LOCATION=/usr/local/src/analytic-libs/spark-1.6.0/tam\n",
      "NOTEBOOK_HOME=/usr/local/src/bluemix_jupyter_bundle.v31/notebook\n",
      "NOTEBOOK_KERNEL=python2\n",
      "NOTEBOOK_TENANT_ID=23fd2775-6db5-4d40-b15a-8ea34840daaa\n",
      "PATH=/usr/local/src/scala/2.10/bin:/usr/local/src/spark160master/ibm-java-x86_64-80/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/bin:/bin\n",
      "PIP_DISABLE_PIP_VERSION_CHECK=true\n",
      "PIP_USER=true\n",
      "PWD=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/work\n",
      "_py_cads_dir=/usr/local/src/cognitive-assistant/cads_packages\n",
      "_py_dir_=/usr/local/src/analytic-libs/python\n",
      "_py_spark_dir_=/usr/local/src/analytic-libs/spark-1.6.0/python\n",
      "PYSPARK_DRIVER_PYTHON_OPTS=-m ipykernel -f \"/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/jupyter-rt/kernel-ca960e98-2971-44f9-9e2c-25f125822604.json\"\n",
      "PYSPARK_PYTHON=/usr/local/src/bluemix_jupyter_bundle.v31/notebook/bin/python\n",
      "PYSPARK_SUBMIT_ARGS=--master \"spark://yp-spark-dal09-env5-0038:7082\" --jar-dir \"/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/downloads\"\n",
      "PYTHONPATH=/usr/local/src/bluemix_jupyter_bundle.v31/provision/jupyter-ax-ext/cdsax_jupyter_extensions.egg:/usr/local/src/bluemix_jupyter_bundle.v31/provision/site-python::/usr/local/src/analytic-libs/python-2.7:/usr/local/src/analytic-libs/python:/usr/local/src/cognitive-assistant/cads_packages:/usr/local/src/analytic-libs/spark-1.6.0/python-2.7:/usr/local/src/analytic-libs/spark-1.6.0/python\n",
      "_py_version_=2.7\n",
      "R_HOME_PREFIX=/usr/local/src/bluemix_jupyter_bundle.v31\n",
      "R_LIBS_USER=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/R/libs\n",
      "RUNTIME_ENV_NOTEBOOK=prod\n",
      "RUNTIME_ENV_PAAS=sl\n",
      "RUNTIME_ENV_SPARK=prod\n",
      "RUNTIME_ENV_STOREFRONT=bluemix/prod\n",
      "SCALA_HOME=/usr/local/src/scala/2.10\n",
      "SERVICE_CALLER=AX\n",
      "SERVICE_HOME=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook\n",
      "SHLVL=2\n",
      "SPARK_CONF_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/spark-config/spark160master\n",
      "SPARK_CONFIG_HOME=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/spark-config\n",
      "SPARK_DEPLOY_RESOURCE_SCHEDULER=ego\n",
      "SPARK_DIST_CLASSPATH=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/data/libs/scala-2.10/*:/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/data/libs:/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/data/libs/*:/usr/local/src/dataconnector-s3/spark-1.6.0/libs/*:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/*:/usr/local/src/dataconnector-cloudant/*:/usr/local/src/analytic-libs/spark-1.6.0/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jdbc/lib/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/*:/usr/local/src/dataconnector-dw/spark-1.6.0/libs/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aws/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/config:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/FaspStreamSDK/lib/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/JISPlugins/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/branded_jdbc/lib/*:/usr/local/src/dataconnector-dw/spark-1.6.0/ASBServer/apps/lib/iis/*/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/connectors/*/*:/usr/local/src/event-stream/spark-1.6.0/libs/*:/usr/local/src/dataconnector-db2/*\n",
      "SPARK_DRIVER_MEMORY=1512M\n",
      "SPARK_EGO_APP_SCHEDULE_POLICY=hierarchy\n",
      "SPARK_EGO_CLASSPATH=/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/avro-1.8.0.jar:\n",
      "SPARK_EGO_CLIENT_TIMEOUT=1200\n",
      "SPARK_EGO_CONSUMER=/SparkOnBluemix/s15a-8ea34840daaa3e-39ca506ba762\n",
      "SPARK_EGO_DRIVER_CONSUMER=/SparkDrivers\n",
      "SPARK_EGO_DRIVER_PLAN=ComputeHosts\n",
      "SPARK_EGO_EXECUTOR_CONSUMER=/SparkOnBluemix/Spark160MasterLow/*\n",
      "SPARK_EGO_EXECUTOR_IDLE_TIMEOUT=600\n",
      "SPARK_EGO_EXECUTOR_PLAN=ComputeHosts\n",
      "SPARK_EGO_EXECUTOR_SLOTS_MAX=3\n",
      "SPARK_EGO_HIERARCHY_CONF_FILE=/usr/local/src/spark160master/spark/profile/notebook/tag.json\n",
      "SPARK_EGO_IPYTHON=true\n",
      "SPARK_EGO_JARS=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/spark-config/spark160master:/usr/local/src/spark160master/spark/ego/spark-launcher_2.10-1.6.0.jar:/usr/local/src/spark160master/spark/ego/spark-network-shuffle_2.10-1.6.0.jar:/usr/local/src/spark160master/spark/ego/gson-2.2.4.jar:/usr/local/src/spark160master/spark/ego/guava-14.0.1.jar:/usr/local/src/spark160master/spark/ego/Java-WebSocket-1.3.0.jar:/usr/local/src/spark160master/spark/ego/spark-ego_2.10-1.6.0.jar\n",
      "SPARK_EGO_LOG_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/spark/executor\n",
      "SPARK_EGO_NATIVE_LIBRARY=/usr/local/src/spark160master/spark/ego/libSparkVEMApi.so\n",
      "SPARK_EGO_RUN_AS_SERVICE=true\n",
      "SPARK_EGO_STAGING_DIR=/tmp/spark-160-ego-master/staging\n",
      "SPARK_EGO_TAG_PATH=/root/s/s15a-8ea34840daaa3e-39ca506ba762\n",
      "SPARK_ENV_LOADED=1\n",
      "SPARK_EXECUTOR_MEMORY=6G\n",
      "SPARK_EXECUTOR_OPTS=-Dspark.shuffle.service.port=7340\n",
      "SPARK_HISTORY_DATA=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/events\n",
      "SPARK_HISTORY_LOG=/gpfs/fs01/spark-ego-master\n",
      "SPARK_HISTORY_OPTS= -Dspark.eventLog.enabled=true -Dspark.eventLog.dir=/gpfs/fs01/spark-ego-master -Dspark.history.fs.logDirectory=/gpfs/fs01/spark-ego-master\n",
      "SPARK_HOME=/usr/local/src/spark160master/spark\n",
      "SPARK_LOCAL_DIRS=/tmp/spark-160-ego-master/work\n",
      "SPARK_LOG_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/logs\n",
      "SPARK_MASTER_IP=yp-spark-dal09-env5-0038\n",
      "SPARK_MASTER_PORT=7082\n",
      "SPARK_MASTER_WEBUI_PORT=12024\n",
      "SPARK_PACKAGE=spark160master\n",
      "SPARK_PID_DIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/process-ids\n",
      "SPARK_SCALA_VERSION=2.10\n",
      "SPARK_SERVICE_NAME=4jan2017\n",
      "SPARK_SUBMIT_CLASSPATH=/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/avro-1.8.0.jar:\n",
      "SPARK_SUBMIT_LIBRARY_PATH=/usr/local/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-sca:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-xml:/usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/amd64/compressedrefs:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:\n",
      "SPARK_TENANT_ID=s15a-8ea34840daaa3e-39ca506ba762\n",
      "SPARK_WORK_DIR=/tmp/spark-160-ego-master/work\n",
      "TEMP=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp\n",
      "TMPDIR=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp\n",
      "TMP=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp\n",
      "USER=s15a-8ea34840daaa3e-39ca506ba762\n",
      "VCAP_SERVICES={}\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/slf4j-log4j12-1.7.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/src/analytic-libs/spark-1.6.0/tika-app-1.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "17/01/05 15:24:47 INFO apache.spark.SparkContext: Running Spark version 1.6.0\n",
      "17/01/05 15:24:48 WARN hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/01/05 15:24:48 INFO apache.spark.SparkContext: Spark configuration:\n",
      "spark.app.name=PySparkShell\n",
      "spark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\n",
      "spark.driver.maxResultSize=1210M\n",
      "spark.driver.memory=1512M\n",
      "spark.eventLog.dir=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/events\n",
      "spark.eventLog.enabled=true\n",
      "spark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\n",
      "spark.executor.memory=6G\n",
      "spark.history.fs.logDirectory=/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/events\n",
      "spark.logConf=true\n",
      "spark.master=spark://yp-spark-dal09-env5-0038:7082\n",
      "spark.port.maxRetries=512\n",
      "spark.r.command=/usr/local/src/bluemix_jupyter_bundle.v31/R/bin/Rscript\n",
      "spark.rdd.compress=True\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.shuffle.service.enabled=true\n",
      "spark.shuffle.service.port=7340\n",
      "spark.sql.tungsten.enabled=false\n",
      "spark.sql.unsafe.enabled=false\n",
      "spark.submit.deployMode=client\n",
      "spark.task.maxFailures=10\n",
      "spark.ui.enabled=false\n",
      "spark.ui.retainedJobs=0\n",
      "spark.ui.retainedStages=0\n",
      "spark.worker.ui.retainedExecutors=0\n",
      "17/01/05 15:24:48 INFO apache.spark.SecurityManager: Changing view acls to: s15a-8ea34840daaa3e-39ca506ba762\n",
      "17/01/05 15:24:48 INFO apache.spark.SecurityManager: Changing modify acls to: s15a-8ea34840daaa3e-39ca506ba762\n",
      "17/01/05 15:24:48 INFO apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s15a-8ea34840daaa3e-39ca506ba762); users with modify permissions: Set(s15a-8ea34840daaa3e-39ca506ba762)\n",
      "17/01/05 15:24:49 INFO spark.util.Utils: Successfully started service 'sparkDriver' on port 44896.\n",
      "17/01/05 15:24:49 INFO apache.spark.SparkEnv: The address of rpcenv is :10.143.133.19:44896\n",
      "17/01/05 15:24:49 INFO event.slf4j.Slf4jLogger: Slf4jLogger started\n",
      "17/01/05 15:24:49 INFO Remoting: Starting remoting\n",
      "17/01/05 15:24:49 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.143.133.19:37263]\n",
      "17/01/05 15:24:49 INFO spark.util.Utils: Successfully started service 'sparkDriverActorSystem' on port 37263.\n",
      "17/01/05 15:24:49 INFO apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "17/01/05 15:24:49 INFO apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "17/01/05 15:24:49 INFO spark.storage.DiskBlockManager: Created local directory at /tmp/spark-160-ego-master/work/blockmgr-fb0cde4e-8b8e-48ab-94f3-5709e72db8e4\n",
      "17/01/05 15:24:49 INFO spark.storage.MemoryStore: MemoryStore started with capacity 909.0 MB\n",
      "17/01/05 15:24:49 INFO apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "17/01/05 15:24:49 INFO spark.util.EGOSparkDockerConfig: Docker not enabled\n",
      "17/01/05 15:24:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: setting reserve=0, priority=1, limit=2147483647,  master=spark://yp-spark-dal09-env5-0038:7082\n",
      "17/01/05 15:24:50 INFO client.ego.EGOAppClient$ClientEndpoint: Connecting to master spark://yp-spark-dal09-env5-0038:7082...\n",
      "17/01/05 15:24:50 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Connected to Spark cluster with app ID app-20170105152450-0470-e5de83c4-52fa-49fa-926c-086e94ea59f6\n",
      "17/01/05 15:24:50 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Application registered successfully as app-20170105152450-0470-e5de83c4-52fa-49fa-926c-086e94ea59f6\n",
      "17/01/05 15:24:50 INFO spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45040.\n",
      "17/01/05 15:24:50 INFO network.netty.NettyBlockTransferService: Server created on 45040\n",
      "17/01/05 15:24:50 INFO spark.storage.BlockManager: external shuffle service port = 7340\n",
      "17/01/05 15:24:50 INFO spark.storage.BlockManagerMaster: Trying to register BlockManager\n",
      "17/01/05 15:24:50 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager 10.143.133.19:45040 with 909.0 MB RAM, BlockManagerId(driver, 10.143.133.19, 45040)\n",
      "17/01/05 15:24:50 INFO spark.storage.BlockManagerMaster: Registered BlockManager\n",
      "17/01/05 15:24:50 INFO spark.scheduler.EventLoggingListener: Logging events to file:/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/events/app-20170105152450-0470-e5de83c4-52fa-49fa-926c-086e94ea59f6\n",
      "17/01/05 15:24:50 INFO cluster.ego.EGODeployScheduler: Spark context initialized.\n",
      "17/01/05 15:25:00 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:25:00 CST]\n",
      "17/01/05 15:25:01 INFO sql.hive.HiveContext: Initializing execution hive, version 1.2.1\n",
      "17/01/05 15:25:01 INFO hive.client.ClientWrapper: Inspected Hadoop version: 2.6.0\n",
      "17/01/05 15:25:01 INFO hive.client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0\n",
      "17/01/05 15:25:01 INFO hive.metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "17/01/05 15:25:01 INFO hive.metastore.ObjectStore: ObjectStore, initialize called\n",
      "17/01/05 15:25:02 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "17/01/05 15:25:02 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n",
      "17/01/05 15:25:04 INFO hive.metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "17/01/05 15:25:05 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:05 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:06 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:06 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:07 INFO hive.metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "17/01/05 15:25:07 INFO hive.metastore.ObjectStore: Initialized ObjectStore\n",
      "17/01/05 15:25:07 WARN hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n",
      "17/01/05 15:25:07 WARN hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "17/01/05 15:25:07 INFO hive.metastore.HiveMetaStore: Added admin role in metastore\n",
      "17/01/05 15:25:07 INFO hive.metastore.HiveMetaStore: Added public role in metastore\n",
      "17/01/05 15:25:07 INFO hive.metastore.HiveMetaStore: No user is added in admin role, since config is empty\n",
      "17/01/05 15:25:07 INFO hive.metastore.HiveMetaStore: 0: get_all_databases\n",
      "17/01/05 15:25:07 INFO metastore.HiveMetaStore.audit: ugi=s15a-8ea34840daaa3e-39ca506ba762\tip=unknown-ip-addr\tcmd=get_all_databases\t\n",
      "17/01/05 15:25:07 INFO hive.metastore.HiveMetaStore: 0: get_functions: db=default pat=*\n",
      "17/01/05 15:25:07 INFO metastore.HiveMetaStore.audit: ugi=s15a-8ea34840daaa3e-39ca506ba762\tip=unknown-ip-addr\tcmd=get_functions: db=default pat=*\t\n",
      "17/01/05 15:25:07 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:08 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp/d4d67ee1-e6bd-481a-ac16-d194c14264ca_resources\n",
      "17/01/05 15:25:08 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/s15a-8ea34840daaa3e-39ca506ba762/d4d67ee1-e6bd-481a-ac16-d194c14264ca\n",
      "17/01/05 15:25:08 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp/s15a-8ea34840daaa3e-39ca506ba762/d4d67ee1-e6bd-481a-ac16-d194c14264ca\n",
      "17/01/05 15:25:08 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/s15a-8ea34840daaa3e-39ca506ba762/d4d67ee1-e6bd-481a-ac16-d194c14264ca/_tmp_space.db\n",
      "17/01/05 15:25:08 INFO sql.hive.HiveContext: default warehouse location is /user/hive/warehouse\n",
      "17/01/05 15:25:08 INFO sql.hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.\n",
      "17/01/05 15:25:08 INFO hive.client.ClientWrapper: Inspected Hadoop version: 2.6.0\n",
      "17/01/05 15:25:08 INFO hive.client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0\n",
      "17/01/05 15:25:08 INFO hive.metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "17/01/05 15:25:08 INFO hive.metastore.ObjectStore: ObjectStore, initialize called\n",
      "17/01/05 15:25:08 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "17/01/05 15:25:08 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n",
      "17/01/05 15:25:11 INFO hive.metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "17/01/05 15:25:11 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:11 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:14 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:14 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:14 INFO hive.metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "17/01/05 15:25:14 INFO hive.metastore.ObjectStore: Initialized ObjectStore\n",
      "17/01/05 15:25:14 WARN hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n",
      "17/01/05 15:25:14 WARN hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "17/01/05 15:25:15 INFO hive.metastore.HiveMetaStore: Added admin role in metastore\n",
      "17/01/05 15:25:15 INFO hive.metastore.HiveMetaStore: Added public role in metastore\n",
      "17/01/05 15:25:15 INFO hive.metastore.HiveMetaStore: No user is added in admin role, since config is empty\n",
      "17/01/05 15:25:15 INFO hive.metastore.HiveMetaStore: 0: get_all_databases\n",
      "17/01/05 15:25:15 INFO metastore.HiveMetaStore.audit: ugi=s15a-8ea34840daaa3e-39ca506ba762\tip=unknown-ip-addr\tcmd=get_all_databases\t\n",
      "17/01/05 15:25:15 INFO hive.metastore.HiveMetaStore: 0: get_functions: db=default pat=*\n",
      "17/01/05 15:25:15 INFO metastore.HiveMetaStore.audit: ugi=s15a-8ea34840daaa3e-39ca506ba762\tip=unknown-ip-addr\tcmd=get_functions: db=default pat=*\t\n",
      "17/01/05 15:25:15 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "17/01/05 15:25:15 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp/813368b4-e6ff-4b1c-9721-bd4cdf27b80a_resources\n",
      "17/01/05 15:25:15 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/s15a-8ea34840daaa3e-39ca506ba762/813368b4-e6ff-4b1c-9721-bd4cdf27b80a\n",
      "17/01/05 15:25:15 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp/s15a-8ea34840daaa3e-39ca506ba762/813368b4-e6ff-4b1c-9721-bd4cdf27b80a\n",
      "17/01/05 15:25:15 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/s15a-8ea34840daaa3e-39ca506ba762/813368b4-e6ff-4b1c-9721-bd4cdf27b80a/_tmp_space.db\n",
      "Use connectorVersion=1.6.4, dbName=ratingdb, indexName=null, viewName=null,jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=-1\n",
      "[WARN] [01/05/2017 15:25:16.105] [Thread-7] [JsonStoreDataAccess(akka://CloudantSpark-b78eae3e-c762-4fcc-abab-0c377be53fd2)] Loading data from Cloudant using query: https://9aefd1f0-d288-4666-a12f-abd93ee724fc-bluemix.cloudant.com/ratingdb/_all_docs?limit=1\n",
      "17/01/05 15:25:17 INFO spark.common.JsonStoreRDD: Partition config - total=5, limit=200002 for totalRows of 1000009\n",
      "17/01/05 15:25:17 INFO apache.spark.SparkContext: Starting job: json at DefaultSource.scala:130\n",
      "17/01/05 15:25:17 INFO spark.scheduler.DAGScheduler: Got job 0 (json at DefaultSource.scala:130) with 5 output partitions\n",
      "17/01/05 15:25:17 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (json at DefaultSource.scala:130)\n",
      "17/01/05 15:25:17 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n",
      "17/01/05 15:25:17 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:25:17 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at json at DefaultSource.scala:130), which has no missing parents\n",
      "17/01/05 15:25:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(0)\n",
      "17/01/05 15:25:17 INFO spark.storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KB, free 4.1 KB)\n",
      "17/01/05 15:25:17 INFO spark.storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KB, free 6.6 KB)\n",
      "17/01/05 15:25:17 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.143.133.19:45040 (size: 2.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:25:17 INFO apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:25:17 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at DefaultSource.scala:130)\n",
      "17/01/05 15:25:17 INFO cluster.ego.EGODeployScheduler: Adding task set 0.0 with 5 tasks\n",
      "17/01/05 15:25:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: <EVENT> Spark driver SPARKDRIVER:df09e320-19fe-4682-b0cb-e3f32a06d311 workload coming in\n",
      "17/01/05 15:25:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0035:35714) with ID fc1a78c2-e3bc-4b1c-add1-f28e75f069e5\n",
      "17/01/05 15:25:22 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:25:22 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:25:22 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0035:38118 with 4.3 GB RAM, BlockManagerId(fc1a78c2-e3bc-4b1c-add1-f28e75f069e5, yp-spark-dal09-env5-0035, 38118)\n",
      "17/01/05 15:25:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0031:48096) with ID 199ffa1e-81d3-4064-a1ce-ba297461282b\n",
      "17/01/05 15:25:22 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:25:22 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:25:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0024:38796) with ID c50e301a-905f-49bd-a75c-1a1eeaeed808\n",
      "17/01/05 15:25:23 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, yp-spark-dal09-env5-0024, partition 3,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:25:23 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:25:23 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0031:45704 with 4.3 GB RAM, BlockManagerId(199ffa1e-81d3-4064-a1ce-ba297461282b, yp-spark-dal09-env5-0031, 45704)\n",
      "17/01/05 15:25:23 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0024:33191 with 4.3 GB RAM, BlockManagerId(c50e301a-905f-49bd-a75c-1a1eeaeed808, yp-spark-dal09-env5-0024, 33191)\n",
      "17/01/05 15:25:23 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:25:23 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:28:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 165275 ms on yp-spark-dal09-env5-0035 (1/5)\n",
      "17/01/05 15:28:26 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 183645 ms on yp-spark-dal09-env5-0035 (2/5)\n",
      "17/01/05 15:32:07 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 404364 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:32:19 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 416678 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 446086 ms on yp-spark-dal09-env5-0024 (5/5)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: ResultStage 0 (json at DefaultSource.scala:130) finished in 451.324 s\n",
      "17/01/05 15:32:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:32:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(0)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Job 0 finished: json at DefaultSource.scala:130, took 451.514577 s\n",
      "17/01/05 15:32:49 INFO spark.storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 220.7 KB, free 227.3 KB)\n",
      "17/01/05 15:32:49 INFO spark.storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 246.6 KB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.143.133.19:45040 (size: 19.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:32:49 INFO apache.spark.SparkContext: Created broadcast 1 from rdd at DefaultSource.scala:54\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.143.133.19:45040 in memory (size: 2.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:32:49 INFO apache.spark.ContextCleaner: Cleaned accumulator 2\n",
      "17/01/05 15:32:49 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:32:49 CST]\n",
      "17/01/05 15:32:49 INFO apache.spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:-2\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Registering RDD 11 (count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:-2) with 1 output partitions\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "17/01/05 15:32:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(1)\n",
      "17/01/05 15:32:49 INFO spark.storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.9 KB, free 254.9 KB)\n",
      "17/01/05 15:32:49 INFO spark.storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.1 KB, free 261.9 KB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.143.133.19:45040 (size: 7.1 KB, free: 909.0 MB)\n",
      "17/01/05 15:32:49 INFO apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:32:49 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:32:49 INFO cluster.ego.EGODeployScheduler: Adding task set 1.0 with 5 tasks\n",
      "17/01/05 15:32:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 5, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 6, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:32:49 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 7, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.143.133.19:45040 in memory (size: 19.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:32:49 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:32:51 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 8, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:32:51 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 9, yp-spark-dal09-env5-0035, partition 4,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:33:10 INFO spark.storage.BlockManagerInfo: Added rdd_8_0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.9 MB, free: 4.3 GB)\n",
      "17/01/05 15:33:11 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 5) in 21750 ms on yp-spark-dal09-env5-0024 (1/5)\n",
      "17/01/05 15:33:23 INFO spark.storage.BlockManagerInfo: Added rdd_8_1 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.3 MB, free: 4.3 GB)\n",
      "17/01/05 15:33:24 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 6) in 34418 ms on yp-spark-dal09-env5-0035 (2/5)\n",
      "17/01/05 15:33:40 INFO spark.storage.BlockManagerInfo: Added rdd_8_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.2 MB, free: 4.3 GB)\n",
      "17/01/05 15:33:41 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 7) in 51574 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:33:52 INFO spark.storage.BlockManagerInfo: Added rdd_8_3 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.4 MB, free: 4.3 GB)\n",
      "17/01/05 15:33:52 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 8) in 60832 ms on yp-spark-dal09-env5-0035 (4/5)\n",
      "17/01/05 15:34:01 INFO spark.storage.BlockManagerInfo: Added rdd_8_4 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.3 MB, free: 4.2 GB)\n",
      "17/01/05 15:34:01 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 9) in 70094 ms on yp-spark-dal09-env5-0035 (5/5)\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:-2) finished in 71.607 s\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(1)\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 2)\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(2)\n",
      "17/01/05 15:34:01 INFO spark.storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.3 KB, free 31.2 KB)\n",
      "17/01/05 15:34:01 INFO spark.storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.6 KB, free 35.8 KB)\n",
      "17/01/05 15:34:01 INFO spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.143.133.19:45040 (size: 4.6 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:01 INFO apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGODeployScheduler: Adding task set 2.0 with 1 tasks\n",
      "17/01/05 15:34:01 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10, yp-spark-dal09-env5-0031, partition 0,NODE_LOCAL, 1999 bytes)\n",
      "17/01/05 15:34:01 INFO spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 4.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:01 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 268 bytes\n",
      "17/01/05 15:34:01 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 155 ms on yp-spark-dal09-env5-0031 (1/1)\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: ResultStage 2 (count at NativeMethodAccessorImpl.java:-2) finished in 0.156 s\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Job 1 finished: count at NativeMethodAccessorImpl.java:-2, took 71.806017 s\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(2)\n",
      "17/01/05 15:34:01 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\n",
      "17/01/05 15:34:01 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:34:01 CST]\n",
      "17/01/05 15:34:01 INFO apache.spark.SparkContext: Starting job: runJob at PythonRDD.scala:393\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Got job 2 (runJob at PythonRDD.scala:393) with 1 output partitions\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:393)\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[18] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(3)\n",
      "17/01/05 15:34:01 INFO spark.storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.4 KB, free 49.2 KB)\n",
      "17/01/05 15:34:01 INFO spark.storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.1 KB, free 56.2 KB)\n",
      "17/01/05 15:34:01 INFO spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.143.133.19:45040 (size: 7.1 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:01 INFO apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:01 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[18] at RDD at PythonRDD.scala:43)\n",
      "17/01/05 15:34:01 INFO cluster.ego.EGODeployScheduler: Adding task set 3.0 with 1 tasks\n",
      "17/01/05 15:34:01 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 11, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:34:01 INFO spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 11) in 849 ms on yp-spark-dal09-env5-0024 (1/1)\n",
      "17/01/05 15:34:02 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:393) finished in 0.852 s\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:393, took 0.864462 s\n",
      "17/01/05 15:34:02 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(3)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.143.133.19:45040 in memory (size: 4.6 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 4.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 14\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.143.133.19:45040 in memory (size: 7.1 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 7.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 13\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned shuffle 0\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 12\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 11\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 10\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 9\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 8\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 7\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 6\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 5\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.143.133.19:45040 in memory (size: 7.1 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO apache.spark.ContextCleaner: Cleaned accumulator 15\n",
      "17/01/05 15:34:02 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:596\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Registering RDD 22 (mapPartitions at ALS.scala:837)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Registering RDD 25 (map at ALS.scala:1080)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Got job 3 (count at ALS.scala:596) with 3 output partitions\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (count at ALS.scala:596)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[22] at mapPartitions at ALS.scala:837), which has no missing parents\n",
      "17/01/05 15:34:02 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(4)\n",
      "17/01/05 15:34:02 INFO spark.storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.4 KB, free 15.4 KB)\n",
      "17/01/05 15:34:02 INFO spark.storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.8 KB, free 23.2 KB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.143.133.19:45040 (size: 7.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:02 INFO apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:02 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[22] at mapPartitions at ALS.scala:837)\n",
      "17/01/05 15:34:02 INFO cluster.ego.EGODeployScheduler: Adding task set 4.0 with 5 tasks\n",
      "17/01/05 15:34:02 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:34:02 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:02 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:04 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 15, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:34:05 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 2861 ms on yp-spark-dal09-env5-0024 (1/5)\n",
      "17/01/05 15:34:05 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 16, yp-spark-dal09-env5-0035, partition 4,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:34:05 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 3366 ms on yp-spark-dal09-env5-0035 (2/5)\n",
      "17/01/05 15:34:06 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 3441 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:34:06 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 15) in 2605 ms on yp-spark-dal09-env5-0035 (4/5)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 16) in 2458 ms on yp-spark-dal09-env5-0035 (5/5)\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 4 (mapPartitions at ALS.scala:837) finished in 5.825 s\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 6)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(4)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[25] at map at ALS.scala:1080), which has no missing parents\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(5)\n",
      "17/01/05 15:34:08 INFO spark.storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 16.5 KB, free 39.7 KB)\n",
      "17/01/05 15:34:08 INFO spark.storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.3 KB, free 48.0 KB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.143.133.19:45040 (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:08 INFO apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[25] at map at ALS.scala:1080)\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGODeployScheduler: Adding task set 5.0 with 5 tasks\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 17, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 18, yp-spark-dal09-env5-0031, partition 1,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 19, yp-spark-dal09-env5-0035, partition 2,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 20, yp-spark-dal09-env5-0035, partition 3,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 283 bytes\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added rdd_24_0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.0 B, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 21, yp-spark-dal09-env5-0024, partition 4,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 17) in 83 ms on yp-spark-dal09-env5-0024 (1/5)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added rdd_24_2 in memory on yp-spark-dal09-env5-0035:38118 (size: 1344.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added rdd_24_4 in memory on yp-spark-dal09-env5-0024:33191 (size: 1327.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added rdd_24_1 in memory on yp-spark-dal09-env5-0031:45704 (size: 3.8 MB, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added rdd_24_3 in memory on yp-spark-dal09-env5-0035:38118 (size: 5.1 MB, free: 4.2 GB)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 19) in 292 ms on yp-spark-dal09-env5-0035 (2/5)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 20) in 405 ms on yp-spark-dal09-env5-0035 (3/5)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 18) in 440 ms on yp-spark-dal09-env5-0031 (4/5)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 21) in 388 ms on yp-spark-dal09-env5-0024 (5/5)\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 5 (map at ALS.scala:1080) finished in 0.473 s\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 6)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(5)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 6 (userOutBlocks MapPartitionsRDD[28] at mapValues at ALS.scala:1117), which has no missing parents\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(6)\n",
      "17/01/05 15:34:08 INFO spark.storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 17.1 KB, free 65.1 KB)\n",
      "17/01/05 15:34:08 INFO spark.storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.5 KB, free 73.6 KB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.143.133.19:45040 (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:08 INFO apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:08 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 6 (userOutBlocks MapPartitionsRDD[28] at mapValues at ALS.scala:1117)\n",
      "17/01/05 15:34:08 INFO cluster.ego.EGODeployScheduler: Adding task set 6.0 with 3 tasks\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 22, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 23, yp-spark-dal09-env5-0035, partition 1,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:34:08 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 24, yp-spark-dal09-env5-0031, partition 2,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:08 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 287 bytes\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added rdd_27_1 in memory on yp-spark-dal09-env5-0035:38118 (size: 2.5 MB, free: 4.2 GB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added rdd_27_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 2.5 MB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added rdd_28_1 in memory on yp-spark-dal09-env5-0035:38118 (size: 23.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 23) in 409 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added rdd_27_0 in memory on yp-spark-dal09-env5-0024:33191 (size: 2.6 MB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added rdd_28_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 23.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 24) in 443 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added rdd_28_0 in memory on yp-spark-dal09-env5-0024:33191 (size: 23.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 22) in 476 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: ResultStage 6 (count at ALS.scala:596) finished in 0.476 s\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(6)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Job 3 finished: count at ALS.scala:596, took 6.809484 s\n",
      "17/01/05 15:34:09 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:604\n",
      "17/01/05 15:34:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 283 bytes\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Registering RDD 30 (map at ALS.scala:1080)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Got job 4 (count at ALS.scala:604) with 3 output partitions\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 9 (count at ALS.scala:604)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[30] at map at ALS.scala:1080), which has no missing parents\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(8)\n",
      "17/01/05 15:34:09 INFO spark.storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.7 KB, free 90.3 KB)\n",
      "17/01/05 15:34:09 INFO spark.storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.3 KB, free 98.6 KB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.143.133.19:45040 (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:09 INFO apache.spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[30] at map at ALS.scala:1080)\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGODeployScheduler: Adding task set 8.0 with 5 tasks\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 25, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 27, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 28, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 8.0 (TID 29, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 25) in 21 ms on yp-spark-dal09-env5-0024 (1/5)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 59 ms on yp-spark-dal09-env5-0035 (2/5)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 8.0 (TID 29) in 52 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 8.0 (TID 28) in 160 ms on yp-spark-dal09-env5-0035 (4/5)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 27) in 217 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 8 (map at ALS.scala:1080) finished in 0.219 s\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 9)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(8)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 9 (itemOutBlocks MapPartitionsRDD[33] at mapValues at ALS.scala:1117), which has no missing parents\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(9)\n",
      "17/01/05 15:34:09 INFO spark.storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.3 KB, free 115.9 KB)\n",
      "17/01/05 15:34:09 INFO spark.storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.5 KB, free 124.4 KB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.143.133.19:45040 (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:09 INFO apache.spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:09 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (itemOutBlocks MapPartitionsRDD[33] at mapValues at ALS.scala:1117)\n",
      "17/01/05 15:34:09 INFO cluster.ego.EGODeployScheduler: Adding task set 9.0 with 3 tasks\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 30, yp-spark-dal09-env5-0035, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 31, yp-spark-dal09-env5-0031, partition 1,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:34:09 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 32, yp-spark-dal09-env5-0035, partition 2,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:09 INFO spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 287 bytes\n",
      "17/01/05 15:34:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added rdd_32_0 in memory on yp-spark-dal09-env5-0035:38118 (size: 2.6 MB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added rdd_33_0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added rdd_32_2 in memory on yp-spark-dal09-env5-0035:38118 (size: 2.6 MB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 30) in 532 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added rdd_33_2 in memory on yp-spark-dal09-env5-0035:38118 (size: 13.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 32) in 538 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added rdd_32_1 in memory on yp-spark-dal09-env5-0031:45704 (size: 2.5 MB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added rdd_33_1 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 31) in 577 ms on yp-spark-dal09-env5-0031 (3/3)\n",
      "17/01/05 15:34:10 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: ResultStage 9 (count at ALS.scala:604) finished in 0.577 s\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Job 4 finished: count at ALS.scala:604, took 0.816859 s\n",
      "17/01/05 15:34:10 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(9)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.143.133.19:45040 in memory (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO apache.spark.ContextCleaner: Cleaned accumulator 20\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.143.133.19:45040 in memory (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO apache.spark.ContextCleaner: Cleaned accumulator 19\n",
      "17/01/05 15:34:10 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:263\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.143.133.19:45040 in memory (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO apache.spark.ContextCleaner: Cleaned accumulator 18\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.143.133.19:45040 in memory (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO apache.spark.ContextCleaner: Cleaned accumulator 17\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.143.133.19:45040 in memory (size: 7.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO apache.spark.ContextCleaner: Cleaned accumulator 16\n",
      "17/01/05 15:34:10 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 283 bytes\n",
      "17/01/05 15:34:10 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 287 bytes\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 34 (map at ALS.scala:752)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 39 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 48 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 57 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 66 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 75 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 84 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 93 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 102 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 111 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 120 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 129 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 138 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 147 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 156 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 165 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 174 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 183 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 192 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 201 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 210 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 219 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 228 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 237 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 246 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 255 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 264 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 273 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 282 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 291 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 300 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 309 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 318 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 327 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 336 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 345 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 354 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 363 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 372 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 381 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Registering RDD 390 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Got job 5 (count at ALS.scala:263) with 3 output partitions\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 55 (count at ALS.scala:263)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13, ShuffleMapStage 54)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[34] at map at ALS.scala:752), which has no missing parents\n",
      "17/01/05 15:34:10 INFO spark.storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.1 KB, free 17.1 KB)\n",
      "17/01/05 15:34:10 INFO spark.storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.5 KB, free 25.6 KB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.143.133.19:45040 (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:10 INFO apache.spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:10 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[34] at map at ALS.scala:752)\n",
      "17/01/05 15:34:10 INFO cluster.ego.EGODeployScheduler: Adding task set 14.0 with 3 tasks\n",
      "17/01/05 15:34:10 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 14.0 (TID 33, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 34, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:10 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 35, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:10 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(14)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 34) in 77 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 14.0 (TID 33) in 77 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 35) in 85 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 14 (map at ALS.scala:752) finished in 0.088 s\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 15, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 16, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 17, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(14)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[39] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(15)\n",
      "17/01/05 15:34:11 INFO spark.storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 18.2 KB, free 43.8 KB)\n",
      "17/01/05 15:34:11 INFO spark.storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.8 KB, free 52.5 KB)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.143.133.19:45040 (size: 8.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:11 INFO apache.spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[39] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGODeployScheduler: Adding task set 15.0 with 3 tasks\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 36, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 37, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 38, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 8.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 44 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 272 bytes\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 44 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 44 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 36) in 62 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 37) in 60 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 38) in 61 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 15 (flatMap at ALS.scala:1170) finished in 0.070 s\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 16, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 17, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(15)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[48] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(16)\n",
      "17/01/05 15:34:11 INFO spark.storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 20.4 KB, free 72.9 KB)\n",
      "17/01/05 15:34:11 INFO spark.storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 9.5 KB, free 82.4 KB)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.143.133.19:45040 (size: 9.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:11 INFO apache.spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:11 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[48] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:11 INFO cluster.ego.EGODeployScheduler: Adding task set 16.0 with 3 tasks\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 39, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:11 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 40, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 9.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:11 INFO spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 9.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 43 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 43 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 266 bytes\n",
      "17/01/05 15:34:11 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 266 bytes\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 16.0 (TID 41, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 39) in 1206 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 40) in 1255 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 16.0 (TID 41) in 618 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:12 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 16 (flatMap at ALS.scala:1170) finished in 1.823 s\n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 17, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(16)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[57] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(17)\n",
      "17/01/05 15:34:12 INFO spark.storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 21.3 KB, free 103.7 KB)\n",
      "17/01/05 15:34:12 INFO spark.storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KB, free 113.3 KB)\n",
      "17/01/05 15:34:12 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.143.133.19:45040 (size: 9.7 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:12 INFO apache.spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:12 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[57] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:12 INFO cluster.ego.EGODeployScheduler: Adding task set 17.0 with 3 tasks\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 42, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 43, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:12 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 17.0 (TID 44, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:13 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 9.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:13 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:13 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:13 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 242 bytes\n",
      "17/01/05 15:34:13 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 242 bytes\n",
      "17/01/05 15:34:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:13 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 43) in 639 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:13 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 17.0 (TID 44) in 651 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 42) in 1308 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 17 (flatMap at ALS.scala:1170) finished in 1.308 s\n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(17)\n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[66] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(18)\n",
      "17/01/05 15:34:14 INFO spark.storage.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.1 KB, free 135.5 KB)\n",
      "17/01/05 15:34:14 INFO spark.storage.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.9 KB, free 145.4 KB)\n",
      "17/01/05 15:34:14 INFO spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.143.133.19:45040 (size: 9.9 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:14 INFO apache.spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:14 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[66] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:14 INFO cluster.ego.EGODeployScheduler: Adding task set 18.0 with 3 tasks\n",
      "17/01/05 15:34:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 45, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 46, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:14 INFO spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 9.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:14 INFO spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 9.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 266 bytes\n",
      "17/01/05 15:34:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:14 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 18.0 (TID 47, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 46) in 606 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 45) in 618 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:15 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 18.0 (TID 47) in 593 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:15 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 18 (flatMap at ALS.scala:1170) finished in 1.200 s\n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(18)\n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[75] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(19)\n",
      "17/01/05 15:34:15 INFO spark.storage.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 23.0 KB, free 168.4 KB)\n",
      "17/01/05 15:34:15 INFO spark.storage.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 10.1 KB, free 178.5 KB)\n",
      "17/01/05 15:34:15 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.143.133.19:45040 (size: 10.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:15 INFO apache.spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:15 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[75] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:15 INFO cluster.ego.EGODeployScheduler: Adding task set 19.0 with 3 tasks\n",
      "17/01/05 15:34:15 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 48, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:15 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 49, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:15 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 19.0 (TID 50, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:15 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 10.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:15 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:15 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:15 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 242 bytes\n",
      "17/01/05 15:34:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:16 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 49) in 629 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:16 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 19.0 (TID 50) in 648 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:16 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 48) in 682 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:16 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 19 (flatMap at ALS.scala:1170) finished in 0.684 s\n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(19)\n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[84] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(20)\n",
      "17/01/05 15:34:16 INFO spark.storage.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 23.9 KB, free 202.4 KB)\n",
      "17/01/05 15:34:16 INFO spark.storage.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 10.3 KB, free 212.6 KB)\n",
      "17/01/05 15:34:16 INFO spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.143.133.19:45040 (size: 10.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:16 INFO apache.spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:16 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[84] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:16 INFO cluster.ego.EGODeployScheduler: Adding task set 20.0 with 3 tasks\n",
      "17/01/05 15:34:16 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 51, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:16 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 52, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:16 INFO spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 10.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:16 INFO spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:16 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 266 bytes\n",
      "17/01/05 15:34:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:16 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 52) in 608 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 20.0 (TID 53, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 51) in 816 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 20.0 (TID 53) in 696 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:17 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 20 (flatMap at ALS.scala:1170) finished in 1.512 s\n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(20)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[93] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(21)\n",
      "17/01/05 15:34:17 INFO spark.storage.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 24.8 KB, free 237.4 KB)\n",
      "17/01/05 15:34:17 INFO spark.storage.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.4 KB, free 247.8 KB)\n",
      "17/01/05 15:34:17 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.143.133.19:45040 (size: 10.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:17 INFO apache.spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:17 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[93] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:17 INFO cluster.ego.EGODeployScheduler: Adding task set 21.0 with 3 tasks\n",
      "17/01/05 15:34:17 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 54, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 55, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:17 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 21.0 (TID 56, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:17 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 10.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:17 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:17 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:17 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 242 bytes\n",
      "17/01/05 15:34:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:18 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 54) in 628 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:18 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 21.0 (TID 56) in 636 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:18 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 55) in 675 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:18 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 21 (flatMap at ALS.scala:1170) finished in 0.676 s\n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(21)\n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[102] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(22)\n",
      "17/01/05 15:34:18 INFO spark.storage.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 25.6 KB, free 273.4 KB)\n",
      "17/01/05 15:34:18 INFO spark.storage.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.6 KB, free 284.1 KB)\n",
      "17/01/05 15:34:18 INFO spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.143.133.19:45040 (size: 10.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:18 INFO apache.spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:18 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[102] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:18 INFO cluster.ego.EGODeployScheduler: Adding task set 22.0 with 3 tasks\n",
      "17/01/05 15:34:18 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 57, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:18 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 58, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:18 INFO spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 10.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:18 INFO spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:18 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 266 bytes\n",
      "17/01/05 15:34:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 22.0 (TID 59, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 58) in 603 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 57) in 652 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 22.0 (TID 59) in 598 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:19 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 22 (flatMap at ALS.scala:1170) finished in 1.201 s\n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:19 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(22)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[111] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:19 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(23)\n",
      "17/01/05 15:34:19 INFO spark.storage.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 26.5 KB, free 310.6 KB)\n",
      "17/01/05 15:34:19 INFO spark.storage.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 10.8 KB, free 321.4 KB)\n",
      "17/01/05 15:34:19 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.143.133.19:45040 (size: 10.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:19 INFO apache.spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:19 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[111] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:19 INFO cluster.ego.EGODeployScheduler: Adding task set 23.0 with 3 tasks\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 23.0 (TID 60, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 61, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:19 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 23.0 (TID 62, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:19 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 10.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:19 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:19 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:19 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 242 bytes\n",
      "17/01/05 15:34:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 23.0 (TID 60) in 606 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 61) in 667 ms on yp-spark-dal09-env5-0024 (2/3)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 23.0 (TID 62) in 713 ms on yp-spark-dal09-env5-0031 (3/3)\n",
      "17/01/05 15:34:20 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 23 (flatMap at ALS.scala:1170) finished in 0.714 s\n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(23)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[120] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(24)\n",
      "17/01/05 15:34:20 INFO spark.storage.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 27.4 KB, free 348.8 KB)\n",
      "17/01/05 15:34:20 INFO spark.storage.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 11.0 KB, free 359.8 KB)\n",
      "17/01/05 15:34:20 INFO spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.143.133.19:45040 (size: 11.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:20 INFO apache.spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:20 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[120] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:20 INFO cluster.ego.EGODeployScheduler: Adding task set 24.0 with 3 tasks\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 63, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 64, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:20 INFO spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:20 INFO spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 11.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:20 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 266 bytes\n",
      "17/01/05 15:34:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 24.0 (TID 65, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 64) in 584 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:20 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 63) in 604 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:21 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 24.0 (TID 65) in 574 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:21 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 24 (flatMap at ALS.scala:1170) finished in 1.159 s\n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(24)\n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[129] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(25)\n",
      "17/01/05 15:34:21 INFO spark.storage.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.3 KB, free 388.0 KB)\n",
      "17/01/05 15:34:21 INFO spark.storage.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 11.1 KB, free 399.2 KB)\n",
      "17/01/05 15:34:21 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.143.133.19:45040 (size: 11.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:21 INFO apache.spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:21 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[129] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:21 INFO cluster.ego.EGODeployScheduler: Adding task set 25.0 with 3 tasks\n",
      "17/01/05 15:34:21 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 66, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:21 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 25.0 (TID 67, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:21 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 25.0 (TID 68, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:21 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 11.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:21 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:21 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 242 bytes\n",
      "17/01/05 15:34:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 242 bytes\n",
      "17/01/05 15:34:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 25.0 (TID 68) in 616 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 25.0 (TID 67) in 627 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 66) in 679 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:22 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 25 (flatMap at ALS.scala:1170) finished in 0.679 s\n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(25)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[138] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(26)\n",
      "17/01/05 15:34:22 INFO spark.storage.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 29.2 KB, free 428.3 KB)\n",
      "17/01/05 15:34:22 INFO spark.storage.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 11.3 KB, free 439.7 KB)\n",
      "17/01/05 15:34:22 INFO spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.143.133.19:45040 (size: 11.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:22 INFO apache.spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:22 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[138] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:22 INFO cluster.ego.EGODeployScheduler: Adding task set 26.0 with 3 tasks\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 69, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 70, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:22 INFO spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 11.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:22 INFO spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:22 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 266 bytes\n",
      "17/01/05 15:34:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 26.0 (TID 71, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 70) in 579 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:22 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 69) in 614 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:23 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 26.0 (TID 71) in 581 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:23 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 26 (flatMap at ALS.scala:1170) finished in 1.160 s\n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(26)\n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[147] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(27)\n",
      "17/01/05 15:34:23 INFO spark.storage.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 30.0 KB, free 469.7 KB)\n",
      "17/01/05 15:34:23 INFO spark.storage.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 11.5 KB, free 481.2 KB)\n",
      "17/01/05 15:34:23 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.143.133.19:45040 (size: 11.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:23 INFO apache.spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:23 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[147] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:23 INFO cluster.ego.EGODeployScheduler: Adding task set 27.0 with 3 tasks\n",
      "17/01/05 15:34:23 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 72, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:23 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 73, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:23 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 27.0 (TID 74, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:23 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:23 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:23 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 11.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 242 bytes\n",
      "17/01/05 15:34:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:23 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 72) in 615 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:23 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 27.0 (TID 74) in 619 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 73) in 661 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 27 (flatMap at ALS.scala:1170) finished in 0.662 s\n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(27)\n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[156] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(28)\n",
      "17/01/05 15:34:24 INFO spark.storage.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 30.9 KB, free 512.1 KB)\n",
      "17/01/05 15:34:24 INFO spark.storage.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 11.7 KB, free 523.8 KB)\n",
      "17/01/05 15:34:24 INFO spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.143.133.19:45040 (size: 11.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:24 INFO apache.spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:24 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[156] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:24 INFO cluster.ego.EGODeployScheduler: Adding task set 28.0 with 3 tasks\n",
      "17/01/05 15:34:24 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 28.0 (TID 75, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 76, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:24 INFO spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:24 INFO spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 11.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:24 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:24 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 266 bytes\n",
      "17/01/05 15:34:24 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:24 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 28.0 (TID 75) in 596 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:24 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 28.0 (TID 77, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 76) in 608 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 28.0 (TID 77) in 581 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 28 (flatMap at ALS.scala:1170) finished in 1.191 s\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 29)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(28)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[165] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(29)\n",
      "17/01/05 15:34:25 INFO spark.storage.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 31.8 KB, free 555.5 KB)\n",
      "17/01/05 15:34:25 INFO spark.storage.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 11.8 KB, free 567.4 KB)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.143.133.19:45040 (size: 11.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:25 INFO apache.spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[165] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGODeployScheduler: Adding task set 29.0 with 3 tasks\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 78, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 29.0 (TID 79, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 80, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 11.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 242 bytes\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 78) in 608 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 29.0 (TID 79) in 615 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 80) in 672 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 29 (flatMap at ALS.scala:1170) finished in 0.672 s\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(29)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[174] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(30)\n",
      "17/01/05 15:34:25 INFO spark.storage.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.7 KB, free 600.0 KB)\n",
      "17/01/05 15:34:25 INFO spark.storage.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 12.2 KB, free 612.2 KB)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.143.133.19:45040 (size: 12.2 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:25 INFO apache.spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:25 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[174] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:25 INFO cluster.ego.EGODeployScheduler: Adding task set 30.0 with 3 tasks\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 81, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:25 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 82, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:25 INFO spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 266 bytes\n",
      "17/01/05 15:34:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:26 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 30.0 (TID 83, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:26 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 82) in 582 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:26 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 81) in 673 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 30.0 (TID 83) in 615 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 30 (flatMap at ALS.scala:1170) finished in 1.196 s\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(30)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[183] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(31)\n",
      "17/01/05 15:34:27 INFO spark.storage.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 33.5 KB, free 645.8 KB)\n",
      "17/01/05 15:34:27 INFO spark.storage.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.4 KB, free 658.1 KB)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.143.133.19:45040 (size: 12.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:27 INFO apache.spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[183] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGODeployScheduler: Adding task set 31.0 with 3 tasks\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 84, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 31.0 (TID 85, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 86, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 242 bytes\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 86) in 642 ms on yp-spark-dal09-env5-0024 (1/3)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 31.0 (TID 85) in 658 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 84) in 751 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 31 (flatMap at ALS.scala:1170) finished in 0.752 s\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(31)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[192] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(32)\n",
      "17/01/05 15:34:27 INFO spark.storage.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 34.4 KB, free 692.6 KB)\n",
      "17/01/05 15:34:27 INFO spark.storage.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 12.5 KB, free 705.1 KB)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.143.133.19:45040 (size: 12.5 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:27 INFO apache.spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:27 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[192] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:27 INFO cluster.ego.EGODeployScheduler: Adding task set 32.0 with 3 tasks\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 87, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:27 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 32.0 (TID 88, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:27 INFO spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 266 bytes\n",
      "17/01/05 15:34:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:28 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 32.0 (TID 88) in 588 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:28 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 32.0 (TID 89, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:28 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 87) in 643 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 32.0 (TID 89) in 570 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 32 (flatMap at ALS.scala:1170) finished in 1.213 s\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(32)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[201] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(33)\n",
      "17/01/05 15:34:29 INFO spark.storage.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 35.3 KB, free 740.4 KB)\n",
      "17/01/05 15:34:29 INFO spark.storage.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.7 KB, free 753.1 KB)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.143.133.19:45040 (size: 12.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:29 INFO apache.spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[201] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGODeployScheduler: Adding task set 33.0 with 3 tasks\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 33.0 (TID 90, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 91, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 92, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 242 bytes\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 33.0 (TID 90) in 622 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 92) in 629 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 91) in 655 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 33 (flatMap at ALS.scala:1170) finished in 0.656 s\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(33)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[210] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(34)\n",
      "17/01/05 15:34:29 INFO spark.storage.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 36.2 KB, free 789.3 KB)\n",
      "17/01/05 15:34:29 INFO spark.storage.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 12.9 KB, free 802.2 KB)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.143.133.19:45040 (size: 12.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:29 INFO apache.spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:29 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[210] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:29 INFO cluster.ego.EGODeployScheduler: Adding task set 34.0 with 3 tasks\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 93, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:29 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 34.0 (TID 94, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:29 INFO spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 266 bytes\n",
      "17/01/05 15:34:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:30 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 34.0 (TID 94) in 628 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:30 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 34.0 (TID 95, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:30 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 93) in 794 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 34.0 (TID 95) in 590 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 34 (flatMap at ALS.scala:1170) finished in 1.385 s\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 35, ShuffleMapStage 36)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(34)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[219] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(35)\n",
      "17/01/05 15:34:31 INFO spark.storage.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 37.1 KB, free 839.3 KB)\n",
      "17/01/05 15:34:31 INFO spark.storage.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 13.1 KB, free 852.4 KB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.143.133.19:45040 (size: 13.1 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO apache.spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[219] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGODeployScheduler: Adding task set 35.0 with 3 tasks\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 35.0 (TID 96, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 97, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 35.0 (TID 98, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 13.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 242 bytes\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 242 bytes\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 35.0 (TID 98) in 603 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 35.0 (TID 96) in 609 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 97) in 655 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 35 (flatMap at ALS.scala:1170) finished in 0.655 s\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(35)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 36)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[228] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(36)\n",
      "17/01/05 15:34:31 INFO spark.storage.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 37.9 KB, free 890.3 KB)\n",
      "17/01/05 15:34:31 INFO spark.storage.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 13.2 KB, free 903.5 KB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.143.133.19:45040 (size: 13.2 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.143.133.19:45040 in memory (size: 12.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO apache.spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:31 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[228] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:31 INFO cluster.ego.EGODeployScheduler: Adding task set 36.0 with 3 tasks\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_30_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 12.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_30_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 99, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:31 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 36.0 (TID 100, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.143.133.19:45040 in memory (size: 12.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 12.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 12.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.143.133.19:45040 in memory (size: 12.5 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 12.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 13.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.143.133.19:45040 in memory (size: 12.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 12.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 266 bytes\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.143.133.19:45040 in memory (size: 12.2 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 266 bytes\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 12.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.143.133.19:45040 in memory (size: 11.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 11.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.143.133.19:45040 in memory (size: 11.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 11.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.143.133.19:45040 in memory (size: 11.5 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 11.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.143.133.19:45040 in memory (size: 11.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 11.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.143.133.19:45040 in memory (size: 11.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 11.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.143.133.19:45040 in memory (size: 11.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 11.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.143.133.19:45040 in memory (size: 10.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 10.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.143.133.19:45040 in memory (size: 10.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 10.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.143.133.19:45040 in memory (size: 10.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 10.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.143.133.19:45040 in memory (size: 10.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 10.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.143.133.19:45040 in memory (size: 10.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 10.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.143.133.19:45040 in memory (size: 9.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 9.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 9.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.143.133.19:45040 in memory (size: 9.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 9.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.143.133.19:45040 in memory (size: 9.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 9.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 9.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.143.133.19:45040 in memory (size: 8.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 8.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.143.133.19:45040 in memory (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:31 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:32 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 36.0 (TID 100) in 577 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:32 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 36.0 (TID 101, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:32 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 99) in 589 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 36.0 (TID 101) in 579 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 36 (flatMap at ALS.scala:1170) finished in 1.168 s\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(36)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[237] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(37)\n",
      "17/01/05 15:34:33 INFO spark.storage.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 38.8 KB, free 140.1 KB)\n",
      "17/01/05 15:34:33 INFO spark.storage.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 13.4 KB, free 153.5 KB)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.143.133.19:45040 (size: 13.4 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:33 INFO apache.spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[237] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGODeployScheduler: Adding task set 37.0 with 3 tasks\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 37.0 (TID 102, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 103, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 37.0 (TID 104, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 13.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 242 bytes\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 242 bytes\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 37.0 (TID 104) in 597 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 37.0 (TID 102) in 609 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 103) in 647 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 37 (flatMap at ALS.scala:1170) finished in 0.647 s\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(37)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[246] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(38)\n",
      "17/01/05 15:34:33 INFO spark.storage.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 39.7 KB, free 193.2 KB)\n",
      "17/01/05 15:34:33 INFO spark.storage.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 13.6 KB, free 206.8 KB)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.143.133.19:45040 (size: 13.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:33 INFO apache.spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:33 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[246] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:33 INFO cluster.ego.EGODeployScheduler: Adding task set 38.0 with 3 tasks\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 38.0 (TID 105, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:33 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 106, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 13.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:33 INFO spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 266 bytes\n",
      "17/01/05 15:34:33 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 266 bytes\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 38.0 (TID 107, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 106) in 582 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 38.0 (TID 105) in 610 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 38.0 (TID 107) in 569 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:34 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 38 (flatMap at ALS.scala:1170) finished in 1.153 s\n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:34 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(38)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 39 (MapPartitionsRDD[255] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:34 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(39)\n",
      "17/01/05 15:34:34 INFO spark.storage.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 40.6 KB, free 247.4 KB)\n",
      "17/01/05 15:34:34 INFO spark.storage.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 13.8 KB, free 261.2 KB)\n",
      "17/01/05 15:34:34 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.143.133.19:45040 (size: 13.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:34 INFO apache.spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:34 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[255] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:34 INFO cluster.ego.EGODeployScheduler: Adding task set 39.0 with 3 tasks\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 39.0 (TID 108, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 39.0 (TID 109, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:34 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 110, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:34 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 13.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:34 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:34 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:34 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:34 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 242 bytes\n",
      "17/01/05 15:34:34 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:34 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:35 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 39.0 (TID 109) in 600 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:35 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 39.0 (TID 108) in 612 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:35 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 110) in 651 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:35 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 39 (flatMap at ALS.scala:1170) finished in 0.651 s\n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:35 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(39)\n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 40, ShuffleMapStage 41)\n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[264] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:35 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(40)\n",
      "17/01/05 15:34:35 INFO spark.storage.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 41.4 KB, free 302.7 KB)\n",
      "17/01/05 15:34:35 INFO spark.storage.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 14.0 KB, free 316.6 KB)\n",
      "17/01/05 15:34:35 INFO spark.storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.143.133.19:45040 (size: 14.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:35 INFO apache.spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:35 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[264] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:35 INFO cluster.ego.EGODeployScheduler: Adding task set 40.0 with 3 tasks\n",
      "17/01/05 15:34:35 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 111, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:35 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 40.0 (TID 112, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:35 INFO spark.storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:35 INFO spark.storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:35 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:35 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:35 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 266 bytes\n",
      "17/01/05 15:34:35 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 266 bytes\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 40.0 (TID 112) in 607 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 40.0 (TID 113, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 111) in 614 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 40.0 (TID 113) in 578 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:36 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 40 (flatMap at ALS.scala:1170) finished in 1.193 s\n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 41)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:36 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(40)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[273] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:36 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(41)\n",
      "17/01/05 15:34:36 INFO spark.storage.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 42.3 KB, free 359.0 KB)\n",
      "17/01/05 15:34:36 INFO spark.storage.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 14.2 KB, free 373.1 KB)\n",
      "17/01/05 15:34:36 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.143.133.19:45040 (size: 14.2 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:36 INFO apache.spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:36 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[273] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:36 INFO cluster.ego.EGODeployScheduler: Adding task set 41.0 with 3 tasks\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 114, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 41.0 (TID 115, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:36 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 41.0 (TID 116, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:36 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:36 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:36 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:36 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:36 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 242 bytes\n",
      "17/01/05 15:34:36 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:36 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 41.0 (TID 116) in 598 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 41.0 (TID 115) in 629 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 114) in 661 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:37 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 41 (flatMap at ALS.scala:1170) finished in 0.661 s\n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:37 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(41)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[282] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:37 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(42)\n",
      "17/01/05 15:34:37 INFO spark.storage.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 43.2 KB, free 416.3 KB)\n",
      "17/01/05 15:34:37 INFO spark.storage.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 14.3 KB, free 430.7 KB)\n",
      "17/01/05 15:34:37 INFO spark.storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.143.133.19:45040 (size: 14.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:37 INFO apache.spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:37 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[282] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:37 INFO cluster.ego.EGODeployScheduler: Adding task set 42.0 with 3 tasks\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 42.0 (TID 117, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 118, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:37 INFO spark.storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:37 INFO spark.storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:37 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:37 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 266 bytes\n",
      "17/01/05 15:34:37 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 42.0 (TID 119, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 118) in 588 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:37 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 42.0 (TID 117) in 597 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:38 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 42.0 (TID 119) in 597 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:38 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 42 (flatMap at ALS.scala:1170) finished in 1.186 s\n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46)\n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:38 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(42)\n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[291] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:38 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(43)\n",
      "17/01/05 15:34:38 INFO spark.storage.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 44.1 KB, free 474.7 KB)\n",
      "17/01/05 15:34:38 INFO spark.storage.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 14.5 KB, free 489.2 KB)\n",
      "17/01/05 15:34:38 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.143.133.19:45040 (size: 14.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:38 INFO apache.spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:38 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[291] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:38 INFO cluster.ego.EGODeployScheduler: Adding task set 43.0 with 3 tasks\n",
      "17/01/05 15:34:38 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 120, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:38 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 43.0 (TID 121, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:38 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 43.0 (TID 122, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:38 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:38 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:38 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:38 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 242 bytes\n",
      "17/01/05 15:34:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 43.0 (TID 122) in 613 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 43.0 (TID 121) in 636 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 120) in 658 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:39 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 43 (flatMap at ALS.scala:1170) finished in 0.659 s\n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:39 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(43)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[300] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:39 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(44)\n",
      "17/01/05 15:34:39 INFO spark.storage.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 45.0 KB, free 534.2 KB)\n",
      "17/01/05 15:34:39 INFO spark.storage.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 14.7 KB, free 548.8 KB)\n",
      "17/01/05 15:34:39 INFO spark.storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.143.133.19:45040 (size: 14.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:39 INFO apache.spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:39 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[300] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:39 INFO cluster.ego.EGODeployScheduler: Adding task set 44.0 with 3 tasks\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 123, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 44.0 (TID 124, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:39 INFO spark.storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:39 INFO spark.storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:39 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:39 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 266 bytes\n",
      "17/01/05 15:34:39 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 44.0 (TID 124) in 579 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 44.0 (TID 125, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:39 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 123) in 664 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:40 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 44.0 (TID 125) in 572 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:40 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 44 (flatMap at ALS.scala:1170) finished in 1.236 s\n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 45, ShuffleMapStage 46)\n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:40 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(44)\n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[309] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:40 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(45)\n",
      "17/01/05 15:34:40 INFO spark.storage.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 45.8 KB, free 594.7 KB)\n",
      "17/01/05 15:34:40 INFO spark.storage.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 14.8 KB, free 609.5 KB)\n",
      "17/01/05 15:34:40 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.143.133.19:45040 (size: 14.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:40 INFO apache.spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:40 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[309] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:40 INFO cluster.ego.EGODeployScheduler: Adding task set 45.0 with 3 tasks\n",
      "17/01/05 15:34:40 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 45.0 (TID 126, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:40 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 127, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:40 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 45.0 (TID 128, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:40 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:40 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:40 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:40 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:40 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 242 bytes\n",
      "17/01/05 15:34:40 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:40 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 45.0 (TID 128) in 606 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 45.0 (TID 126) in 612 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 127) in 790 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:41 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 45 (flatMap at ALS.scala:1170) finished in 0.792 s\n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 46)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:41 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(45)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 46 (MapPartitionsRDD[318] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:41 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(46)\n",
      "17/01/05 15:34:41 INFO spark.storage.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 46.7 KB, free 656.2 KB)\n",
      "17/01/05 15:34:41 INFO spark.storage.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 15.0 KB, free 671.1 KB)\n",
      "17/01/05 15:34:41 INFO spark.storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.143.133.19:45040 (size: 15.0 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:41 INFO apache.spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:41 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[318] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:41 INFO cluster.ego.EGODeployScheduler: Adding task set 46.0 with 3 tasks\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 129, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 46.0 (TID 130, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:41 INFO spark.storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:41 INFO spark.storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:41 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:41 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 266 bytes\n",
      "17/01/05 15:34:41 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 46.0 (TID 131, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 129) in 582 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:41 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 46.0 (TID 130) in 621 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:42 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 46.0 (TID 131) in 570 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:42 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 46 (flatMap at ALS.scala:1170) finished in 1.153 s\n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50)\n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:42 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(46)\n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[327] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:42 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(47)\n",
      "17/01/05 15:34:42 INFO spark.storage.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 47.6 KB, free 718.7 KB)\n",
      "17/01/05 15:34:42 INFO spark.storage.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 15.1 KB, free 733.9 KB)\n",
      "17/01/05 15:34:42 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.143.133.19:45040 (size: 15.1 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:42 INFO apache.spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:42 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[327] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:42 INFO cluster.ego.EGODeployScheduler: Adding task set 47.0 with 3 tasks\n",
      "17/01/05 15:34:42 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 47.0 (TID 132, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:42 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 133, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:42 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 47.0 (TID 134, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:42 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:42 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:42 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:42 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:42 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 242 bytes\n",
      "17/01/05 15:34:42 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:42 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 47.0 (TID 134) in 603 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 47.0 (TID 132) in 604 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 133) in 650 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:43 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 47 (flatMap at ALS.scala:1170) finished in 0.651 s\n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:43 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(47)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 48 (MapPartitionsRDD[336] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:43 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(48)\n",
      "17/01/05 15:34:43 INFO spark.storage.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 48.5 KB, free 782.3 KB)\n",
      "17/01/05 15:34:43 INFO spark.storage.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 15.3 KB, free 797.6 KB)\n",
      "17/01/05 15:34:43 INFO spark.storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.143.133.19:45040 (size: 15.3 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:43 INFO apache.spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:43 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[336] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:43 INFO cluster.ego.EGODeployScheduler: Adding task set 48.0 with 3 tasks\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 48.0 (TID 135, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 48.0 (TID 136, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:43 INFO spark.storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:43 INFO spark.storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:43 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:43 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 266 bytes\n",
      "17/01/05 15:34:43 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 48.0 (TID 137, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 48.0 (TID 135) in 579 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:43 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 48.0 (TID 136) in 590 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 48.0 (TID 137) in 572 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 48 (flatMap at ALS.scala:1170) finished in 1.152 s\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 49, ShuffleMapStage 50)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(48)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[345] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(49)\n",
      "17/01/05 15:34:44 INFO spark.storage.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.3 KB, free 846.9 KB)\n",
      "17/01/05 15:34:44 INFO spark.storage.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 15.4 KB, free 862.3 KB)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.143.133.19:45040 (size: 15.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:44 INFO apache.spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[345] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGODeployScheduler: Adding task set 49.0 with 3 tasks\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 138, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 49.0 (TID 139, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 49.0 (TID 140, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 242 bytes\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 242 bytes\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 49.0 (TID 140) in 612 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 49.0 (TID 139) in 628 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 138) in 641 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 49 (flatMap at ALS.scala:1170) finished in 0.642 s\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 50)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(49)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[354] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(50)\n",
      "17/01/05 15:34:44 INFO spark.storage.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 50.2 KB, free 912.6 KB)\n",
      "17/01/05 15:34:44 INFO spark.storage.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 15.6 KB, free 928.2 KB)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.143.133.19:45040 (size: 15.6 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:44 INFO apache.spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:44 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[354] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:44 INFO cluster.ego.EGODeployScheduler: Adding task set 50.0 with 3 tasks\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 141, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:44 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 50.0 (TID 142, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:44 INFO spark.storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 266 bytes\n",
      "17/01/05 15:34:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:45 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 50.0 (TID 142) in 579 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:45 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 50.0 (TID 143, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:45 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 141) in 601 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 50.0 (TID 143) in 593 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 50 (flatMap at ALS.scala:1170) finished in 1.194 s\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(50)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[363] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(51)\n",
      "17/01/05 15:34:46 INFO spark.storage.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 51.1 KB, free 979.3 KB)\n",
      "17/01/05 15:34:46 INFO spark.storage.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 15.7 KB, free 995.0 KB)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.143.133.19:45040 (size: 15.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:46 INFO apache.spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[363] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGODeployScheduler: Adding task set 51.0 with 3 tasks\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 144, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 51.0 (TID 145, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 51.0 (TID 146, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 242 bytes\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 51.0 (TID 145) in 619 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 51.0 (TID 146) in 627 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 144) in 657 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 51 (flatMap at ALS.scala:1170) finished in 0.659 s\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55)\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(51)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 52 (MapPartitionsRDD[372] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(52)\n",
      "17/01/05 15:34:46 INFO spark.storage.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 52.0 KB, free 1046.9 KB)\n",
      "17/01/05 15:34:46 INFO spark.storage.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 15.9 KB, free 1062.8 KB)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.143.133.19:45040 (size: 15.9 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:46 INFO apache.spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:46 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[372] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:46 INFO cluster.ego.EGODeployScheduler: Adding task set 52.0 with 3 tasks\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 52.0 (TID 147, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:46 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 52.0 (TID 148, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 15.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:46 INFO spark.storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 266 bytes\n",
      "17/01/05 15:34:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 52.0 (TID 149, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 52.0 (TID 147) in 583 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 52.0 (TID 148) in 614 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 52.0 (TID 149) in 569 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:47 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 52 (flatMap at ALS.scala:1170) finished in 1.153 s\n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(52)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 53 (MapPartitionsRDD[381] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(53)\n",
      "17/01/05 15:34:47 INFO spark.storage.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 52.9 KB, free 1115.7 KB)\n",
      "17/01/05 15:34:47 INFO spark.storage.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 16.0 KB, free 1131.7 KB)\n",
      "17/01/05 15:34:47 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.143.133.19:45040 (size: 16.0 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:47 INFO apache.spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:47 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 53 (MapPartitionsRDD[381] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:47 INFO cluster.ego.EGODeployScheduler: Adding task set 53.0 with 3 tasks\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 53.0 (TID 150, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 151, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:47 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 53.0 (TID 152, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:47 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:47 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:47 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:47 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:47 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 242 bytes\n",
      "17/01/05 15:34:47 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:47 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:48 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 53.0 (TID 150) in 611 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:48 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 151) in 640 ms on yp-spark-dal09-env5-0024 (2/3)\n",
      "17/01/05 15:34:48 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 53.0 (TID 152) in 655 ms on yp-spark-dal09-env5-0031 (3/3)\n",
      "17/01/05 15:34:48 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 53 (flatMap at ALS.scala:1170) finished in 0.656 s\n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 54, ResultStage 55)\n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(53)\n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[390] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:34:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(54)\n",
      "17/01/05 15:34:48 INFO spark.storage.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 53.7 KB, free 1185.4 KB)\n",
      "17/01/05 15:34:48 INFO spark.storage.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 16.2 KB, free 1201.7 KB)\n",
      "17/01/05 15:34:48 INFO spark.storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.143.133.19:45040 (size: 16.2 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:48 INFO apache.spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:48 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[390] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:34:48 INFO cluster.ego.EGODeployScheduler: Adding task set 54.0 with 3 tasks\n",
      "17/01/05 15:34:48 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 153, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:48 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 54.0 (TID 154, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:48 INFO spark.storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:48 INFO spark.storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 16.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:48 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 266 bytes\n",
      "17/01/05 15:34:48 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 54.0 (TID 154) in 599 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 54.0 (TID 155, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 153) in 619 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 54.0 (TID 155) in 574 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 54 (flatMap at ALS.scala:1170) finished in 1.193 s\n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 55)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:34:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(54)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 55 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255), which has no missing parents\n",
      "17/01/05 15:34:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(55)\n",
      "17/01/05 15:34:49 INFO spark.storage.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 54.9 KB, free 1256.6 KB)\n",
      "17/01/05 15:34:49 INFO spark.storage.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 16.5 KB, free 1273.1 KB)\n",
      "17/01/05 15:34:49 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.143.133.19:45040 (size: 16.5 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:49 INFO apache.spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:49 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 55 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255)\n",
      "17/01/05 15:34:49 INFO cluster.ego.EGODeployScheduler: Adding task set 55.0 with 3 tasks\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 55.0 (TID 156, yp-spark-dal09-env5-0035, partition 1,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 157, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:49 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 55.0 (TID 158, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:49 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 16.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:49 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:49 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:49 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:49 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 242 bytes\n",
      "17/01/05 15:34:49 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:49 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:34:50 INFO spark.storage.BlockManagerInfo: Added rdd_406_1 in memory on yp-spark-dal09-env5-0035:38118 (size: 898.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 55.0 (TID 156) in 644 ms on yp-spark-dal09-env5-0035 (1/3)\n",
      "17/01/05 15:34:50 INFO spark.storage.BlockManagerInfo: Added rdd_406_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 898.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 55.0 (TID 158) in 678 ms on yp-spark-dal09-env5-0031 (2/3)\n",
      "17/01/05 15:34:50 INFO spark.storage.BlockManagerInfo: Added rdd_406_0 in memory on yp-spark-dal09-env5-0024:33191 (size: 898.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 157) in 726 ms on yp-spark-dal09-env5-0024 (3/3)\n",
      "17/01/05 15:34:50 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: ResultStage 55 (count at ALS.scala:263) finished in 0.727 s\n",
      "17/01/05 15:34:50 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(55)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Job 5 finished: count at ALS.scala:263, took 39.604737 s\n",
      "17/01/05 15:34:50 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:264\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 283 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 287 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 287 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 272 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 266 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 242 bytes\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 266 bytes\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Got job 6 (count at ALS.scala:264) with 3 output partitions\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 100 (count at ALS.scala:264)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 99, ShuffleMapStage 57)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 100 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259), which has no missing parents\n",
      "17/01/05 15:34:50 INFO spark.storage.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 54.1 KB, free 1327.2 KB)\n",
      "17/01/05 15:34:50 INFO spark.storage.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 16.4 KB, free 1343.6 KB)\n",
      "17/01/05 15:34:50 INFO spark.storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.143.133.19:45040 (size: 16.4 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:50 INFO apache.spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:50 INFO spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 100 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259)\n",
      "17/01/05 15:34:50 INFO cluster.ego.EGODeployScheduler: Adding task set 100.0 with 3 tasks\n",
      "17/01/05 15:34:50 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 100.0 (TID 159, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:50 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 160, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:50 INFO spark.storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:50 INFO spark.storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:34:50 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:34:50 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(100)\n",
      "17/01/05 15:34:51 INFO spark.storage.BlockManagerInfo: Added rdd_407_1 in memory on yp-spark-dal09-env5-0031:45704 (size: 539.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 100.0 (TID 159) in 609 ms on yp-spark-dal09-env5-0031 (1/3)\n",
      "17/01/05 15:34:51 INFO spark.storage.BlockManagerInfo: Added rdd_407_0 in memory on yp-spark-dal09-env5-0035:38118 (size: 539.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 100.0 (TID 161, yp-spark-dal09-env5-0035, partition 2,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 160) in 715 ms on yp-spark-dal09-env5-0035 (2/3)\n",
      "17/01/05 15:34:51 INFO spark.storage.BlockManagerInfo: Added rdd_407_2 in memory on yp-spark-dal09-env5-0035:38118 (size: 539.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 100.0 (TID 161) in 600 ms on yp-spark-dal09-env5-0035 (3/3)\n",
      "17/01/05 15:34:51 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 100.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: ResultStage 100 (count at ALS.scala:264) finished in 1.315 s\n",
      "17/01/05 15:34:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(100)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Job 6 finished: count at ALS.scala:264, took 1.351166 s\n",
      "17/01/05 15:34:51 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Got job 7 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 146 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 104, ShuffleMapStage 145)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 146 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255), which has no missing parents\n",
      "17/01/05 15:34:51 INFO spark.storage.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 55.1 KB, free 1398.7 KB)\n",
      "17/01/05 15:34:51 INFO spark.storage.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 16.6 KB, free 1415.3 KB)\n",
      "17/01/05 15:34:51 INFO spark.storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.143.133.19:45040 (size: 16.6 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:51 INFO apache.spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 146 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255)\n",
      "17/01/05 15:34:51 INFO cluster.ego.EGODeployScheduler: Adding task set 146.0 with 1 tasks\n",
      "17/01/05 15:34:51 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 146.0 (TID 162, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:51 INFO spark.storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(146)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 146.0 (TID 162) in 25 ms on yp-spark-dal09-env5-0024 (1/1)\n",
      "17/01/05 15:34:51 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 146.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: ResultStage 146 (first at MatrixFactorizationModel.scala:67) finished in 0.025 s\n",
      "17/01/05 15:34:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(146)\n",
      "17/01/05 15:34:51 INFO spark.scheduler.DAGScheduler: Job 7 finished: first at MatrixFactorizationModel.scala:67, took 0.050181 s\n",
      "17/01/05 15:34:51 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Got job 8 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 191 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 190, ShuffleMapStage 148)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 191 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259), which has no missing parents\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 54.2 KB, free 1469.5 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.4 KB, free 1485.9 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.143.133.19:45040 (size: 16.4 KB, free: 908.6 MB)\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 191 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Adding task set 191.0 with 1 tasks\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 191.0 (TID 163, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(191)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 191.0 (TID 163) in 13 ms on yp-spark-dal09-env5-0035 (1/1)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 191.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: ResultStage 191 (first at MatrixFactorizationModel.scala:67) finished in 0.013 s\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(191)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Job 8 finished: first at MatrixFactorizationModel.scala:67, took 0.032944 s\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.143.133.19:45040 in memory (size: 16.4 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_54_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 65\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Got job 9 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 237 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 195, ShuffleMapStage 236)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.143.133.19:45040 in memory (size: 16.6 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 237 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255), which has no missing parents\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_53_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 64\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.143.133.19:45040 in memory (size: 16.4 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_52_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 55.1 KB, free 1328.2 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_52_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 63\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 16.6 KB, free 1344.8 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.143.133.19:45040 (size: 16.6 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 237 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Adding task set 237.0 with 1 tasks\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.143.133.19:45040 in memory (size: 16.5 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 16.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 237.0 (TID 164, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 62\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.143.133.19:45040 in memory (size: 16.2 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_50_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 16.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_50_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 61\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.143.133.19:45040 in memory (size: 16.0 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 60\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.143.133.19:45040 in memory (size: 15.9 KB, free: 908.7 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_48_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_48_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(237)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 59\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.143.133.19:45040 in memory (size: 15.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 58\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 237.0 (TID 164) in 14 ms on yp-spark-dal09-env5-0024 (1/1)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 237.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.143.133.19:45040 in memory (size: 15.6 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: ResultStage 237 (first at MatrixFactorizationModel.scala:67) finished in 0.014 s\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(237)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_46_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_46_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 57\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Job 9 finished: first at MatrixFactorizationModel.scala:67, took 0.061478 s\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.143.133.19:45040 in memory (size: 15.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 56\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.143.133.19:45040 in memory (size: 15.3 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_44_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_44_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 55\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.143.133.19:45040 in memory (size: 15.1 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 54\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.143.133.19:45040 in memory (size: 15.0 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_42_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 15.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_42_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 53\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.143.133.19:45040 in memory (size: 14.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 52\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.143.133.19:45040 in memory (size: 14.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_40_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 14.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_40_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 51\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.143.133.19:45040 in memory (size: 14.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 14.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 50\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.143.133.19:45040 in memory (size: 14.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_38_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 14.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_38_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 49\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Got job 10 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 282 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 281, ShuffleMapStage 239)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.143.133.19:45040 in memory (size: 14.2 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 282 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259), which has no missing parents\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 14.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 48\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.143.133.19:45040 in memory (size: 14.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_36_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 14.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 54.2 KB, free 387.2 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_36_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 47\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 16.4 KB, free 403.6 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.143.133.19:45040 (size: 16.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.143.133.19:45040 in memory (size: 13.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 282 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Adding task set 282.0 with 1 tasks\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 13.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 282.0 (TID 165, yp-spark-dal09-env5-0035, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 46\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.143.133.19:45040 in memory (size: 13.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_34_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 13.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_34_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 45\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.143.133.19:45040 in memory (size: 13.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 13.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 44\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.143.133.19:45040 in memory (size: 13.2 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(282)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_32_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 13.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_32_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 43\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.143.133.19:45040 in memory (size: 13.1 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 13.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 282.0 (TID 165) in 11 ms on yp-spark-dal09-env5-0035 (1/1)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 282.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: ResultStage 282 (first at MatrixFactorizationModel.scala:67) finished in 0.012 s\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 42\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(282)\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 41\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 40\n",
      "17/01/05 15:34:52 INFO apache.spark.ContextCleaner: Cleaned accumulator 39\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Job 10 finished: first at MatrixFactorizationModel.scala:67, took 0.035803 s\n",
      "17/01/05 15:34:52 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:34:52 CST]\n",
      "17/01/05 15:34:52 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:34:52 CST]\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Starting job: runJob at PythonRDD.scala:393\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Registering RDD 411 (flatMap at MatrixFactorizationModel.scala:278)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Got job 11 (runJob at PythonRDD.scala:393) with 1 output partitions\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 372 (runJob at PythonRDD.scala:393)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 371)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 371)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 371 (MapPartitionsRDD[411] at flatMap at MatrixFactorizationModel.scala:278), which has no missing parents\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 58.4 KB, free 200.8 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 17.9 KB, free 218.7 KB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.143.133.19:45040 (size: 17.9 KB, free: 909.0 MB)\n",
      "17/01/05 15:34:52 INFO apache.spark.SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:34:52 INFO spark.scheduler.DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 371 (MapPartitionsRDD[411] at flatMap at MatrixFactorizationModel.scala:278)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGODeployScheduler: Adding task set 371.0 with 9 tasks\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Starting task 6.0 in stage 371.0 (TID 166, yp-spark-dal09-env5-0031, partition 6,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 371.0 (TID 167, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:52 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 371.0 (TID 168, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:34:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(371)\n",
      "17/01/05 15:34:54 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 371.0 (TID 169, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:54 INFO spark.scheduler.TaskSetManager: Starting task 7.0 in stage 371.0 (TID 170, yp-spark-dal09-env5-0031, partition 7,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:54 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 371.0 (TID 171, yp-spark-dal09-env5-0035, partition 4,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:54 INFO spark.scheduler.TaskSetManager: Starting task 5.0 in stage 371.0 (TID 172, yp-spark-dal09-env5-0035, partition 5,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:55 INFO spark.scheduler.TaskSetManager: Starting task 8.0 in stage 371.0 (TID 173, yp-spark-dal09-env5-0031, partition 8,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:55 INFO spark.scheduler.TaskSetManager: Finished task 6.0 in stage 371.0 (TID 166) in 2769 ms on yp-spark-dal09-env5-0031 (1/9)\n",
      "17/01/05 15:34:55 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 371.0 (TID 174, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:34:55 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 371.0 (TID 167) in 3368 ms on yp-spark-dal09-env5-0024 (2/9)\n",
      "17/01/05 15:34:55 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 371.0 (TID 168) in 3607 ms on yp-spark-dal09-env5-0035 (3/9)\n",
      "17/01/05 15:34:56 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 371.0 (TID 169) in 1820 ms on yp-spark-dal09-env5-0024 (4/9)\n",
      "17/01/05 15:34:56 INFO spark.scheduler.TaskSetManager: Finished task 5.0 in stage 371.0 (TID 172) in 1994 ms on yp-spark-dal09-env5-0035 (5/9)\n",
      "17/01/05 15:34:56 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 371.0 (TID 171) in 2159 ms on yp-spark-dal09-env5-0035 (6/9)\n",
      "17/01/05 15:34:57 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 371.0 (TID 174) in 1796 ms on yp-spark-dal09-env5-0024 (7/9)\n",
      "17/01/05 15:34:57 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0022:60288) with ID a4f69d43-b5b8-41e1-853f-c73e73b084eb\n",
      "17/01/05 15:34:57 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0022:42616 with 4.3 GB RAM, BlockManagerId(a4f69d43-b5b8-41e1-853f-c73e73b084eb, yp-spark-dal09-env5-0022, 42616)\n",
      "17/01/05 15:34:57 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0025:36366) with ID 3639e19a-0672-4d91-9a41-3fb77f9fe245\n",
      "17/01/05 15:34:57 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0025:36914 with 4.3 GB RAM, BlockManagerId(3639e19a-0672-4d91-9a41-3fb77f9fe245, yp-spark-dal09-env5-0025, 36914)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Finished task 8.0 in stage 371.0 (TID 173) in 26018 ms on yp-spark-dal09-env5-0031 (8/9)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Finished task 7.0 in stage 371.0 (TID 170) in 26812 ms on yp-spark-dal09-env5-0031 (9/9)\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 371.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 371 (flatMap at MatrixFactorizationModel.scala:278) finished in 28.788 s\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 372)\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(371)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 372 (PythonRDD[417] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(372)\n",
      "17/01/05 15:35:21 INFO spark.storage.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 60.5 KB, free 279.2 KB)\n",
      "17/01/05 15:35:21 INFO spark.storage.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 18.9 KB, free 298.1 KB)\n",
      "17/01/05 15:35:21 INFO spark.storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.143.133.19:45040 (size: 18.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:35:21 INFO apache.spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 372 (PythonRDD[417] at RDD at PythonRDD.scala:43)\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGODeployScheduler: Adding task set 372.0 with 1 tasks\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 372.0 (TID 175, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:21 INFO spark.storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 18.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 45 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 45 is 277 bytes\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 372.0 (TID 175) in 187 ms on yp-spark-dal09-env5-0024 (1/1)\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 372.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: ResultStage 372 (runJob at PythonRDD.scala:393) finished in 0.188 s\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(372)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:393, took 29.012896 s\n",
      "17/01/05 15:35:21 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:35:21 CST]\n",
      "17/01/05 15:35:21 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483650939]\n",
      "17/01/05 15:35:21 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483652121]\n",
      "17/01/05 15:35:21 INFO apache.spark.SparkContext: Starting job: collect at <ipython-input-6-eaf6e8f5ec75>:119\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 283 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 287 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 287 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 272 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 242 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 266 bytes\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 242 bytes\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Got job 12 (collect at <ipython-input-6-eaf6e8f5ec75>:119) with 9 output partitions\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 462 (collect at <ipython-input-6-eaf6e8f5ec75>:119)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 461)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 462 (PythonRDD[427] at collect at <ipython-input-6-eaf6e8f5ec75>:119), which has no missing parents\n",
      "17/01/05 15:35:21 INFO spark.storage.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 71.9 KB, free 370.0 KB)\n",
      "17/01/05 15:35:21 INFO spark.storage.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 24.7 KB, free 394.7 KB)\n",
      "17/01/05 15:35:21 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.143.133.19:45040 (size: 24.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:35:21 INFO apache.spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:35:21 INFO spark.scheduler.DAGScheduler: Submitting 9 missing tasks from ResultStage 462 (PythonRDD[427] at collect at <ipython-input-6-eaf6e8f5ec75>:119)\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGODeployScheduler: Adding task set 462.0 with 9 tasks\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 462.0 (TID 176, yp-spark-dal09-env5-0035, partition 1,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 462.0 (TID 177, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:21 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 462.0 (TID 178, yp-spark-dal09-env5-0031, partition 2,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:21 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 24.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:21 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 24.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:35:21 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 24.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(462)\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 45 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:35:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 45 to yp-spark-dal09-env5-0035:35714\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_1 in memory on yp-spark-dal09-env5-0035:38118 (size: 276.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 275.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_0 in memory on yp-spark-dal09-env5-0024:33191 (size: 275.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 462.0 (TID 179, yp-spark-dal09-env5-0035, partition 4,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 462.0 (TID 176) in 568 ms on yp-spark-dal09-env5-0035 (1/9)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 462.0 (TID 180, yp-spark-dal09-env5-0024, partition 3,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 462.0 (TID 177) in 608 ms on yp-spark-dal09-env5-0024 (2/9)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Starting task 5.0 in stage 462.0 (TID 181, yp-spark-dal09-env5-0031, partition 5,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 462.0 (TID 178) in 640 ms on yp-spark-dal09-env5-0031 (3/9)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_4 in memory on yp-spark-dal09-env5-0035:38118 (size: 276.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_3 in memory on yp-spark-dal09-env5-0024:33191 (size: 276.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_5 in memory on yp-spark-dal09-env5-0031:45704 (size: 275.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Starting task 7.0 in stage 462.0 (TID 182, yp-spark-dal09-env5-0035, partition 7,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 462.0 (TID 179) in 253 ms on yp-spark-dal09-env5-0035 (4/9)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Starting task 8.0 in stage 462.0 (TID 183, yp-spark-dal09-env5-0031, partition 8,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 5.0 in stage 462.0 (TID 181) in 271 ms on yp-spark-dal09-env5-0031 (5/9)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Starting task 6.0 in stage 462.0 (TID 184, yp-spark-dal09-env5-0024, partition 6,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 462.0 (TID 180) in 315 ms on yp-spark-dal09-env5-0024 (6/9)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_7 in memory on yp-spark-dal09-env5-0035:38118 (size: 275.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_6 in memory on yp-spark-dal09-env5-0024:33191 (size: 275.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:22 INFO spark.storage.BlockManagerInfo: Added rdd_423_8 in memory on yp-spark-dal09-env5-0031:45704 (size: 275.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:35:22 INFO spark.scheduler.TaskSetManager: Finished task 7.0 in stage 462.0 (TID 182) in 290 ms on yp-spark-dal09-env5-0035 (7/9)\n",
      "17/01/05 15:35:23 INFO spark.scheduler.TaskSetManager: Finished task 6.0 in stage 462.0 (TID 184) in 305 ms on yp-spark-dal09-env5-0024 (8/9)\n",
      "17/01/05 15:35:23 INFO spark.scheduler.TaskSetManager: Finished task 8.0 in stage 462.0 (TID 183) in 315 ms on yp-spark-dal09-env5-0031 (9/9)\n",
      "17/01/05 15:35:23 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 462.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:35:23 INFO spark.scheduler.DAGScheduler: ResultStage 462 (collect at <ipython-input-6-eaf6e8f5ec75>:119) finished in 1.228 s\n",
      "17/01/05 15:35:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(462)\n",
      "17/01/05 15:35:23 INFO spark.scheduler.DAGScheduler: Job 12 finished: collect at <ipython-input-6-eaf6e8f5ec75>:119, took 1.258814 s\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:35:23 CST]\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:35:24 CST]\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:35:24 CST]\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:35:24 CST]\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:35:24 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:35:25 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:35:26 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:35:27 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:35:28 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:35:29 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:35:30 CST]\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:35:31 CST]\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:35:31 CST]\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:35:31 CST]\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483652121]\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483652121, 2017-01-05 15:35:31 CST]\n",
      "17/01/05 15:40:13 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:40:13 CST]\n",
      "Use connectorVersion=1.6.4, dbName=ratingdb, indexName=null, viewName=null,jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=-1\n",
      "[WARN] [01/05/2017 15:40:13.376] [Thread-7] [JsonStoreDataAccess(akka://CloudantSpark-b78eae3e-c762-4fcc-abab-0c377be53fd2)] Loading data from Cloudant using query: https://9aefd1f0-d288-4666-a12f-abd93ee724fc-bluemix.cloudant.com/ratingdb/_all_docs?limit=1\n",
      "17/01/05 15:40:13 INFO spark.common.JsonStoreRDD: Partition config - total=5, limit=200002 for totalRows of 1000009\n",
      "17/01/05 15:40:13 INFO apache.spark.SparkContext: Starting job: json at DefaultSource.scala:130\n",
      "17/01/05 15:40:13 INFO spark.scheduler.DAGScheduler: Got job 13 (json at DefaultSource.scala:130) with 5 output partitions\n",
      "17/01/05 15:40:13 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 463 (json at DefaultSource.scala:130)\n",
      "17/01/05 15:40:13 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n",
      "17/01/05 15:40:13 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:40:13 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 463 (MapPartitionsRDD[430] at json at DefaultSource.scala:130), which has no missing parents\n",
      "17/01/05 15:40:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(463)\n",
      "17/01/05 15:40:13 INFO spark.storage.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 4.1 KB, free 398.8 KB)\n",
      "17/01/05 15:40:13 INFO spark.storage.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 2.5 KB, free 401.3 KB)\n",
      "17/01/05 15:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.143.133.19:45040 (size: 2.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:40:13 INFO apache.spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:40:13 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 463 (MapPartitionsRDD[430] at json at DefaultSource.scala:130)\n",
      "17/01/05 15:40:13 INFO cluster.ego.EGODeployScheduler: Adding task set 463.0 with 5 tasks\n",
      "17/01/05 15:40:13 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 463.0 (TID 185, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:40:13 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 463.0 (TID 186, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:40:13 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 463.0 (TID 187, yp-spark-dal09-env5-0025, partition 2,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:40:13 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 463.0 (TID 188, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:40:13 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 463.0 (TID 189, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 2.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:40:37 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 463.0 (TID 185) in 23610 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:40:43 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 463.0 (TID 186) in 30105 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:41:01 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 463.0 (TID 187) in 48405 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:41:05 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 463.0 (TID 188) in 52459 ms on yp-spark-dal09-env5-0035 (4/5)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 463.0 (TID 189) in 62744 ms on yp-spark-dal09-env5-0024 (5/5)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: ResultStage 463 (json at DefaultSource.scala:130) finished in 62.746 s\n",
      "17/01/05 15:41:16 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 463.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:41:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(463)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Job 13 finished: json at DefaultSource.scala:130, took 62.750277 s\n",
      "17/01/05 15:41:16 INFO spark.storage.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 220.7 KB, free 622.0 KB)\n",
      "17/01/05 15:41:16 INFO spark.storage.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 19.3 KB, free 641.3 KB)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.143.133.19:45040 (size: 19.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:41:16 INFO apache.spark.SparkContext: Created broadcast 61 from rdd at DefaultSource.scala:54\n",
      "17/01/05 15:41:16 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:41:16 CST]\n",
      "17/01/05 15:41:16 INFO apache.spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:-2\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Registering RDD 439 (count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Got job 14 (count at NativeMethodAccessorImpl.java:-2) with 1 output partitions\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 465 (count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 464)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 464)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 464 (MapPartitionsRDD[439] at count at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "17/01/05 15:41:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(464)\n",
      "17/01/05 15:41:16 INFO spark.storage.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 14.9 KB, free 656.2 KB)\n",
      "17/01/05 15:41:16 INFO spark.storage.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 7.1 KB, free 663.3 KB)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.143.133.19:45040 (size: 7.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:41:16 INFO apache.spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:41:16 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 464 (MapPartitionsRDD[439] at count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:41:16 INFO cluster.ego.EGODeployScheduler: Adding task set 464.0 with 5 tasks\n",
      "17/01/05 15:41:16 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 464.0 (TID 190, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 464.0 (TID 191, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 464.0 (TID 192, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 464.0 (TID 193, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:41:16 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 464.0 (TID 194, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 7.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:41:16 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:41:35 INFO spark.storage.BlockManagerInfo: Added rdd_436_0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.9 MB, free: 4.3 GB)\n",
      "17/01/05 15:41:35 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 464.0 (TID 190) in 19061 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:41:59 INFO spark.storage.BlockManagerInfo: Added rdd_436_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.2 MB, free: 4.2 GB)\n",
      "17/01/05 15:41:59 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 464.0 (TID 192) in 43296 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:42:10 INFO spark.storage.BlockManagerInfo: Added rdd_436_3 in memory on yp-spark-dal09-env5-0035:38118 (size: 12.4 MB, free: 4.2 GB)\n",
      "17/01/05 15:42:10 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 464.0 (TID 193) in 53847 ms on yp-spark-dal09-env5-0035 (3/5)\n",
      "17/01/05 15:42:20 INFO spark.storage.BlockManagerInfo: Added rdd_436_4 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.3 MB, free: 4.3 GB)\n",
      "17/01/05 15:42:20 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 464.0 (TID 194) in 64161 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:42:46 INFO spark.storage.BlockManagerInfo: Added rdd_436_1 in memory on yp-spark-dal09-env5-0025:36914 (size: 12.3 MB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 464.0 (TID 191) in 90732 ms on yp-spark-dal09-env5-0025 (5/5)\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 464.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 464 (count at NativeMethodAccessorImpl.java:-2) finished in 90.732 s\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 465)\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(464)\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 465 (MapPartitionsRDD[442] at count at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(465)\n",
      "17/01/05 15:42:47 INFO spark.storage.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 9.3 KB, free 672.5 KB)\n",
      "17/01/05 15:42:47 INFO spark.storage.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 4.6 KB, free 677.1 KB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.143.133.19:45040 (size: 4.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO apache.spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 465 (MapPartitionsRDD[442] at count at NativeMethodAccessorImpl.java:-2)\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGODeployScheduler: Adding task set 465.0 with 1 tasks\n",
      "17/01/05 15:42:47 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 465.0 (TID 195, yp-spark-dal09-env5-0025, partition 0,NODE_LOCAL, 1999 bytes)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 4.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 46 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:47 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 46 is 316 bytes\n",
      "17/01/05 15:42:47 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 465.0 (TID 195) in 141 ms on yp-spark-dal09-env5-0025 (1/1)\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 465.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: ResultStage 465 (count at NativeMethodAccessorImpl.java:-2) finished in 0.142 s\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Job 14 finished: count at NativeMethodAccessorImpl.java:-2, took 90.882560 s\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(465)\n",
      "17/01/05 15:42:47 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\n",
      "17/01/05 15:42:47 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:42:47 CST]\n",
      "17/01/05 15:42:47 INFO apache.spark.SparkContext: Starting job: runJob at PythonRDD.scala:393\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Got job 15 (runJob at PythonRDD.scala:393) with 1 output partitions\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 466 (runJob at PythonRDD.scala:393)\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 466 (PythonRDD[446] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(466)\n",
      "17/01/05 15:42:47 INFO spark.storage.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 13.4 KB, free 690.5 KB)\n",
      "17/01/05 15:42:47 INFO spark.storage.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 7.1 KB, free 697.6 KB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.143.133.19:45040 (size: 7.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO apache.spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:47 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 466 (PythonRDD[446] at RDD at PythonRDD.scala:43)\n",
      "17/01/05 15:42:47 INFO cluster.ego.EGODeployScheduler: Adding task set 466.0 with 1 tasks\n",
      "17/01/05 15:42:47 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 466.0 (TID 196, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2744 bytes)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.143.133.19:45040 in memory (size: 4.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_63_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 4.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 83\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.143.133.19:45040 in memory (size: 7.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_62_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 7.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_62_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 7.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_62_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_62_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_62_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 82\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned shuffle 46\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 81\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 80\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 79\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 78\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 77\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 76\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 75\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 74\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.143.133.19:45040 in memory (size: 19.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.143.133.19:45040 in memory (size: 2.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_60_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 2.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_60_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_60_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 2.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_60_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_60_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 2.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 72\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.143.133.19:45040 in memory (size: 24.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_59_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 24.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_59_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 24.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_59_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 24.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 71\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.143.133.19:45040 in memory (size: 18.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_58_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 18.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 69\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.143.133.19:45040 in memory (size: 17.9 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 68\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.143.133.19:45040 in memory (size: 16.4 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_56_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 67\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.143.133.19:45040 in memory (size: 16.6 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:47 INFO spark.storage.BlockManagerInfo: Removed broadcast_55_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:47 INFO apache.spark.ContextCleaner: Cleaned accumulator 66\n",
      "17/01/05 15:42:48 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 466.0 (TID 196) in 845 ms on yp-spark-dal09-env5-0022 (1/1)\n",
      "17/01/05 15:42:48 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 466.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: ResultStage 466 (runJob at PythonRDD.scala:393) finished in 0.845 s\n",
      "17/01/05 15:42:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(466)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:393, took 0.849157 s\n",
      "17/01/05 15:42:48 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:596\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Registering RDD 450 (mapPartitions at ALS.scala:837)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Registering RDD 453 (map at ALS.scala:1080)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Got job 16 (count at ALS.scala:596) with 5 output partitions\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 469 (count at ALS.scala:596)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 468)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 468)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 467 (MapPartitionsRDD[450] at mapPartitions at ALS.scala:837), which has no missing parents\n",
      "17/01/05 15:42:48 INFO spark.storage.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 15.4 KB, free 35.9 KB)\n",
      "17/01/05 15:42:48 INFO spark.storage.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 7.8 KB, free 43.7 KB)\n",
      "17/01/05 15:42:48 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.143.133.19:45040 (size: 7.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(467)\n",
      "17/01/05 15:42:48 INFO apache.spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:48 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 467 (MapPartitionsRDD[450] at mapPartitions at ALS.scala:837)\n",
      "17/01/05 15:42:48 INFO cluster.ego.EGODeployScheduler: Adding task set 467.0 with 5 tasks\n",
      "17/01/05 15:42:48 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 467.0 (TID 197, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 467.0 (TID 198, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 467.0 (TID 199, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 467.0 (TID 200, yp-spark-dal09-env5-0035, partition 3,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:42:48 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 467.0 (TID 201, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2733 bytes)\n",
      "17/01/05 15:42:48 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:48 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0035:38118 (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:48 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:48 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:48 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:50 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 467.0 (TID 201) in 2631 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:42:50 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 467.0 (TID 200) in 2671 ms on yp-spark-dal09-env5-0035 (2/5)\n",
      "17/01/05 15:42:50 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 467.0 (TID 199) in 2693 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:50 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 467.0 (TID 197) in 2906 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 467.0 (TID 198) in 3417 ms on yp-spark-dal09-env5-0025 (5/5)\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 467.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 467 (mapPartitions at ALS.scala:837) finished in 3.418 s\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 468, ResultStage 469)\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(467)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 468 (MapPartitionsRDD[453] at map at ALS.scala:1080), which has no missing parents\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(468)\n",
      "17/01/05 15:42:51 INFO spark.storage.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 16.5 KB, free 60.2 KB)\n",
      "17/01/05 15:42:51 INFO spark.storage.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 8.3 KB, free 68.5 KB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.143.133.19:45040 (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:51 INFO apache.spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 468 (MapPartitionsRDD[453] at map at ALS.scala:1080)\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGODeployScheduler: Adding task set 468.0 with 5 tasks\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 468.0 (TID 202, yp-spark-dal09-env5-0031, partition 1,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 468.0 (TID 203, yp-spark-dal09-env5-0022, partition 2,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 468.0 (TID 204, yp-spark-dal09-env5-0025, partition 0,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 468.0 (TID 205, yp-spark-dal09-env5-0024, partition 3,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 48 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 48 is 332 bytes\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 48 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 48 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 48 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_452_1 in memory on yp-spark-dal09-env5-0031:45704 (size: 3.2 MB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_452_3 in memory on yp-spark-dal09-env5-0024:33191 (size: 2.7 MB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_452_0 in memory on yp-spark-dal09-env5-0025:36914 (size: 1407.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 468.0 (TID 206, yp-spark-dal09-env5-0031, partition 4,NODE_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 468.0 (TID 202) in 199 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_452_2 in memory on yp-spark-dal09-env5-0022:42616 (size: 3.2 MB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_452_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 927.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 468.0 (TID 205) in 229 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 468.0 (TID 206) in 55 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 468.0 (TID 204) in 322 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 468.0 (TID 203) in 422 ms on yp-spark-dal09-env5-0022 (5/5)\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 468.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 468 (map at ALS.scala:1080) finished in 0.422 s\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 469)\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(468)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 469 (userOutBlocks MapPartitionsRDD[456] at mapValues at ALS.scala:1117), which has no missing parents\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(469)\n",
      "17/01/05 15:42:51 INFO spark.storage.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 17.1 KB, free 85.6 KB)\n",
      "17/01/05 15:42:51 INFO spark.storage.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 8.5 KB, free 94.1 KB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.143.133.19:45040 (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:51 INFO apache.spark.SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:51 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 469 (userOutBlocks MapPartitionsRDD[456] at mapValues at ALS.scala:1117)\n",
      "17/01/05 15:42:51 INFO cluster.ego.EGODeployScheduler: Adding task set 469.0 with 5 tasks\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 469.0 (TID 207, yp-spark-dal09-env5-0022, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 469.0 (TID 208, yp-spark-dal09-env5-0025, partition 3,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 469.0 (TID 209, yp-spark-dal09-env5-0024, partition 1,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 469.0 (TID 210, yp-spark-dal09-env5-0031, partition 2,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 47 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 47 is 319 bytes\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 47 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 47 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:51 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 47 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_455_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 1572.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_456_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 23.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_455_1 in memory on yp-spark-dal09-env5-0024:33191 (size: 1601.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 469.0 (TID 211, yp-spark-dal09-env5-0031, partition 4,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 469.0 (TID 210) in 62 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:51 INFO spark.storage.BlockManagerInfo: Added rdd_456_1 in memory on yp-spark-dal09-env5-0024:33191 (size: 23.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:51 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 469.0 (TID 209) in 65 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_455_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 1565.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_456_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 23.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 469.0 (TID 211) in 57 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_455_0 in memory on yp-spark-dal09-env5-0022:42616 (size: 1559.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_455_3 in memory on yp-spark-dal09-env5-0025:36914 (size: 1561.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_456_0 in memory on yp-spark-dal09-env5-0022:42616 (size: 23.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 469.0 (TID 207) in 337 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_456_3 in memory on yp-spark-dal09-env5-0025:36914 (size: 23.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 469.0 (TID 208) in 364 ms on yp-spark-dal09-env5-0025 (5/5)\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 469.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: ResultStage 469 (count at ALS.scala:596) finished in 0.365 s\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(469)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Job 16 finished: count at ALS.scala:596, took 4.219643 s\n",
      "17/01/05 15:42:52 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:604\n",
      "17/01/05 15:42:52 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 48 is 332 bytes\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Registering RDD 458 (map at ALS.scala:1080)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Got job 17 (count at ALS.scala:604) with 5 output partitions\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 472 (count at ALS.scala:604)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 471)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 471)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 471 (MapPartitionsRDD[458] at map at ALS.scala:1080), which has no missing parents\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(471)\n",
      "17/01/05 15:42:52 INFO spark.storage.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 16.7 KB, free 110.8 KB)\n",
      "17/01/05 15:42:52 INFO spark.storage.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 8.3 KB, free 119.1 KB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.143.133.19:45040 (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:52 INFO apache.spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 471 (MapPartitionsRDD[458] at map at ALS.scala:1080)\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGODeployScheduler: Adding task set 471.0 with 5 tasks\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 471.0 (TID 212, yp-spark-dal09-env5-0024, partition 3,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 471.0 (TID 213, yp-spark-dal09-env5-0025, partition 0,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 471.0 (TID 214, yp-spark-dal09-env5-0022, partition 2,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 471.0 (TID 215, yp-spark-dal09-env5-0031, partition 1,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 471.0 (TID 213) in 67 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 471.0 (TID 212) in 103 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 471.0 (TID 216, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 471.0 (TID 215) in 132 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 471.0 (TID 216) in 36 ms on yp-spark-dal09-env5-0031 (4/5)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 471.0 (TID 214) in 173 ms on yp-spark-dal09-env5-0022 (5/5)\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 471.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 471 (map at ALS.scala:1080) finished in 0.174 s\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 472)\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(471)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 472 (itemOutBlocks MapPartitionsRDD[461] at mapValues at ALS.scala:1117), which has no missing parents\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(472)\n",
      "17/01/05 15:42:52 INFO spark.storage.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 17.3 KB, free 136.4 KB)\n",
      "17/01/05 15:42:52 INFO spark.storage.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 8.5 KB, free 144.9 KB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.143.133.19:45040 (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:52 INFO apache.spark.SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 472 (itemOutBlocks MapPartitionsRDD[461] at mapValues at ALS.scala:1117)\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGODeployScheduler: Adding task set 472.0 with 5 tasks\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 472.0 (TID 217, yp-spark-dal09-env5-0025, partition 1,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 472.0 (TID 218, yp-spark-dal09-env5-0031, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 472.0 (TID 219, yp-spark-dal09-env5-0024, partition 2,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 472.0 (TID 220, yp-spark-dal09-env5-0022, partition 3,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 49 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:52 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 49 is 317 bytes\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 49 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 49 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:52 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 49 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_460_0 in memory on yp-spark-dal09-env5-0031:45704 (size: 1574.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_461_0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 472.0 (TID 221, yp-spark-dal09-env5-0031, partition 4,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 472.0 (TID 218) in 222 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_460_2 in memory on yp-spark-dal09-env5-0024:33191 (size: 1541.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_461_2 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 472.0 (TID 219) in 249 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_460_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 1476.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_461_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 472.0 (TID 221) in 198 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_460_1 in memory on yp-spark-dal09-env5-0025:36914 (size: 1576.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_461_1 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 472.0 (TID 217) in 427 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_460_3 in memory on yp-spark-dal09-env5-0022:42616 (size: 1674.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.storage.BlockManagerInfo: Added rdd_461_3 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 472.0 (TID 220) in 467 ms on yp-spark-dal09-env5-0022 (5/5)\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 472.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: ResultStage 472 (count at ALS.scala:604) finished in 0.468 s\n",
      "17/01/05 15:42:52 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(472)\n",
      "17/01/05 15:42:52 INFO spark.scheduler.DAGScheduler: Job 17 finished: count at ALS.scala:604, took 0.650824 s\n",
      "17/01/05 15:42:53 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:263\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 48 is 332 bytes\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 47 is 319 bytes\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 462 (map at ALS.scala:752)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 467 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 476 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 485 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 494 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 503 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 512 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 521 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 530 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 539 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 548 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 557 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 566 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 575 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 584 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 593 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 602 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 611 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 620 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 629 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 638 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 647 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 656 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 665 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 674 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 683 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 692 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 701 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 710 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 719 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 728 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 737 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 746 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 755 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 764 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 773 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 782 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 791 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 800 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 809 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Registering RDD 818 (flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Got job 18 (count at ALS.scala:263) with 5 output partitions\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 518 (count at ALS.scala:263)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 476, ShuffleMapStage 517)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 517)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 477 (MapPartitionsRDD[462] at map at ALS.scala:752), which has no missing parents\n",
      "17/01/05 15:42:53 INFO spark.storage.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 17.1 KB, free 162.0 KB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.143.133.19:45040 in memory (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_69_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_69_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 8.5 KB, free 144.7 KB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.143.133.19:45040 (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_69_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_69_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 477 (MapPartitionsRDD[462] at map at ALS.scala:752)\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGODeployScheduler: Adding task set 477.0 with 5 tasks\n",
      "17/01/05 15:42:53 INFO apache.spark.ContextCleaner: Cleaned accumulator 89\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 477.0 (TID 222, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 477.0 (TID 223, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 477.0 (TID 224, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 477.0 (TID 225, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.143.133.19:45040 in memory (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_68_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_68_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_68_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_68_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.ContextCleaner: Cleaned accumulator 88\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(477)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.143.133.19:45040 in memory (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_67_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_67_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_67_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_67_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.ContextCleaner: Cleaned accumulator 87\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.143.133.19:45040 in memory (size: 8.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_66_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_66_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_66_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_66_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 8.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.ContextCleaner: Cleaned accumulator 86\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.143.133.19:45040 in memory (size: 7.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_65_piece0 on yp-spark-dal09-env5-0035:38118 in memory (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_65_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_65_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 7.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_65_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_65_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 7.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.ContextCleaner: Cleaned accumulator 85\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.143.133.19:45040 in memory (size: 7.1 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Removed broadcast_64_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 7.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.ContextCleaner: Cleaned accumulator 84\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 477.0 (TID 226, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 1883 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 477.0 (TID 222) in 25 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 477.0 (TID 224) in 25 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 477.0 (TID 226) in 15 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 477.0 (TID 225) in 93 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 477.0 (TID 223) in 105 ms on yp-spark-dal09-env5-0025 (5/5)\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 477.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 477 (map at ALS.scala:752) finished in 0.106 s\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(477)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 481, ShuffleMapStage 511, ShuffleMapStage 482, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 483, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 478, ShuffleMapStage 500, ShuffleMapStage 479, ShuffleMapStage 501, ShuffleMapStage 480, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 478 (MapPartitionsRDD[467] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(478)\n",
      "17/01/05 15:42:53 INFO spark.storage.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 18.2 KB, free 43.8 KB)\n",
      "17/01/05 15:42:53 INFO spark.storage.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 8.8 KB, free 52.5 KB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.143.133.19:45040 (size: 8.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO apache.spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 478 (MapPartitionsRDD[467] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGODeployScheduler: Adding task set 478.0 with 5 tasks\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 478.0 (TID 227, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 478.0 (TID 228, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 478.0 (TID 229, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 478.0 (TID 230, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 8.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 8.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 90 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 90 is 307 bytes\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 90 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 90 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 90 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 478.0 (TID 231, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2110 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 478.0 (TID 227) in 23 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 478.0 (TID 230) in 24 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 478.0 (TID 231) in 17 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 478.0 (TID 229) in 47 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 478.0 (TID 228) in 51 ms on yp-spark-dal09-env5-0025 (5/5)\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 478.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 478 (flatMap at ALS.scala:1170) finished in 0.051 s\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(478)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 481, ShuffleMapStage 511, ShuffleMapStage 482, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 483, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 479, ShuffleMapStage 501, ShuffleMapStage 480, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 479 (MapPartitionsRDD[476] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(479)\n",
      "17/01/05 15:42:53 INFO spark.storage.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 20.4 KB, free 72.9 KB)\n",
      "17/01/05 15:42:53 INFO spark.storage.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 9.5 KB, free 82.4 KB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.143.133.19:45040 (size: 9.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:53 INFO apache.spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:53 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 479 (MapPartitionsRDD[476] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:53 INFO cluster.ego.EGODeployScheduler: Adding task set 479.0 with 5 tasks\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 479.0 (TID 232, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 479.0 (TID 233, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 479.0 (TID 234, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 479.0 (TID 235, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 9.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 9.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 9.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO spark.storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 9.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 89 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 89 is 291 bytes\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 89 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 89 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:53 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 89 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 479.0 (TID 236, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 479.0 (TID 233) in 358 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:53 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 479.0 (TID 232) in 363 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 479.0 (TID 236) in 344 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 479.0 (TID 234) in 885 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 479.0 (TID 235) in 924 ms on yp-spark-dal09-env5-0025 (5/5)\n",
      "17/01/05 15:42:54 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 479.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 479 (flatMap at ALS.scala:1170) finished in 0.924 s\n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 481, ShuffleMapStage 511, ShuffleMapStage 482, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 483, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 480, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(479)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 480 (MapPartitionsRDD[485] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(480)\n",
      "17/01/05 15:42:54 INFO spark.storage.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 21.3 KB, free 103.7 KB)\n",
      "17/01/05 15:42:54 INFO spark.storage.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 9.7 KB, free 113.4 KB)\n",
      "17/01/05 15:42:54 INFO spark.storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.143.133.19:45040 (size: 9.7 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:54 INFO apache.spark.SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:54 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 480 (MapPartitionsRDD[485] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:54 INFO cluster.ego.EGODeployScheduler: Adding task set 480.0 with 5 tasks\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 480.0 (TID 237, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 480.0 (TID 238, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 480.0 (TID 239, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 480.0 (TID 240, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:54 INFO spark.storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 9.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:54 INFO spark.storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 9.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:54 INFO spark.storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:54 INFO spark.storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:54 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 88 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:54 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 88 is 294 bytes\n",
      "17/01/05 15:42:54 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 88 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:54 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 88 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:54 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 88 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 480.0 (TID 241, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 480.0 (TID 240) in 379 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:54 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 480.0 (TID 239) in 394 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 480.0 (TID 237) in 470 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 480.0 (TID 238) in 489 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 480.0 (TID 241) in 381 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:55 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 480.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 480 (flatMap at ALS.scala:1170) finished in 0.761 s\n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 481, ShuffleMapStage 511, ShuffleMapStage 482, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 483, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:55 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(480)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 481 (MapPartitionsRDD[494] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:55 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(481)\n",
      "17/01/05 15:42:55 INFO spark.storage.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 22.1 KB, free 135.5 KB)\n",
      "17/01/05 15:42:55 INFO spark.storage.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 9.9 KB, free 145.4 KB)\n",
      "17/01/05 15:42:55 INFO spark.storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.143.133.19:45040 (size: 9.9 KB, free: 909.0 MB)\n",
      "17/01/05 15:42:55 INFO apache.spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:55 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 481 (MapPartitionsRDD[494] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:55 INFO cluster.ego.EGODeployScheduler: Adding task set 481.0 with 5 tasks\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 481.0 (TID 242, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 481.0 (TID 243, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 481.0 (TID 244, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 481.0 (TID 245, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:55 INFO spark.storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 9.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:55 INFO spark.storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 9.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:55 INFO spark.storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 9.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:55 INFO spark.storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 9.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:55 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 87 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:55 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 87 is 291 bytes\n",
      "17/01/05 15:42:55 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 87 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:55 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 87 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:55 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 87 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 481.0 (TID 246, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 481.0 (TID 244) in 360 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 481.0 (TID 245) in 368 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 481.0 (TID 242) in 397 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:42:55 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 481.0 (TID 243) in 406 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 481.0 (TID 246) in 336 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 481.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 481 (flatMap at ALS.scala:1170) finished in 0.697 s\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(481)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 482, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 483, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 482 (MapPartitionsRDD[503] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(482)\n",
      "17/01/05 15:42:56 INFO spark.storage.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 23.0 KB, free 168.4 KB)\n",
      "17/01/05 15:42:56 INFO spark.storage.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 10.1 KB, free 178.5 KB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.143.133.19:45040 (size: 10.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:56 INFO apache.spark.SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 482 (MapPartitionsRDD[503] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGODeployScheduler: Adding task set 482.0 with 5 tasks\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 482.0 (TID 247, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 482.0 (TID 248, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 482.0 (TID 249, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 482.0 (TID 250, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 86 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 86 is 294 bytes\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 86 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 86 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 86 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 482.0 (TID 251, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 482.0 (TID 250) in 379 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 482.0 (TID 247) in 392 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 482.0 (TID 248) in 395 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 482.0 (TID 249) in 400 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 482.0 (TID 251) in 369 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 482.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 482 (flatMap at ALS.scala:1170) finished in 0.748 s\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(482)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 483, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 483 (MapPartitionsRDD[512] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(483)\n",
      "17/01/05 15:42:56 INFO spark.storage.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 23.9 KB, free 202.4 KB)\n",
      "17/01/05 15:42:56 INFO spark.storage.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 10.3 KB, free 212.7 KB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.143.133.19:45040 (size: 10.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:56 INFO apache.spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:56 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 483 (MapPartitionsRDD[512] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:56 INFO cluster.ego.EGODeployScheduler: Adding task set 483.0 with 5 tasks\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 483.0 (TID 252, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 483.0 (TID 253, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 483.0 (TID 254, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 483.0 (TID 255, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 10.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:56 INFO spark.storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 10.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 85 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 85 is 291 bytes\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 85 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 85 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:56 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 85 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 483.0 (TID 256, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 483.0 (TID 254) in 361 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 483.0 (TID 255) in 374 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 483.0 (TID 252) in 377 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 483.0 (TID 253) in 400 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 483.0 (TID 256) in 334 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:57 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 483.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 483 (flatMap at ALS.scala:1170) finished in 0.697 s\n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:57 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(483)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 484, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 484 (MapPartitionsRDD[521] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:57 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(484)\n",
      "17/01/05 15:42:57 INFO spark.storage.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 24.8 KB, free 237.5 KB)\n",
      "17/01/05 15:42:57 INFO spark.storage.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 10.4 KB, free 247.9 KB)\n",
      "17/01/05 15:42:57 INFO spark.storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.143.133.19:45040 (size: 10.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:57 INFO apache.spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:57 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 484 (MapPartitionsRDD[521] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:57 INFO cluster.ego.EGODeployScheduler: Adding task set 484.0 with 5 tasks\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 484.0 (TID 257, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 484.0 (TID 258, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 484.0 (TID 259, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 484.0 (TID 260, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:57 INFO spark.storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:57 INFO spark.storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:57 INFO spark.storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:57 INFO spark.storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:57 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 84 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:57 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 84 is 294 bytes\n",
      "17/01/05 15:42:57 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 84 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:57 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 84 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:57 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 84 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 484.0 (TID 258) in 392 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 484.0 (TID 257) in 400 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 484.0 (TID 260) in 401 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 484.0 (TID 261, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:57 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 484.0 (TID 259) in 410 ms on yp-spark-dal09-env5-0031 (4/5)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 484.0 (TID 261) in 383 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 484.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 484 (flatMap at ALS.scala:1170) finished in 0.792 s\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(484)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 485, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 485 (MapPartitionsRDD[530] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(485)\n",
      "17/01/05 15:42:58 INFO spark.storage.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 25.6 KB, free 273.5 KB)\n",
      "17/01/05 15:42:58 INFO spark.storage.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 10.7 KB, free 284.2 KB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.143.133.19:45040 (size: 10.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:58 INFO apache.spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 485 (MapPartitionsRDD[530] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGODeployScheduler: Adding task set 485.0 with 5 tasks\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 485.0 (TID 262, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 485.0 (TID 263, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 485.0 (TID 264, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 485.0 (TID 265, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 10.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 10.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 83 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 83 is 291 bytes\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 83 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 83 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 83 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 485.0 (TID 266, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 485.0 (TID 263) in 363 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 485.0 (TID 262) in 366 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 485.0 (TID 264) in 373 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 485.0 (TID 265) in 403 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 485.0 (TID 266) in 336 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 485.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 485 (flatMap at ALS.scala:1170) finished in 0.700 s\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 486, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(485)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 486 (MapPartitionsRDD[539] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(486)\n",
      "17/01/05 15:42:58 INFO spark.storage.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 26.5 KB, free 310.7 KB)\n",
      "17/01/05 15:42:58 INFO spark.storage.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 10.8 KB, free 321.6 KB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.143.133.19:45040 (size: 10.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:58 INFO apache.spark.SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:58 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 486 (MapPartitionsRDD[539] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:58 INFO cluster.ego.EGODeployScheduler: Adding task set 486.0 with 5 tasks\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 486.0 (TID 267, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 486.0 (TID 268, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 486.0 (TID 269, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 486.0 (TID 270, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 10.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:58 INFO spark.storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 10.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 82 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 82 is 294 bytes\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 82 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 82 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:42:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 82 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 486.0 (TID 271, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 486.0 (TID 269) in 376 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 486.0 (TID 268) in 393 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 486.0 (TID 270) in 395 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 486.0 (TID 267) in 398 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 486.0 (TID 271) in 363 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:42:59 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 486.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 486 (flatMap at ALS.scala:1170) finished in 0.741 s\n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:42:59 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(486)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508, ShuffleMapStage 487)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 487 (MapPartitionsRDD[548] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:42:59 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(487)\n",
      "17/01/05 15:42:59 INFO spark.storage.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 27.4 KB, free 349.0 KB)\n",
      "17/01/05 15:42:59 INFO spark.storage.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 11.0 KB, free 360.0 KB)\n",
      "17/01/05 15:42:59 INFO spark.storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.143.133.19:45040 (size: 11.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:42:59 INFO apache.spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:42:59 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 487 (MapPartitionsRDD[548] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:42:59 INFO cluster.ego.EGODeployScheduler: Adding task set 487.0 with 5 tasks\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 487.0 (TID 272, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 487.0 (TID 273, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 487.0 (TID 274, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:59 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 487.0 (TID 275, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:42:59 INFO spark.storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:59 INFO spark.storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:42:59 INFO spark.storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 11.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:59 INFO spark.storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:42:59 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 81 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:42:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 81 is 291 bytes\n",
      "17/01/05 15:42:59 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 81 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:42:59 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 81 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:42:59 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 81 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 487.0 (TID 276, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 487.0 (TID 275) in 360 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 487.0 (TID 272) in 360 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 487.0 (TID 274) in 373 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 487.0 (TID 273) in 397 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 487.0 (TID 276) in 347 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:00 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 487.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 487 (flatMap at ALS.scala:1170) finished in 0.707 s\n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 488, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(487)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 488 (MapPartitionsRDD[557] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(488)\n",
      "17/01/05 15:43:00 INFO spark.storage.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 28.3 KB, free 388.3 KB)\n",
      "17/01/05 15:43:00 INFO spark.storage.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 11.2 KB, free 399.4 KB)\n",
      "17/01/05 15:43:00 INFO spark.storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.143.133.19:45040 (size: 11.2 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:00 INFO apache.spark.SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:00 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 488 (MapPartitionsRDD[557] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:00 INFO cluster.ego.EGODeployScheduler: Adding task set 488.0 with 5 tasks\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 488.0 (TID 277, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 488.0 (TID 278, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 488.0 (TID 279, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 488.0 (TID 280, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:00 INFO spark.storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:00 INFO spark.storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:00 INFO spark.storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:00 INFO spark.storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 11.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:00 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 80 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:00 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 80 is 294 bytes\n",
      "17/01/05 15:43:00 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 80 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:00 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 80 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:00 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 80 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 488.0 (TID 281, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 488.0 (TID 279) in 378 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 488.0 (TID 277) in 389 ms on yp-spark-dal09-env5-0022 (2/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 488.0 (TID 280) in 395 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:00 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 488.0 (TID 278) in 408 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 488.0 (TID 281) in 369 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 488.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 488 (flatMap at ALS.scala:1170) finished in 0.748 s\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 489, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(488)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 489 (MapPartitionsRDD[566] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(489)\n",
      "17/01/05 15:43:01 INFO spark.storage.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 29.2 KB, free 428.6 KB)\n",
      "17/01/05 15:43:01 INFO spark.storage.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 11.4 KB, free 439.9 KB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.143.133.19:45040 (size: 11.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:01 INFO apache.spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 489 (MapPartitionsRDD[566] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGODeployScheduler: Adding task set 489.0 with 5 tasks\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 489.0 (TID 282, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 489.0 (TID 283, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 489.0 (TID 284, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 489.0 (TID 285, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 11.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 79 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 79 is 291 bytes\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 79 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 79 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 79 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 489.0 (TID 286, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 489.0 (TID 282) in 359 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 489.0 (TID 285) in 368 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 489.0 (TID 283) in 371 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 489.0 (TID 284) in 395 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 489.0 (TID 286) in 337 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 489.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 489 (flatMap at ALS.scala:1170) finished in 0.696 s\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 490, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(489)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 490 (MapPartitionsRDD[575] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(490)\n",
      "17/01/05 15:43:01 INFO spark.storage.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 30.0 KB, free 470.0 KB)\n",
      "17/01/05 15:43:01 INFO spark.storage.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 11.5 KB, free 481.5 KB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.143.133.19:45040 (size: 11.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:01 INFO apache.spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:01 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 490 (MapPartitionsRDD[575] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:01 INFO cluster.ego.EGODeployScheduler: Adding task set 490.0 with 5 tasks\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 490.0 (TID 287, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 490.0 (TID 288, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 490.0 (TID 289, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 490.0 (TID 290, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:01 INFO spark.storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 78 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 78 is 294 bytes\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 78 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 78 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:01 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 78 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 490.0 (TID 291, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 490.0 (TID 289) in 378 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 490.0 (TID 287) in 384 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 490.0 (TID 290) in 391 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 490.0 (TID 288) in 398 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 490.0 (TID 291) in 371 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:02 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 490.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 490 (flatMap at ALS.scala:1170) finished in 0.750 s\n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 491, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:02 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(490)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 491 (MapPartitionsRDD[584] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:02 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(491)\n",
      "17/01/05 15:43:02 INFO spark.storage.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 30.9 KB, free 512.4 KB)\n",
      "17/01/05 15:43:02 INFO spark.storage.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 11.7 KB, free 524.1 KB)\n",
      "17/01/05 15:43:02 INFO spark.storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.143.133.19:45040 (size: 11.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:02 INFO apache.spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:02 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 491 (MapPartitionsRDD[584] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:02 INFO cluster.ego.EGODeployScheduler: Adding task set 491.0 with 5 tasks\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 491.0 (TID 292, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 491.0 (TID 293, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 491.0 (TID 294, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:02 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 491.0 (TID 295, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:02 INFO spark.storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:02 INFO spark.storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:02 INFO spark.storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:02 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 77 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:02 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 77 is 291 bytes\n",
      "17/01/05 15:43:02 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 77 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:02 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 77 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:02 INFO spark.storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 11.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:02 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 77 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 491.0 (TID 296, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 491.0 (TID 293) in 360 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 491.0 (TID 295) in 367 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 491.0 (TID 292) in 391 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 491.0 (TID 294) in 404 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 491.0 (TID 296) in 332 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:03 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 491.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 491 (flatMap at ALS.scala:1170) finished in 0.692 s\n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 492, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:03 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(491)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 492 (MapPartitionsRDD[593] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:03 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(492)\n",
      "17/01/05 15:43:03 INFO spark.storage.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 31.8 KB, free 555.9 KB)\n",
      "17/01/05 15:43:03 INFO spark.storage.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 11.9 KB, free 567.8 KB)\n",
      "17/01/05 15:43:03 INFO spark.storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.143.133.19:45040 (size: 11.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:03 INFO apache.spark.SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:03 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 492 (MapPartitionsRDD[593] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:03 INFO cluster.ego.EGODeployScheduler: Adding task set 492.0 with 5 tasks\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 492.0 (TID 297, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 492.0 (TID 298, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 492.0 (TID 299, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 492.0 (TID 300, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:03 INFO spark.storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 11.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:03 INFO spark.storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 11.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:03 INFO spark.storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 11.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:03 INFO spark.storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 11.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:03 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 76 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:03 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 76 is 294 bytes\n",
      "17/01/05 15:43:03 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 76 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:03 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 76 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:03 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 76 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 492.0 (TID 298) in 380 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 492.0 (TID 301, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 492.0 (TID 297) in 382 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 492.0 (TID 299) in 385 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:03 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 492.0 (TID 300) in 400 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 492.0 (TID 301) in 373 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 492.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 492 (flatMap at ALS.scala:1170) finished in 0.754 s\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 493, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(492)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 493 (MapPartitionsRDD[602] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(493)\n",
      "17/01/05 15:43:04 INFO spark.storage.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 32.7 KB, free 600.5 KB)\n",
      "17/01/05 15:43:04 INFO spark.storage.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 12.2 KB, free 612.7 KB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.143.133.19:45040 (size: 12.2 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:04 INFO apache.spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 493 (MapPartitionsRDD[602] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGODeployScheduler: Adding task set 493.0 with 5 tasks\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 493.0 (TID 302, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 493.0 (TID 303, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 493.0 (TID 304, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 493.0 (TID 305, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 12.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 12.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 75 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 75 is 291 bytes\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 75 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 75 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 75 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 493.0 (TID 306, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 493.0 (TID 303) in 360 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 493.0 (TID 302) in 363 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 493.0 (TID 304) in 369 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 493.0 (TID 305) in 379 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 493.0 (TID 306) in 333 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 493.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 493 (flatMap at ALS.scala:1170) finished in 0.694 s\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 494, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(493)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 494 (MapPartitionsRDD[611] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(494)\n",
      "17/01/05 15:43:04 INFO spark.storage.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 33.5 KB, free 646.2 KB)\n",
      "17/01/05 15:43:04 INFO spark.storage.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 12.4 KB, free 658.7 KB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.143.133.19:45040 (size: 12.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:04 INFO apache.spark.SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:04 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 494 (MapPartitionsRDD[611] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:04 INFO cluster.ego.EGODeployScheduler: Adding task set 494.0 with 5 tasks\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 494.0 (TID 307, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 494.0 (TID 308, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 494.0 (TID 309, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 494.0 (TID 310, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:04 INFO spark.storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 74 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 74 is 294 bytes\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 74 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 74 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:04 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 74 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 494.0 (TID 307) in 375 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 494.0 (TID 308) in 380 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 494.0 (TID 309) in 393 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 494.0 (TID 311, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 494.0 (TID 310) in 399 ms on yp-spark-dal09-env5-0031 (4/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 494.0 (TID 311) in 375 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:05 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 494.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 494 (flatMap at ALS.scala:1170) finished in 0.775 s\n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 495, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:05 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(494)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 495 (MapPartitionsRDD[620] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:05 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(495)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.143.133.19:45040 in memory (size: 12.2 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_86_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_86_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 12.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_86_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 12.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_86_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 12.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 34.4 KB, free 648.2 KB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_85_piece0 on 10.143.133.19:45040 in memory (size: 11.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_85_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 12.6 KB, free 617.1 KB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_85_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.143.133.19:45040 (size: 12.6 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_85_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 11.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_85_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 11.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO apache.spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:05 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 495 (MapPartitionsRDD[620] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:05 INFO cluster.ego.EGODeployScheduler: Adding task set 495.0 with 5 tasks\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 495.0 (TID 312, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 495.0 (TID 313, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.143.133.19:45040 in memory (size: 11.7 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 495.0 (TID 314, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 495.0 (TID 315, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_84_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_84_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_84_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 11.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_84_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 11.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.143.133.19:45040 in memory (size: 11.5 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_83_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_83_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_83_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_83_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 11.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 12.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.143.133.19:45040 in memory (size: 11.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 12.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_82_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_82_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_82_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 11.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_82_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 11.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_81_piece0 on 10.143.133.19:45040 in memory (size: 11.2 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_81_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 73 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_81_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 73 is 291 bytes\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_81_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 11.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_81_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 11.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 73 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:05 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 73 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.143.133.19:45040 in memory (size: 11.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 73 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_80_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 11.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_80_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 11.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_80_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 11.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_80_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 11.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_79_piece0 on 10.143.133.19:45040 in memory (size: 10.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_79_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_79_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_79_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_79_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 10.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.143.133.19:45040 in memory (size: 10.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_78_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_78_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 10.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_78_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 10.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_78_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.143.133.19:45040 in memory (size: 10.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_77_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_77_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_77_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_77_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 10.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.143.133.19:45040 in memory (size: 10.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_76_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_76_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_76_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 10.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_76_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 10.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_75_piece0 on 10.143.133.19:45040 in memory (size: 10.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_75_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 10.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_75_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 10.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_75_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_75_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 10.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.143.133.19:45040 in memory (size: 9.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_74_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 9.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_74_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 9.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_74_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 9.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_74_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 9.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_73_piece0 on 10.143.133.19:45040 in memory (size: 9.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_73_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 9.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_73_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 9.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_73_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_73_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 9.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.143.133.19:45040 in memory (size: 9.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_72_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 9.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_72_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 9.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_72_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 9.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_72_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 9.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.143.133.19:45040 in memory (size: 8.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_71_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_71_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_71_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_71_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 8.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.143.133.19:45040 in memory (size: 8.5 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_70_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_70_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 8.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_70_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.storage.BlockManagerInfo: Removed broadcast_70_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 8.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 495.0 (TID 315) in 361 ms on yp-spark-dal09-env5-0024 (1/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 495.0 (TID 316, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 495.0 (TID 314) in 363 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 495.0 (TID 312) in 365 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:43:05 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 495.0 (TID 313) in 378 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 495.0 (TID 316) in 343 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:06 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 495.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 495 (flatMap at ALS.scala:1170) finished in 0.707 s\n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ShuffleMapStage 496, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(495)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 496 (MapPartitionsRDD[629] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(496)\n",
      "17/01/05 15:43:06 INFO spark.storage.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 35.3 KB, free 128.3 KB)\n",
      "17/01/05 15:43:06 INFO spark.storage.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 12.8 KB, free 141.1 KB)\n",
      "17/01/05 15:43:06 INFO spark.storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.143.133.19:45040 (size: 12.8 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:06 INFO apache.spark.SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:06 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 496 (MapPartitionsRDD[629] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:06 INFO cluster.ego.EGODeployScheduler: Adding task set 496.0 with 5 tasks\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 496.0 (TID 317, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 496.0 (TID 318, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 496.0 (TID 319, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 496.0 (TID 320, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:06 INFO spark.storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 12.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:06 INFO spark.storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 12.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:06 INFO spark.storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 12.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:06 INFO spark.storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 12.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:06 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 72 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:06 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 72 is 294 bytes\n",
      "17/01/05 15:43:06 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 72 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:06 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 72 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:06 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 72 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 496.0 (TID 318) in 377 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 496.0 (TID 321, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 496.0 (TID 317) in 378 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 496.0 (TID 319) in 379 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:43:06 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 496.0 (TID 320) in 394 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 496.0 (TID 321) in 374 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 496.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 496 (flatMap at ALS.scala:1170) finished in 0.752 s\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 497, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(496)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 497 (MapPartitionsRDD[638] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(497)\n",
      "17/01/05 15:43:07 INFO spark.storage.MemoryStore: Block broadcast_90 stored as values in memory (estimated size 36.2 KB, free 177.3 KB)\n",
      "17/01/05 15:43:07 INFO spark.storage.MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 13.0 KB, free 190.2 KB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.143.133.19:45040 (size: 13.0 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:07 INFO apache.spark.SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 497 (MapPartitionsRDD[638] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGODeployScheduler: Adding task set 497.0 with 5 tasks\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 497.0 (TID 322, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 497.0 (TID 323, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 497.0 (TID 324, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 497.0 (TID 325, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 13.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 71 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 71 is 291 bytes\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 71 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 71 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 71 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 497.0 (TID 326, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 497.0 (TID 323) in 362 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 497.0 (TID 325) in 361 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 497.0 (TID 324) in 364 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 497.0 (TID 322) in 380 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 497.0 (TID 326) in 336 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 497.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 497 (flatMap at ALS.scala:1170) finished in 0.697 s\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 498, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(497)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 498 (MapPartitionsRDD[647] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(498)\n",
      "17/01/05 15:43:07 INFO spark.storage.MemoryStore: Block broadcast_91 stored as values in memory (estimated size 37.1 KB, free 227.3 KB)\n",
      "17/01/05 15:43:07 INFO spark.storage.MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 13.1 KB, free 240.4 KB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.143.133.19:45040 (size: 13.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:07 INFO apache.spark.SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:07 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 498 (MapPartitionsRDD[647] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:07 INFO cluster.ego.EGODeployScheduler: Adding task set 498.0 with 5 tasks\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 498.0 (TID 327, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 498.0 (TID 328, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 498.0 (TID 329, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 498.0 (TID 330, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:07 INFO spark.storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 70 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 70 is 294 bytes\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 70 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 70 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 70 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 498.0 (TID 328) in 378 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 498.0 (TID 331, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 498.0 (TID 330) in 377 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 498.0 (TID 329) in 379 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 498.0 (TID 327) in 396 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 498.0 (TID 331) in 373 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:08 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 498.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 498 (flatMap at ALS.scala:1170) finished in 0.752 s\n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 499, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(498)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 499 (MapPartitionsRDD[656] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(499)\n",
      "17/01/05 15:43:08 INFO spark.storage.MemoryStore: Block broadcast_92 stored as values in memory (estimated size 37.9 KB, free 278.4 KB)\n",
      "17/01/05 15:43:08 INFO spark.storage.MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 13.3 KB, free 291.7 KB)\n",
      "17/01/05 15:43:08 INFO spark.storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.143.133.19:45040 (size: 13.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:08 INFO apache.spark.SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:08 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 499 (MapPartitionsRDD[656] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:08 INFO cluster.ego.EGODeployScheduler: Adding task set 499.0 with 5 tasks\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 499.0 (TID 332, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 499.0 (TID 333, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 499.0 (TID 334, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 499.0 (TID 335, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:08 INFO spark.storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:08 INFO spark.storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:08 INFO spark.storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 13.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:08 INFO spark.storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 69 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:08 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 69 is 291 bytes\n",
      "17/01/05 15:43:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 69 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 69 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 69 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 499.0 (TID 334) in 359 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 499.0 (TID 336, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 499.0 (TID 335) in 360 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 499.0 (TID 333) in 364 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:08 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 499.0 (TID 332) in 380 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 499.0 (TID 336) in 330 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 499.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 499 (flatMap at ALS.scala:1170) finished in 0.691 s\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 500, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(499)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 500 (MapPartitionsRDD[665] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(500)\n",
      "17/01/05 15:43:09 INFO spark.storage.MemoryStore: Block broadcast_93 stored as values in memory (estimated size 38.8 KB, free 330.5 KB)\n",
      "17/01/05 15:43:09 INFO spark.storage.MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 13.5 KB, free 344.0 KB)\n",
      "17/01/05 15:43:09 INFO spark.storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.143.133.19:45040 (size: 13.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:09 INFO apache.spark.SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 500 (MapPartitionsRDD[665] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGODeployScheduler: Adding task set 500.0 with 5 tasks\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 500.0 (TID 337, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 500.0 (TID 338, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 500.0 (TID 339, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 500.0 (TID 340, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:09 INFO spark.storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:09 INFO spark.storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:09 INFO spark.storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 13.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:09 INFO spark.storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 68 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 68 is 294 bytes\n",
      "17/01/05 15:43:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 68 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 68 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 68 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 500.0 (TID 338) in 375 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 500.0 (TID 341, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 500.0 (TID 337) in 379 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 500.0 (TID 340) in 379 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 500.0 (TID 339) in 402 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 500.0 (TID 341) in 372 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 500.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 500 (flatMap at ALS.scala:1170) finished in 0.752 s\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 501, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(500)\n",
      "17/01/05 15:43:09 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 501 (MapPartitionsRDD[674] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(501)\n",
      "17/01/05 15:43:09 INFO spark.storage.MemoryStore: Block broadcast_94 stored as values in memory (estimated size 39.7 KB, free 383.6 KB)\n",
      "17/01/05 15:43:10 INFO spark.storage.MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 13.7 KB, free 397.4 KB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.143.133.19:45040 (size: 13.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:10 INFO apache.spark.SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 501 (MapPartitionsRDD[674] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:10 INFO cluster.ego.EGODeployScheduler: Adding task set 501.0 with 5 tasks\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 501.0 (TID 342, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 501.0 (TID 343, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 501.0 (TID 344, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 501.0 (TID 345, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 13.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 67 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 67 is 291 bytes\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 67 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 67 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 67 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 501.0 (TID 346, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 501.0 (TID 345) in 356 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 501.0 (TID 343) in 361 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 501.0 (TID 342) in 378 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 501.0 (TID 344) in 395 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 501.0 (TID 346) in 334 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:10 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 501.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 501 (flatMap at ALS.scala:1170) finished in 0.691 s\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 502, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:10 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(501)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 502 (MapPartitionsRDD[683] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:10 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(502)\n",
      "17/01/05 15:43:10 INFO spark.storage.MemoryStore: Block broadcast_95 stored as values in memory (estimated size 40.6 KB, free 437.9 KB)\n",
      "17/01/05 15:43:10 INFO spark.storage.MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 13.9 KB, free 451.8 KB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.143.133.19:45040 (size: 13.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:10 INFO apache.spark.SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:10 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 502 (MapPartitionsRDD[683] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:10 INFO cluster.ego.EGODeployScheduler: Adding task set 502.0 with 5 tasks\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 502.0 (TID 347, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 502.0 (TID 348, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 502.0 (TID 349, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 502.0 (TID 350, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 13.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 13.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 13.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:10 INFO spark.storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 13.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 66 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 66 is 294 bytes\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 66 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 66 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:10 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 66 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 502.0 (TID 351, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 502.0 (TID 348) in 377 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 502.0 (TID 347) in 377 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 502.0 (TID 350) in 378 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 502.0 (TID 349) in 396 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 502.0 (TID 351) in 378 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:11 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 502.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 502 (flatMap at ALS.scala:1170) finished in 0.755 s\n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(502)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 503, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 503 (MapPartitionsRDD[692] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(503)\n",
      "17/01/05 15:43:11 INFO spark.storage.MemoryStore: Block broadcast_96 stored as values in memory (estimated size 41.4 KB, free 493.3 KB)\n",
      "17/01/05 15:43:11 INFO spark.storage.MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 14.1 KB, free 507.3 KB)\n",
      "17/01/05 15:43:11 INFO spark.storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.143.133.19:45040 (size: 14.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:11 INFO apache.spark.SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:11 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 503 (MapPartitionsRDD[692] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:11 INFO cluster.ego.EGODeployScheduler: Adding task set 503.0 with 5 tasks\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 503.0 (TID 352, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 503.0 (TID 353, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 503.0 (TID 354, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 503.0 (TID 355, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:11 INFO spark.storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:11 INFO spark.storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:11 INFO spark.storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 14.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:11 INFO spark.storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 65 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:11 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 65 is 291 bytes\n",
      "17/01/05 15:43:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 65 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 65 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 65 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 503.0 (TID 352) in 366 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 503.0 (TID 355) in 368 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 503.0 (TID 353) in 382 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 503.0 (TID 356, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:11 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 503.0 (TID 354) in 396 ms on yp-spark-dal09-env5-0031 (4/5)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 503.0 (TID 356) in 336 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 503.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 503 (flatMap at ALS.scala:1170) finished in 0.732 s\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 504, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(503)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 504 (MapPartitionsRDD[701] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(504)\n",
      "17/01/05 15:43:12 INFO spark.storage.MemoryStore: Block broadcast_97 stored as values in memory (estimated size 42.3 KB, free 549.6 KB)\n",
      "17/01/05 15:43:12 INFO spark.storage.MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 14.2 KB, free 563.8 KB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.143.133.19:45040 (size: 14.2 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:12 INFO apache.spark.SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 504 (MapPartitionsRDD[701] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGODeployScheduler: Adding task set 504.0 with 5 tasks\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 504.0 (TID 357, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 504.0 (TID 358, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 504.0 (TID 359, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 504.0 (TID 360, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 64 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 64 is 294 bytes\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 64 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 64 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 64 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 504.0 (TID 359) in 378 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 504.0 (TID 360) in 386 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 504.0 (TID 361, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 504.0 (TID 358) in 386 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 504.0 (TID 357) in 402 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 504.0 (TID 361) in 373 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 504.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 504 (flatMap at ALS.scala:1170) finished in 0.759 s\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 505, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(504)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 505 (MapPartitionsRDD[710] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(505)\n",
      "17/01/05 15:43:12 INFO spark.storage.MemoryStore: Block broadcast_98 stored as values in memory (estimated size 43.2 KB, free 607.0 KB)\n",
      "17/01/05 15:43:12 INFO spark.storage.MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 14.4 KB, free 621.4 KB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.143.133.19:45040 (size: 14.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:12 INFO apache.spark.SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:12 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 505 (MapPartitionsRDD[710] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:12 INFO cluster.ego.EGODeployScheduler: Adding task set 505.0 with 5 tasks\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 505.0 (TID 362, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 505.0 (TID 363, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 505.0 (TID 364, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 505.0 (TID 365, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 14.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:12 INFO spark.storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 63 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 63 is 291 bytes\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 63 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 63 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 63 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 505.0 (TID 366, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 505.0 (TID 365) in 359 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 505.0 (TID 362) in 377 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 505.0 (TID 363) in 382 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 505.0 (TID 364) in 411 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 505.0 (TID 366) in 353 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:13 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 505.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 505 (flatMap at ALS.scala:1170) finished in 0.713 s\n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 506, ShuffleMapStage 507, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(505)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 506 (MapPartitionsRDD[719] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(506)\n",
      "17/01/05 15:43:13 INFO spark.storage.MemoryStore: Block broadcast_99 stored as values in memory (estimated size 44.1 KB, free 665.5 KB)\n",
      "17/01/05 15:43:13 INFO spark.storage.MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 14.5 KB, free 680.0 KB)\n",
      "17/01/05 15:43:13 INFO spark.storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.143.133.19:45040 (size: 14.5 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:13 INFO apache.spark.SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:13 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 506 (MapPartitionsRDD[719] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:13 INFO cluster.ego.EGODeployScheduler: Adding task set 506.0 with 5 tasks\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 506.0 (TID 367, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 506.0 (TID 368, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 506.0 (TID 369, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:13 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 506.0 (TID 370, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:13 INFO spark.storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:13 INFO spark.storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:13 INFO spark.storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:13 INFO spark.storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 62 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:13 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 62 is 294 bytes\n",
      "17/01/05 15:43:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 62 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 62 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 62 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 506.0 (TID 371, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 506.0 (TID 367) in 377 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 506.0 (TID 370) in 379 ms on yp-spark-dal09-env5-0022 (2/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 506.0 (TID 369) in 400 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 506.0 (TID 368) in 453 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 506.0 (TID 371) in 370 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 506.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 506 (flatMap at ALS.scala:1170) finished in 0.747 s\n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 507, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(506)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 507 (MapPartitionsRDD[728] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(507)\n",
      "17/01/05 15:43:14 INFO spark.storage.MemoryStore: Block broadcast_100 stored as values in memory (estimated size 45.0 KB, free 724.9 KB)\n",
      "17/01/05 15:43:14 INFO spark.storage.MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 14.8 KB, free 739.7 KB)\n",
      "17/01/05 15:43:14 INFO spark.storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.143.133.19:45040 (size: 14.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:14 INFO apache.spark.SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:14 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 507 (MapPartitionsRDD[728] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:14 INFO cluster.ego.EGODeployScheduler: Adding task set 507.0 with 5 tasks\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 507.0 (TID 372, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 507.0 (TID 373, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 507.0 (TID 374, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 507.0 (TID 375, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:14 INFO spark.storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:14 INFO spark.storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:14 INFO spark.storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:14 INFO spark.storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 61 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 61 is 291 bytes\n",
      "17/01/05 15:43:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 61 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 61 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 61 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 507.0 (TID 376, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 507.0 (TID 374) in 361 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 507.0 (TID 373) in 363 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 507.0 (TID 375) in 370 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:14 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 507.0 (TID 372) in 379 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 507.0 (TID 376) in 337 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 507.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 507 (flatMap at ALS.scala:1170) finished in 0.698 s\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(507)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516, ShuffleMapStage 508)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 508 (MapPartitionsRDD[737] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(508)\n",
      "17/01/05 15:43:15 INFO spark.storage.MemoryStore: Block broadcast_101 stored as values in memory (estimated size 45.8 KB, free 785.5 KB)\n",
      "17/01/05 15:43:15 INFO spark.storage.MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 14.8 KB, free 800.3 KB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.143.133.19:45040 (size: 14.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:15 INFO apache.spark.SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 508 (MapPartitionsRDD[737] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGODeployScheduler: Adding task set 508.0 with 5 tasks\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 508.0 (TID 377, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 508.0 (TID 378, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 508.0 (TID 379, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 508.0 (TID 380, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 60 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 60 is 294 bytes\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 60 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 60 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 60 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 508.0 (TID 381, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 508.0 (TID 377) in 374 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 508.0 (TID 380) in 377 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 508.0 (TID 379) in 380 ms on yp-spark-dal09-env5-0022 (3/5)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 508.0 (TID 378) in 406 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 508.0 (TID 381) in 367 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 508.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 508 (flatMap at ALS.scala:1170) finished in 0.739 s\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 509, ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(508)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 509 (MapPartitionsRDD[746] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(509)\n",
      "17/01/05 15:43:15 INFO spark.storage.MemoryStore: Block broadcast_102 stored as values in memory (estimated size 46.7 KB, free 847.0 KB)\n",
      "17/01/05 15:43:15 INFO spark.storage.MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 15.0 KB, free 862.1 KB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.143.133.19:45040 (size: 15.0 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:15 INFO apache.spark.SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:15 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 509 (MapPartitionsRDD[746] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:15 INFO cluster.ego.EGODeployScheduler: Adding task set 509.0 with 5 tasks\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 509.0 (TID 382, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 509.0 (TID 383, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 509.0 (TID 384, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 509.0 (TID 385, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 15.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:15 INFO spark.storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 15.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 59 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 59 is 291 bytes\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 59 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 59 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 59 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 509.0 (TID 386, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 509.0 (TID 382) in 357 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 509.0 (TID 385) in 358 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 509.0 (TID 384) in 361 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 509.0 (TID 383) in 378 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 509.0 (TID 386) in 334 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:16 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 509.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 509 (flatMap at ALS.scala:1170) finished in 0.691 s\n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 510, ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(509)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 510 (MapPartitionsRDD[755] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(510)\n",
      "17/01/05 15:43:16 INFO spark.storage.MemoryStore: Block broadcast_103 stored as values in memory (estimated size 47.6 KB, free 909.7 KB)\n",
      "17/01/05 15:43:16 INFO spark.storage.MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 15.1 KB, free 924.8 KB)\n",
      "17/01/05 15:43:16 INFO spark.storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.143.133.19:45040 (size: 15.1 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:16 INFO apache.spark.SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:16 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 510 (MapPartitionsRDD[755] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:16 INFO cluster.ego.EGODeployScheduler: Adding task set 510.0 with 5 tasks\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 510.0 (TID 387, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 510.0 (TID 388, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 510.0 (TID 389, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 510.0 (TID 390, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:16 INFO spark.storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:16 INFO spark.storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:16 INFO spark.storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:16 INFO spark.storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 58 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:16 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 58 is 294 bytes\n",
      "17/01/05 15:43:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 58 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 58 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 58 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 510.0 (TID 391, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 510.0 (TID 387) in 377 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 510.0 (TID 389) in 378 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:16 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 510.0 (TID 388) in 402 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 510.0 (TID 390) in 429 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 510.0 (TID 391) in 372 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:17 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 510.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 510 (flatMap at ALS.scala:1170) finished in 0.750 s\n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 511, ShuffleMapStage 512, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(510)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 511 (MapPartitionsRDD[764] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(511)\n",
      "17/01/05 15:43:17 INFO spark.storage.MemoryStore: Block broadcast_104 stored as values in memory (estimated size 48.5 KB, free 973.3 KB)\n",
      "17/01/05 15:43:17 INFO spark.storage.MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 15.3 KB, free 988.6 KB)\n",
      "17/01/05 15:43:17 INFO spark.storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on 10.143.133.19:45040 (size: 15.3 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:17 INFO apache.spark.SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:17 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 511 (MapPartitionsRDD[764] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:17 INFO cluster.ego.EGODeployScheduler: Adding task set 511.0 with 5 tasks\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 511.0 (TID 392, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 511.0 (TID 393, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 511.0 (TID 394, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 511.0 (TID 395, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:17 INFO spark.storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:17 INFO spark.storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:17 INFO spark.storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 15.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:17 INFO spark.storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 15.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 57 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:17 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 57 is 291 bytes\n",
      "17/01/05 15:43:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 57 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 57 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 57 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 511.0 (TID 396, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 511.0 (TID 392) in 359 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 511.0 (TID 395) in 361 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 511.0 (TID 393) in 368 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:17 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 511.0 (TID 394) in 427 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 511.0 (TID 396) in 334 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 511.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 511 (flatMap at ALS.scala:1170) finished in 0.693 s\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 512, ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(511)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 512 (MapPartitionsRDD[773] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(512)\n",
      "17/01/05 15:43:18 INFO spark.storage.MemoryStore: Block broadcast_105 stored as values in memory (estimated size 49.3 KB, free 1037.9 KB)\n",
      "17/01/05 15:43:18 INFO spark.storage.MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 15.4 KB, free 1053.4 KB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on 10.143.133.19:45040 (size: 15.4 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:18 INFO apache.spark.SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 512 (MapPartitionsRDD[773] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGODeployScheduler: Adding task set 512.0 with 5 tasks\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 512.0 (TID 397, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 512.0 (TID 398, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 512.0 (TID 399, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 512.0 (TID 400, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 56 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 56 is 294 bytes\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 56 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 56 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 56 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 512.0 (TID 401, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 512.0 (TID 397) in 374 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 512.0 (TID 400) in 374 ms on yp-spark-dal09-env5-0022 (2/5)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 512.0 (TID 399) in 376 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 512.0 (TID 398) in 390 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 512.0 (TID 401) in 371 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 512.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 512 (flatMap at ALS.scala:1170) finished in 0.745 s\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 513, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(512)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 513 (MapPartitionsRDD[782] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(513)\n",
      "17/01/05 15:43:18 INFO spark.storage.MemoryStore: Block broadcast_106 stored as values in memory (estimated size 50.2 KB, free 1103.6 KB)\n",
      "17/01/05 15:43:18 INFO spark.storage.MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 15.6 KB, free 1119.2 KB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on 10.143.133.19:45040 (size: 15.6 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:18 INFO apache.spark.SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:18 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 513 (MapPartitionsRDD[782] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:18 INFO cluster.ego.EGODeployScheduler: Adding task set 513.0 with 5 tasks\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 513.0 (TID 402, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 513.0 (TID 403, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 513.0 (TID 404, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 513.0 (TID 405, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 15.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:18 INFO spark.storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 15.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 55 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 55 is 291 bytes\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 55 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 55 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 55 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 513.0 (TID 406, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 513.0 (TID 402) in 357 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 513.0 (TID 403) in 359 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 513.0 (TID 405) in 366 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 513.0 (TID 404) in 375 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 513.0 (TID 406) in 340 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:19 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 513.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 513 (flatMap at ALS.scala:1170) finished in 0.698 s\n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 514, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:19 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(513)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 514 (MapPartitionsRDD[791] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:19 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(514)\n",
      "17/01/05 15:43:19 INFO spark.storage.MemoryStore: Block broadcast_107 stored as values in memory (estimated size 51.1 KB, free 1170.3 KB)\n",
      "17/01/05 15:43:19 INFO spark.storage.MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 15.7 KB, free 1186.1 KB)\n",
      "17/01/05 15:43:19 INFO spark.storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on 10.143.133.19:45040 (size: 15.7 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:19 INFO apache.spark.SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:19 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 514 (MapPartitionsRDD[791] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:19 INFO cluster.ego.EGODeployScheduler: Adding task set 514.0 with 5 tasks\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 514.0 (TID 407, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 514.0 (TID 408, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 514.0 (TID 409, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 514.0 (TID 410, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:19 INFO spark.storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 15.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:19 INFO spark.storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:19 INFO spark.storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 15.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 54 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:19 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 54 is 294 bytes\n",
      "17/01/05 15:43:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 54 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 54 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:19 INFO spark.storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 54 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 514.0 (TID 408) in 376 ms on yp-spark-dal09-env5-0025 (1/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 514.0 (TID 411, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 514.0 (TID 409) in 379 ms on yp-spark-dal09-env5-0031 (2/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 514.0 (TID 410) in 400 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:19 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 514.0 (TID 407) in 415 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 514.0 (TID 411) in 378 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 514.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 514 (flatMap at ALS.scala:1170) finished in 0.757 s\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 515, ShuffleMapStage 516)\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(514)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 515 (MapPartitionsRDD[800] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(515)\n",
      "17/01/05 15:43:20 INFO spark.storage.MemoryStore: Block broadcast_108 stored as values in memory (estimated size 52.0 KB, free 1238.0 KB)\n",
      "17/01/05 15:43:20 INFO spark.storage.MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 16.0 KB, free 1254.0 KB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on 10.143.133.19:45040 (size: 16.0 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:20 INFO apache.spark.SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 515 (MapPartitionsRDD[800] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGODeployScheduler: Adding task set 515.0 with 5 tasks\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 515.0 (TID 412, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 515.0 (TID 413, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 515.0 (TID 414, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 515.0 (TID 415, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 53 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 53 is 291 bytes\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 53 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 53 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 53 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 515.0 (TID 416, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 515.0 (TID 413) in 359 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 515.0 (TID 415) in 362 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 515.0 (TID 414) in 370 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 515.0 (TID 412) in 381 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 515.0 (TID 416) in 334 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 515.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 515 (flatMap at ALS.scala:1170) finished in 0.694 s\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 517, ResultStage 518, ShuffleMapStage 516)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(515)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 516 (MapPartitionsRDD[809] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(516)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_107_piece0 on 10.143.133.19:45040 in memory (size: 15.7 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_107_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_107_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_107_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_107_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 15.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.MemoryStore: Block broadcast_109 stored as values in memory (estimated size 52.9 KB, free 1240.0 KB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_106_piece0 on 10.143.133.19:45040 in memory (size: 15.6 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_106_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_106_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_106_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 15.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 16.0 KB, free 1190.2 KB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_106_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 15.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on 10.143.133.19:45040 (size: 16.0 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:20 INFO apache.spark.SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:20 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 516 (MapPartitionsRDD[809] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:20 INFO cluster.ego.EGODeployScheduler: Adding task set 516.0 with 5 tasks\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 516.0 (TID 417, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 516.0 (TID 418, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_105_piece0 on 10.143.133.19:45040 in memory (size: 15.4 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 516.0 (TID 419, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 516.0 (TID 420, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_105_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_105_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_105_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_105_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 15.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_104_piece0 on 10.143.133.19:45040 in memory (size: 15.3 KB, free: 908.7 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_104_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_104_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_104_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 15.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_104_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 15.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_103_piece0 on 10.143.133.19:45040 in memory (size: 15.1 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_103_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_103_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_103_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_103_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 15.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_102_piece0 on 10.143.133.19:45040 in memory (size: 15.0 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_102_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 15.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_102_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 15.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_102_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 15.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_102_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 15.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 52 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 52 is 294 bytes\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_101_piece0 on 10.143.133.19:45040 in memory (size: 14.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_101_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_101_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_101_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 52 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_101_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 52 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_100_piece0 on 10.143.133.19:45040 in memory (size: 14.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_100_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_100_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 52 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_100_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_100_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 14.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_99_piece0 on 10.143.133.19:45040 in memory (size: 14.5 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_99_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_99_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_99_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_99_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 14.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_98_piece0 on 10.143.133.19:45040 in memory (size: 14.4 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_98_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_98_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_98_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 14.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_98_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 14.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_97_piece0 on 10.143.133.19:45040 in memory (size: 14.2 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_97_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_97_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_97_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_97_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 14.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_96_piece0 on 10.143.133.19:45040 in memory (size: 14.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_96_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 14.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_96_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 14.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_96_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 14.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:20 INFO spark.storage.BlockManagerInfo: Removed broadcast_96_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 14.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_95_piece0 on 10.143.133.19:45040 in memory (size: 13.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_95_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_95_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_95_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 13.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_95_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 13.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_94_piece0 on 10.143.133.19:45040 in memory (size: 13.7 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_94_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_94_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_94_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 13.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_94_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 13.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_93_piece0 on 10.143.133.19:45040 in memory (size: 13.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_93_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_93_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_93_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 13.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_93_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 13.5 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.143.133.19:45040 in memory (size: 13.3 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_92_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_92_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 13.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_92_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 13.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_92_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_91_piece0 on 10.143.133.19:45040 in memory (size: 13.1 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_91_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_91_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_91_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 13.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_91_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.143.133.19:45040 in memory (size: 13.0 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_90_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 13.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_90_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 13.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_90_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 13.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_90_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 13.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_89_piece0 on 10.143.133.19:45040 in memory (size: 12.8 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_89_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_89_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 12.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_89_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 12.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_89_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 12.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.143.133.19:45040 in memory (size: 12.6 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_88_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_88_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 12.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_88_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 12.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_88_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 12.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_87_piece0 on 10.143.133.19:45040 in memory (size: 12.4 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_87_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 12.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_87_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 12.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_87_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Removed broadcast_87_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 12.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 516.0 (TID 417) in 378 ms on yp-spark-dal09-env5-0022 (1/5)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 516.0 (TID 418) in 381 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 516.0 (TID 421, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 516.0 (TID 419) in 386 ms on yp-spark-dal09-env5-0031 (3/5)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 516.0 (TID 420) in 394 ms on yp-spark-dal09-env5-0024 (4/5)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 516.0 (TID 421) in 384 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:21 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 516.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 516 (flatMap at ALS.scala:1170) finished in 0.770 s\n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 517, ResultStage 518)\n",
      "17/01/05 15:43:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(516)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 517 (MapPartitionsRDD[818] at flatMap at ALS.scala:1170), which has no missing parents\n",
      "17/01/05 15:43:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(517)\n",
      "17/01/05 15:43:21 INFO spark.storage.MemoryStore: Block broadcast_110 stored as values in memory (estimated size 53.7 KB, free 190.6 KB)\n",
      "17/01/05 15:43:21 INFO spark.storage.MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 16.3 KB, free 206.9 KB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on 10.143.133.19:45040 (size: 16.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:21 INFO apache.spark.SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:21 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 517 (MapPartitionsRDD[818] at flatMap at ALS.scala:1170)\n",
      "17/01/05 15:43:21 INFO cluster.ego.EGODeployScheduler: Adding task set 517.0 with 5 tasks\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 517.0 (TID 422, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 517.0 (TID 423, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 517.0 (TID 424, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:21 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 517.0 (TID 425, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 16.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:21 INFO spark.storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 51 is 291 bytes\n",
      "17/01/05 15:43:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 517.0 (TID 426, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2187 bytes)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 517.0 (TID 425) in 354 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 517.0 (TID 424) in 360 ms on yp-spark-dal09-env5-0025 (2/5)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 517.0 (TID 422) in 367 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 517.0 (TID 423) in 380 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 517.0 (TID 426) in 334 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:22 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 517.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 517 (flatMap at ALS.scala:1170) finished in 0.690 s\n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 518)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(517)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 518 (users MapPartitionsRDD[834] at mapValues at ALS.scala:255), which has no missing parents\n",
      "17/01/05 15:43:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(518)\n",
      "17/01/05 15:43:22 INFO spark.storage.MemoryStore: Block broadcast_111 stored as values in memory (estimated size 54.9 KB, free 261.8 KB)\n",
      "17/01/05 15:43:22 INFO spark.storage.MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 16.6 KB, free 278.4 KB)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on 10.143.133.19:45040 (size: 16.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:22 INFO apache.spark.SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:22 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 518 (users MapPartitionsRDD[834] at mapValues at ALS.scala:255)\n",
      "17/01/05 15:43:22 INFO cluster.ego.EGODeployScheduler: Adding task set 518.0 with 5 tasks\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 518.0 (TID 427, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 518.0 (TID 428, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 518.0 (TID 429, yp-spark-dal09-env5-0025, partition 3,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 518.0 (TID 430, yp-spark-dal09-env5-0031, partition 2,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 50 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:22 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 50 is 294 bytes\n",
      "17/01/05 15:43:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 50 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 50 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 50 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added rdd_834_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 539.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 518.0 (TID 431, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 518.0 (TID 430) in 397 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added rdd_834_0 in memory on yp-spark-dal09-env5-0022:42616 (size: 539.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 518.0 (TID 427) in 416 ms on yp-spark-dal09-env5-0022 (2/5)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added rdd_834_1 in memory on yp-spark-dal09-env5-0024:33191 (size: 539.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 518.0 (TID 428) in 430 ms on yp-spark-dal09-env5-0024 (3/5)\n",
      "17/01/05 15:43:22 INFO spark.storage.BlockManagerInfo: Added rdd_834_3 in memory on yp-spark-dal09-env5-0025:36914 (size: 539.1 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:22 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 518.0 (TID 429) in 463 ms on yp-spark-dal09-env5-0025 (4/5)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added rdd_834_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 539.1 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 518.0 (TID 431) in 385 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:23 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 518.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: ResultStage 518 (count at ALS.scala:263) finished in 0.784 s\n",
      "17/01/05 15:43:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(518)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Job 18 finished: count at ALS.scala:263, took 29.834810 s\n",
      "17/01/05 15:43:23 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:264\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 48 is 332 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 49 is 317 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 47 is 319 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 90 is 307 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 89 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 88 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 87 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 86 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 85 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 84 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 83 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 82 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 81 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 80 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 79 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 78 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 77 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 76 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 75 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 74 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 73 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 72 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 71 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 70 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 69 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 68 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 67 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 66 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 65 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 64 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 63 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 62 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 61 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 60 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 59 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 58 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 57 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 56 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 55 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 54 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 53 is 291 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 52 is 294 bytes\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 51 is 291 bytes\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Got job 19 (count at ALS.scala:264) with 5 output partitions\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 563 (count at ALS.scala:264)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 520, ShuffleMapStage 562)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 563 (products MapPartitionsRDD[835] at mapValues at ALS.scala:259), which has no missing parents\n",
      "17/01/05 15:43:23 INFO spark.storage.MemoryStore: Block broadcast_112 stored as values in memory (estimated size 54.1 KB, free 332.5 KB)\n",
      "17/01/05 15:43:23 INFO spark.storage.MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 16.4 KB, free 348.9 KB)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on 10.143.133.19:45040 (size: 16.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:23 INFO apache.spark.SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 563 (products MapPartitionsRDD[835] at mapValues at ALS.scala:259)\n",
      "17/01/05 15:43:23 INFO cluster.ego.EGODeployScheduler: Adding task set 563.0 with 5 tasks\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 563.0 (TID 432, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 563.0 (TID 433, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 563.0 (TID 434, yp-spark-dal09-env5-0025, partition 1,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 563.0 (TID 435, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(563)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 16.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 51 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added rdd_835_0 in memory on yp-spark-dal09-env5-0031:45704 (size: 314.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 563.0 (TID 436, yp-spark-dal09-env5-0031, partition 4,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 563.0 (TID 435) in 370 ms on yp-spark-dal09-env5-0031 (1/5)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added rdd_835_2 in memory on yp-spark-dal09-env5-0024:33191 (size: 314.7 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 563.0 (TID 432) in 380 ms on yp-spark-dal09-env5-0024 (2/5)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added rdd_835_1 in memory on yp-spark-dal09-env5-0025:36914 (size: 314.7 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 563.0 (TID 434) in 385 ms on yp-spark-dal09-env5-0025 (3/5)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added rdd_835_3 in memory on yp-spark-dal09-env5-0022:42616 (size: 314.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 563.0 (TID 433) in 402 ms on yp-spark-dal09-env5-0022 (4/5)\n",
      "17/01/05 15:43:23 INFO spark.storage.BlockManagerInfo: Added rdd_835_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 314.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 563.0 (TID 436) in 351 ms on yp-spark-dal09-env5-0031 (5/5)\n",
      "17/01/05 15:43:23 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 563.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: ResultStage 563 (count at ALS.scala:264) finished in 0.722 s\n",
      "17/01/05 15:43:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(563)\n",
      "17/01/05 15:43:23 INFO spark.scheduler.DAGScheduler: Job 19 finished: count at ALS.scala:264, took 0.742627 s\n",
      "17/01/05 15:43:23 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Got job 20 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 609 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 608, ShuffleMapStage 567)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 609 (users MapPartitionsRDD[834] at mapValues at ALS.scala:255), which has no missing parents\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_113 stored as values in memory (estimated size 55.1 KB, free 404.0 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 16.6 KB, free 420.6 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on 10.143.133.19:45040 (size: 16.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 609 (users MapPartitionsRDD[834] at mapValues at ALS.scala:255)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Adding task set 609.0 with 1 tasks\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 609.0 (TID 437, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(609)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 609.0 (TID 437) in 12 ms on yp-spark-dal09-env5-0022 (1/1)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 609.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: ResultStage 609 (first at MatrixFactorizationModel.scala:67) finished in 0.013 s\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(609)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Job 20 finished: first at MatrixFactorizationModel.scala:67, took 0.027552 s\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Got job 21 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 654 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 611, ShuffleMapStage 653)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 654 (products MapPartitionsRDD[835] at mapValues at ALS.scala:259), which has no missing parents\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_114 stored as values in memory (estimated size 54.2 KB, free 474.9 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 16.5 KB, free 491.4 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on 10.143.133.19:45040 (size: 16.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 654 (products MapPartitionsRDD[835] at mapValues at ALS.scala:259)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Adding task set 654.0 with 1 tasks\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 654.0 (TID 438, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(654)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 654.0 (TID 438) in 10 ms on yp-spark-dal09-env5-0031 (1/1)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 654.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: ResultStage 654 (first at MatrixFactorizationModel.scala:67) finished in 0.011 s\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(654)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Job 21 finished: first at MatrixFactorizationModel.scala:67, took 0.023557 s\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Got job 22 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 700 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 658, ShuffleMapStage 699)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 700 (users MapPartitionsRDD[834] at mapValues at ALS.scala:255), which has no missing parents\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_115 stored as values in memory (estimated size 55.1 KB, free 546.5 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 16.6 KB, free 563.1 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on 10.143.133.19:45040 (size: 16.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 700 (users MapPartitionsRDD[834] at mapValues at ALS.scala:255)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Adding task set 700.0 with 1 tasks\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 700.0 (TID 439, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(700)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 700.0 (TID 439) in 9 ms on yp-spark-dal09-env5-0022 (1/1)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 700.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: ResultStage 700 (first at MatrixFactorizationModel.scala:67) finished in 0.009 s\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(700)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Job 22 finished: first at MatrixFactorizationModel.scala:67, took 0.022707 s\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Got job 23 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 745 (first at MatrixFactorizationModel.scala:67)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 702, ShuffleMapStage 744)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 745 (products MapPartitionsRDD[835] at mapValues at ALS.scala:259), which has no missing parents\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_116 stored as values in memory (estimated size 54.2 KB, free 617.3 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 16.5 KB, free 633.8 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on 10.143.133.19:45040 (size: 16.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 745 (products MapPartitionsRDD[835] at mapValues at ALS.scala:259)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Adding task set 745.0 with 1 tasks\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 745.0 (TID 440, yp-spark-dal09-env5-0031, partition 0,PROCESS_LOCAL, 2198 bytes)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 16.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(745)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 745.0 (TID 440) in 8 ms on yp-spark-dal09-env5-0031 (1/1)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 745.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: ResultStage 745 (first at MatrixFactorizationModel.scala:67) finished in 0.008 s\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(745)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Job 23 finished: first at MatrixFactorizationModel.scala:67, took 0.020752 s\n",
      "17/01/05 15:43:24 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:43:24 CST]\n",
      "17/01/05 15:43:24 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:43:24 CST]\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Starting job: runJob at PythonRDD.scala:393\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Registering RDD 839 (flatMap at MatrixFactorizationModel.scala:278)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Got job 24 (runJob at PythonRDD.scala:393) with 1 output partitions\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 835 (runJob at PythonRDD.scala:393)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 834)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 834)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 834 (MapPartitionsRDD[839] at flatMap at MatrixFactorizationModel.scala:278), which has no missing parents\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_117 stored as values in memory (estimated size 58.4 KB, free 692.3 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 17.9 KB, free 710.2 KB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on 10.143.133.19:45040 (size: 17.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:24 INFO apache.spark.SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:24 INFO spark.scheduler.DAGScheduler: Submitting 25 missing tasks from ShuffleMapStage 834 (MapPartitionsRDD[839] at flatMap at MatrixFactorizationModel.scala:278)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGODeployScheduler: Adding task set 834.0 with 25 tasks\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 5.0 in stage 834.0 (TID 441, yp-spark-dal09-env5-0024, partition 5,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 15.0 in stage 834.0 (TID 442, yp-spark-dal09-env5-0025, partition 15,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 10.0 in stage 834.0 (TID 443, yp-spark-dal09-env5-0031, partition 10,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 834.0 (TID 444, yp-spark-dal09-env5-0022, partition 0,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:24 INFO spark.storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(834)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 6.0 in stage 834.0 (TID 445, yp-spark-dal09-env5-0024, partition 6,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Finished task 5.0 in stage 834.0 (TID 441) in 525 ms on yp-spark-dal09-env5-0024 (1/25)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Starting task 11.0 in stage 834.0 (TID 446, yp-spark-dal09-env5-0031, partition 11,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:24 INFO spark.scheduler.TaskSetManager: Finished task 10.0 in stage 834.0 (TID 443) in 641 ms on yp-spark-dal09-env5-0031 (2/25)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Starting task 7.0 in stage 834.0 (TID 447, yp-spark-dal09-env5-0024, partition 7,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Finished task 6.0 in stage 834.0 (TID 445) in 522 ms on yp-spark-dal09-env5-0024 (3/25)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Starting task 16.0 in stage 834.0 (TID 448, yp-spark-dal09-env5-0025, partition 16,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Finished task 15.0 in stage 834.0 (TID 442) in 1169 ms on yp-spark-dal09-env5-0025 (4/25)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Starting task 12.0 in stage 834.0 (TID 449, yp-spark-dal09-env5-0031, partition 12,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Finished task 11.0 in stage 834.0 (TID 446) in 603 ms on yp-spark-dal09-env5-0031 (5/25)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 834.0 (TID 450, yp-spark-dal09-env5-0022, partition 1,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 834.0 (TID 444) in 1342 ms on yp-spark-dal09-env5-0022 (6/25)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Starting task 8.0 in stage 834.0 (TID 451, yp-spark-dal09-env5-0024, partition 8,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Finished task 7.0 in stage 834.0 (TID 447) in 502 ms on yp-spark-dal09-env5-0024 (7/25)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Starting task 17.0 in stage 834.0 (TID 452, yp-spark-dal09-env5-0025, partition 17,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:25 INFO spark.scheduler.TaskSetManager: Finished task 16.0 in stage 834.0 (TID 448) in 619 ms on yp-spark-dal09-env5-0025 (8/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 13.0 in stage 834.0 (TID 453, yp-spark-dal09-env5-0031, partition 13,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 12.0 in stage 834.0 (TID 449) in 656 ms on yp-spark-dal09-env5-0031 (9/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 834.0 (TID 454, yp-spark-dal09-env5-0022, partition 2,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 834.0 (TID 450) in 652 ms on yp-spark-dal09-env5-0022 (10/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 14.0 in stage 834.0 (TID 455, yp-spark-dal09-env5-0031, partition 14,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 9.0 in stage 834.0 (TID 456, yp-spark-dal09-env5-0024, partition 9,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 8.0 in stage 834.0 (TID 451) in 575 ms on yp-spark-dal09-env5-0024 (11/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 18.0 in stage 834.0 (TID 457, yp-spark-dal09-env5-0025, partition 18,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 17.0 in stage 834.0 (TID 452) in 620 ms on yp-spark-dal09-env5-0025 (12/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 9.0 in stage 834.0 (TID 456) in 538 ms on yp-spark-dal09-env5-0024 (13/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 834.0 (TID 458, yp-spark-dal09-env5-0022, partition 3,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 834.0 (TID 454) in 642 ms on yp-spark-dal09-env5-0022 (14/25)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Starting task 20.0 in stage 834.0 (TID 459, yp-spark-dal09-env5-0031, partition 20,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:26 INFO spark.scheduler.TaskSetManager: Finished task 13.0 in stage 834.0 (TID 453) in 811 ms on yp-spark-dal09-env5-0031 (15/25)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Starting task 21.0 in stage 834.0 (TID 460, yp-spark-dal09-env5-0031, partition 21,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Finished task 14.0 in stage 834.0 (TID 455) in 779 ms on yp-spark-dal09-env5-0031 (16/25)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Starting task 19.0 in stage 834.0 (TID 461, yp-spark-dal09-env5-0025, partition 19,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Finished task 18.0 in stage 834.0 (TID 457) in 672 ms on yp-spark-dal09-env5-0025 (17/25)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Starting task 22.0 in stage 834.0 (TID 462, yp-spark-dal09-env5-0031, partition 22,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Finished task 20.0 in stage 834.0 (TID 459) in 606 ms on yp-spark-dal09-env5-0031 (18/25)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 834.0 (TID 463, yp-spark-dal09-env5-0022, partition 4,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 834.0 (TID 458) in 688 ms on yp-spark-dal09-env5-0022 (19/25)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Starting task 23.0 in stage 834.0 (TID 464, yp-spark-dal09-env5-0031, partition 23,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Finished task 21.0 in stage 834.0 (TID 460) in 674 ms on yp-spark-dal09-env5-0031 (20/25)\n",
      "17/01/05 15:43:27 INFO spark.scheduler.TaskSetManager: Finished task 19.0 in stage 834.0 (TID 461) in 609 ms on yp-spark-dal09-env5-0025 (21/25)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 834.0 (TID 463) in 632 ms on yp-spark-dal09-env5-0022 (22/25)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Starting task 24.0 in stage 834.0 (TID 465, yp-spark-dal09-env5-0031, partition 24,PROCESS_LOCAL, 2519 bytes)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Finished task 22.0 in stage 834.0 (TID 462) in 700 ms on yp-spark-dal09-env5-0031 (23/25)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Finished task 23.0 in stage 834.0 (TID 464) in 668 ms on yp-spark-dal09-env5-0031 (24/25)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Finished task 24.0 in stage 834.0 (TID 465) in 611 ms on yp-spark-dal09-env5-0031 (25/25)\n",
      "17/01/05 15:43:28 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 834.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 834 (flatMap at MatrixFactorizationModel.scala:278) finished in 4.628 s\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: running: Set()\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 835)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: failed: Set()\n",
      "17/01/05 15:43:28 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(834)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 835 (PythonRDD[845] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "17/01/05 15:43:28 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(835)\n",
      "17/01/05 15:43:28 INFO spark.storage.MemoryStore: Block broadcast_118 stored as values in memory (estimated size 60.5 KB, free 770.7 KB)\n",
      "17/01/05 15:43:28 INFO spark.storage.MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 18.9 KB, free 789.7 KB)\n",
      "17/01/05 15:43:28 INFO spark.storage.BlockManagerInfo: Added broadcast_118_piece0 in memory on 10.143.133.19:45040 (size: 18.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:28 INFO apache.spark.SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 835 (PythonRDD[845] at RDD at PythonRDD.scala:43)\n",
      "17/01/05 15:43:28 INFO cluster.ego.EGODeployScheduler: Adding task set 835.0 with 1 tasks\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 835.0 (TID 466, yp-spark-dal09-env5-0022, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:28 INFO spark.storage.BlockManagerInfo: Added broadcast_118_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 18.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:28 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 91 to yp-spark-dal09-env5-0022:60288\n",
      "17/01/05 15:43:28 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 91 is 328 bytes\n",
      "17/01/05 15:43:28 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 835.0 (TID 466) in 115 ms on yp-spark-dal09-env5-0022 (1/1)\n",
      "17/01/05 15:43:28 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 835.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: ResultStage 835 (runJob at PythonRDD.scala:393) finished in 0.116 s\n",
      "17/01/05 15:43:28 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(835)\n",
      "17/01/05 15:43:28 INFO spark.scheduler.DAGScheduler: Job 24 finished: runJob at PythonRDD.scala:393, took 4.770745 s\n",
      "17/01/05 15:43:28 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:43:28 CST]\n",
      "17/01/05 15:43:29 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483652608]\n",
      "17/01/05 15:43:29 INFO apache.spark.SparkContext: Starting job: collect at <ipython-input-20-572bcd7b1f24>:117\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 48 is 332 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 49 is 317 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 47 is 319 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 90 is 307 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 89 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 88 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 87 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 86 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 85 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 84 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 83 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 82 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 81 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 80 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 79 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 78 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 77 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 76 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 75 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 74 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 73 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 72 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 71 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 70 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 69 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 68 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 67 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 66 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 65 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 64 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 63 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 62 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 61 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 60 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 59 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 58 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 57 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 56 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 55 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 54 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 53 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 52 is 294 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 51 is 291 bytes\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 50 is 294 bytes\n",
      "17/01/05 15:43:29 INFO spark.scheduler.DAGScheduler: Got job 25 (collect at <ipython-input-20-572bcd7b1f24>:117) with 25 output partitions\n",
      "17/01/05 15:43:29 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 925 (collect at <ipython-input-20-572bcd7b1f24>:117)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 924)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n",
      "17/01/05 15:43:29 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 925 (PythonRDD[855] at collect at <ipython-input-20-572bcd7b1f24>:117), which has no missing parents\n",
      "17/01/05 15:43:29 INFO spark.storage.MemoryStore: Block broadcast_119 stored as values in memory (estimated size 71.9 KB, free 861.6 KB)\n",
      "17/01/05 15:43:29 INFO spark.storage.MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 24.8 KB, free 886.3 KB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on 10.143.133.19:45040 (size: 24.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:29 INFO apache.spark.SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1006\n",
      "17/01/05 15:43:29 INFO spark.scheduler.DAGScheduler: Submitting 25 missing tasks from ResultStage 925 (PythonRDD[855] at collect at <ipython-input-20-572bcd7b1f24>:117)\n",
      "17/01/05 15:43:29 INFO cluster.ego.EGODeployScheduler: Adding task set 925.0 with 25 tasks\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 925.0 (TID 467, yp-spark-dal09-env5-0024, partition 1,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 925.0 (TID 468, yp-spark-dal09-env5-0025, partition 3,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 925.0 (TID 469, yp-spark-dal09-env5-0031, partition 2,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 925.0 (TID 470, yp-spark-dal09-env5-0022, partition 0,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on yp-spark-dal09-env5-0031:45704 (size: 24.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on yp-spark-dal09-env5-0024:33191 (size: 24.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on yp-spark-dal09-env5-0022:42616 (size: 24.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on yp-spark-dal09-env5-0025:36914 (size: 24.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(925)\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 91 to yp-spark-dal09-env5-0031:48096\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 91 to yp-spark-dal09-env5-0024:38796\n",
      "17/01/05 15:43:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 91 to yp-spark-dal09-env5-0025:36366\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_1 in memory on yp-spark-dal09-env5-0024:33191 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_2 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 925.0 (TID 471, yp-spark-dal09-env5-0031, partition 4,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 925.0 (TID 469) in 142 ms on yp-spark-dal09-env5-0031 (1/25)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 6.0 in stage 925.0 (TID 472, yp-spark-dal09-env5-0024, partition 6,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 925.0 (TID 467) in 158 ms on yp-spark-dal09-env5-0024 (2/25)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_4 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_6 in memory on yp-spark-dal09-env5-0024:33191 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 7.0 in stage 925.0 (TID 473, yp-spark-dal09-env5-0031, partition 7,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 925.0 (TID 471) in 125 ms on yp-spark-dal09-env5-0031 (3/25)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 11.0 in stage 925.0 (TID 474, yp-spark-dal09-env5-0024, partition 11,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 6.0 in stage 925.0 (TID 472) in 129 ms on yp-spark-dal09-env5-0024 (4/25)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_3 in memory on yp-spark-dal09-env5-0025:36914 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_0 in memory on yp-spark-dal09-env5-0022:42616 (size: 99.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_7 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_11 in memory on yp-spark-dal09-env5-0024:33191 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 9.0 in stage 925.0 (TID 475, yp-spark-dal09-env5-0031, partition 9,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 7.0 in stage 925.0 (TID 473) in 124 ms on yp-spark-dal09-env5-0031 (5/25)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 16.0 in stage 925.0 (TID 476, yp-spark-dal09-env5-0024, partition 16,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 11.0 in stage 925.0 (TID 474) in 132 ms on yp-spark-dal09-env5-0024 (6/25)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 8.0 in stage 925.0 (TID 477, yp-spark-dal09-env5-0025, partition 8,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 925.0 (TID 468) in 419 ms on yp-spark-dal09-env5-0025 (7/25)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 5.0 in stage 925.0 (TID 478, yp-spark-dal09-env5-0022, partition 5,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 925.0 (TID 470) in 463 ms on yp-spark-dal09-env5-0022 (8/25)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_9 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_16 in memory on yp-spark-dal09-env5-0024:33191 (size: 99.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 12.0 in stage 925.0 (TID 479, yp-spark-dal09-env5-0031, partition 12,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 9.0 in stage 925.0 (TID 475) in 126 ms on yp-spark-dal09-env5-0031 (9/25)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_8 in memory on yp-spark-dal09-env5-0025:36914 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 21.0 in stage 925.0 (TID 480, yp-spark-dal09-env5-0024, partition 21,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 16.0 in stage 925.0 (TID 476) in 125 ms on yp-spark-dal09-env5-0024 (10/25)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_5 in memory on yp-spark-dal09-env5-0022:42616 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 13.0 in stage 925.0 (TID 481, yp-spark-dal09-env5-0025, partition 13,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 8.0 in stage 925.0 (TID 477) in 158 ms on yp-spark-dal09-env5-0025 (11/25)\n",
      "17/01/05 15:43:29 INFO spark.storage.BlockManagerInfo: Added rdd_851_12 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Starting task 10.0 in stage 925.0 (TID 482, yp-spark-dal09-env5-0022, partition 10,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:29 INFO spark.scheduler.TaskSetManager: Finished task 5.0 in stage 925.0 (TID 478) in 155 ms on yp-spark-dal09-env5-0022 (12/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_21 in memory on yp-spark-dal09-env5-0024:33191 (size: 99.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 14.0 in stage 925.0 (TID 483, yp-spark-dal09-env5-0031, partition 14,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 12.0 in stage 925.0 (TID 479) in 123 ms on yp-spark-dal09-env5-0031 (13/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_13 in memory on yp-spark-dal09-env5-0025:36914 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 21.0 in stage 925.0 (TID 480) in 127 ms on yp-spark-dal09-env5-0024 (14/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_10 in memory on yp-spark-dal09-env5-0022:42616 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 18.0 in stage 925.0 (TID 484, yp-spark-dal09-env5-0025, partition 18,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 13.0 in stage 925.0 (TID 481) in 138 ms on yp-spark-dal09-env5-0025 (15/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_14 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 17.0 in stage 925.0 (TID 485, yp-spark-dal09-env5-0031, partition 17,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 14.0 in stage 925.0 (TID 483) in 120 ms on yp-spark-dal09-env5-0031 (16/25)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 15.0 in stage 925.0 (TID 486, yp-spark-dal09-env5-0022, partition 15,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 10.0 in stage 925.0 (TID 482) in 167 ms on yp-spark-dal09-env5-0022 (17/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_18 in memory on yp-spark-dal09-env5-0025:36914 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_17 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 23.0 in stage 925.0 (TID 487, yp-spark-dal09-env5-0025, partition 23,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 18.0 in stage 925.0 (TID 484) in 134 ms on yp-spark-dal09-env5-0025 (18/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_15 in memory on yp-spark-dal09-env5-0022:42616 (size: 99.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 19.0 in stage 925.0 (TID 488, yp-spark-dal09-env5-0031, partition 19,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 17.0 in stage 925.0 (TID 485) in 119 ms on yp-spark-dal09-env5-0031 (19/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_23 in memory on yp-spark-dal09-env5-0025:36914 (size: 99.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 20.0 in stage 925.0 (TID 489, yp-spark-dal09-env5-0022, partition 20,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 15.0 in stage 925.0 (TID 486) in 159 ms on yp-spark-dal09-env5-0022 (20/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_19 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 22.0 in stage 925.0 (TID 490, yp-spark-dal09-env5-0031, partition 22,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 19.0 in stage 925.0 (TID 488) in 116 ms on yp-spark-dal09-env5-0031 (21/25)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 23.0 in stage 925.0 (TID 487) in 155 ms on yp-spark-dal09-env5-0025 (22/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_20 in memory on yp-spark-dal09-env5-0022:42616 (size: 99.2 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 20.0 in stage 925.0 (TID 489) in 132 ms on yp-spark-dal09-env5-0022 (23/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_22 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Starting task 24.0 in stage 925.0 (TID 491, yp-spark-dal09-env5-0031, partition 24,NODE_LOCAL, 1894 bytes)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 22.0 in stage 925.0 (TID 490) in 121 ms on yp-spark-dal09-env5-0031 (24/25)\n",
      "17/01/05 15:43:30 INFO spark.storage.BlockManagerInfo: Added rdd_851_24 in memory on yp-spark-dal09-env5-0031:45704 (size: 99.2 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.TaskSetManager: Finished task 24.0 in stage 925.0 (TID 491) in 121 ms on yp-spark-dal09-env5-0031 (25/25)\n",
      "17/01/05 15:43:30 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 925.0, whose tasks have all completed, from pool \n",
      "17/01/05 15:43:30 INFO spark.scheduler.DAGScheduler: ResultStage 925 (collect at <ipython-input-20-572bcd7b1f24>:117) finished in 1.235 s\n",
      "17/01/05 15:43:30 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(925)\n",
      "17/01/05 15:43:30 INFO spark.scheduler.DAGScheduler: Job 25 finished: collect at <ipython-input-20-572bcd7b1f24>:117, took 1.267332 s\n",
      "17/01/05 15:43:30 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:43:30 CST]\n",
      "17/01/05 15:43:30 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:43:30 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:43:31 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:43:32 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_119_piece0 on 10.143.133.19:45040 in memory (size: 24.8 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_119_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 24.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_119_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 24.8 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_119_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 24.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_119_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 24.8 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 140\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_118_piece0 on 10.143.133.19:45040 in memory (size: 18.9 KB, free: 908.8 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_118_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 18.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 138\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_117_piece0 on 10.143.133.19:45040 in memory (size: 17.9 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_117_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_117_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 17.9 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_117_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_117_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 17.9 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 137\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_116_piece0 on 10.143.133.19:45040 in memory (size: 16.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_116_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 136\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_115_piece0 on 10.143.133.19:45040 in memory (size: 16.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_115_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 135\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_114_piece0 on 10.143.133.19:45040 in memory (size: 16.5 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_114_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.5 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 134\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_113_piece0 on 10.143.133.19:45040 in memory (size: 16.6 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_113_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 133\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_112_piece0 on 10.143.133.19:45040 in memory (size: 16.4 KB, free: 908.9 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_112_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_112_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.4 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_112_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 16.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_112_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.4 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 132\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_111_piece0 on 10.143.133.19:45040 in memory (size: 16.6 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_111_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_111_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.6 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_111_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_111_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.6 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 131\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_110_piece0 on 10.143.133.19:45040 in memory (size: 16.3 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_110_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_110_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.3 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_110_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_110_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 16.3 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 130\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_109_piece0 on 10.143.133.19:45040 in memory (size: 16.0 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_109_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_109_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_109_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_109_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 129\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_108_piece0 on 10.143.133.19:45040 in memory (size: 16.0 KB, free: 909.0 MB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_108_piece0 on yp-spark-dal09-env5-0024:33191 in memory (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_108_piece0 on yp-spark-dal09-env5-0031:45704 in memory (size: 16.0 KB, free: 4.2 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_108_piece0 on yp-spark-dal09-env5-0022:42616 in memory (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO spark.storage.BlockManagerInfo: Removed broadcast_108_piece0 on yp-spark-dal09-env5-0025:36914 in memory (size: 16.0 KB, free: 4.3 GB)\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 128\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 127\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 126\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 125\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 124\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 123\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 122\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 121\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 120\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 119\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 118\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 117\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 116\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 115\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 114\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 113\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 112\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 111\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 110\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 109\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 108\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 107\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 106\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 105\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 104\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 103\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 102\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 101\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 100\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 99\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 98\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 97\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 96\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 95\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 94\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 93\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 92\n",
      "17/01/05 15:43:33 INFO apache.spark.ContextCleaner: Cleaned accumulator 91\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:43:33 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:43:34 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:43:35 CST]\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:43:36 CST]\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:43:36 CST]\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:43:36 CST]\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:43:36 CST]\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:43:36 CST]\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:43:36 CST]\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:43:37 CST]\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:43:37 CST]\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:43:37 CST]\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:43:37 CST]\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:43:37 CST]\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:43:37 CST]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:43:38 CST]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:43:38 CST]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:43:38 CST]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:43:38 CST]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:43:38 CST]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483652608]\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483652608, 2017-01-05 15:43:38 CST]\n"
     ]
    }
   ],
   "source": [
    "# dump the latest kernel log\n",
    "! cat $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/05 15:25:00 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:25:00 CST]\r\n",
      "17/01/05 15:32:49 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:32:49 CST]\r\n",
      "17/01/05 15:34:01 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "17/01/05 15:34:01 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:34:01 CST]\r\n",
      "17/01/05 15:34:52 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:34:52 CST]\r\n",
      "17/01/05 15:34:52 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:34:52 CST]\r\n",
      "17/01/05 15:35:21 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:35:21 CST]\r\n",
      "17/01/05 15:35:21 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483650939]\r\n",
      "17/01/05 15:35:21 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483652121]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:35:23 CST]\r\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:35:24 CST]\r\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:35:24 CST]\r\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:35:24 CST]\r\n",
      "17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:35:24 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:35:25 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:35:26 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:35:27 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:35:28 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:35:29 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:35:30 CST]\r\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:35:31 CST]\r\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:35:31 CST]\r\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:35:31 CST]\r\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483652121]\r\n",
      "17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483652121, 2017-01-05 15:35:31 CST]\r\n",
      "17/01/05 15:40:13 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:40:13 CST]\r\n",
      "17/01/05 15:41:16 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:41:16 CST]\r\n",
      "17/01/05 15:42:47 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "17/01/05 15:42:47 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:42:47 CST]\r\n",
      "17/01/05 15:43:24 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:43:24 CST]\r\n",
      "17/01/05 15:43:24 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:43:24 CST]\r\n",
      "17/01/05 15:43:28 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:43:28 CST]\r\n",
      "17/01/05 15:43:29 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483652608]\r\n",
      "17/01/05 15:43:30 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:43:30 CST]\r\n",
      "17/01/05 15:43:30 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:43:30 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:43:31 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:43:32 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:43:33 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:43:34 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:43:35 CST]\r\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:43:36 CST]\r\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:43:36 CST]\r\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:43:36 CST]\r\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:43:36 CST]\r\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:43:36 CST]\r\n",
      "17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:43:36 CST]\r\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:43:37 CST]\r\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:43:37 CST]\r\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:43:37 CST]\r\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:43:37 CST]\r\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:43:37 CST]\r\n",
      "17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:43:37 CST]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:43:38 CST]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:43:38 CST]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:43:38 CST]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:43:38 CST]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:43:38 CST]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483652608]\r\n",
      "17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483652608, 2017-01-05 15:43:38 CST]\r\n"
     ]
    }
   ],
   "source": [
    "# look for our log output in the latest kernel log file\n",
    "! grep 'CloudantRecommender' $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:49:08 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 10:49:08]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:53:21 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 10:53:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:03 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:03 INFO CloudantRecommender: [Starting train model: , 2017-01-05 10:56:03]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:34 INFO CloudantRecommender: [Finished train model: , 2017-01-05 10:56:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:34 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 10:56:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:39 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 10:56:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:39 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483634178]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:39 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483635399]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:41 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 10:56:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:42 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 10:56:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:43 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 10:56:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:44 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 10:56:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:45 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 10:56:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:45 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 10:56:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:45 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 10:56:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:45 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 10:56:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:45 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 10:56:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:45 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 10:56:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:46 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 10:56:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:47 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 10:56:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:48 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 10:56:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:48 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 10:56:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:48 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 10:56:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:48 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 10:56:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:48 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 10:56:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:48 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 10:56:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:49 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 10:56:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:49 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 10:56:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:49 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 10:56:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 10:56:49 ERROR CloudantRecommender: ['recommendation_metadata', 2017-01-05 10:56:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:04:57 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 11:04:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:11 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 11:05:11]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:11 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483635399]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:12 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483635842]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:12 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483635912]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:14 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 11:05:14]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:14 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 11:05:14]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:14 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 11:05:14]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:14 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 11:05:14]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:14 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 11:05:14]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:14 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 11:05:14]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:15 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 11:05:15]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:15 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 11:05:15]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:15 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 11:05:15]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:15 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 11:05:15]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:15 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 11:05:15]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:16 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 11:05:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:17 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 11:05:17]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:18 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 11:05:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:19 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 11:05:19]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:20 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 11:05:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:21 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 11:05:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:22 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 11:05:22]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:22 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 11:05:22]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:22 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 11:05:22]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:22 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 11:05:22]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:22 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 11:05:22]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_164844.log:17/01/05 11:05:22 ERROR CloudantRecommender: ['recommendation_metadata', Traceback (most recent call last):\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:07:35 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 11:07:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:09:18 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 11:09:18]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:10:41 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:10:41 INFO CloudantRecommender: [Starting train model: , 2017-01-05 11:10:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:40 INFO CloudantRecommender: [Finished train model: , 2017-01-05 11:11:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:40 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 11:11:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:47 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 11:11:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:47 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483635912]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:48 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483636307]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:51 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 11:11:51]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:51 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 11:11:51]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:51 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 11:11:51]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:52 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 11:11:52]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:53 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 11:11:53]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:54 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 11:11:54]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:55 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 11:11:55]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:56 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 11:11:56]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:56 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 11:11:56]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:56 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 11:11:56]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:56 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 11:11:56]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:56 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 11:11:56]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:56 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 11:11:56]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:57 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 11:11:57]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:58 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 11:11:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:58 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 11:11:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:58 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 11:11:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:58 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 11:11:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:58 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 11:11:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:58 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 11:11:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 11:11:59]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:11:59 ERROR CloudantRecommender: ['CloudantDatabase' object has no attribute 'meta_db', Traceback (most recent call last):\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:14:22 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 11:14:22]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:15:48 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 11:15:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:18:58 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:18:58 INFO CloudantRecommender: [Starting train model: , 2017-01-05 11:18:58]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:32 INFO CloudantRecommender: [Finished train model: , 2017-01-05 11:19:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:32 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 11:19:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:39 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 11:19:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:40 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483636307]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:40 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483636780]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:42 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 11:19:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:42 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 11:19:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:42 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 11:19:42]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:43 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 11:19:43]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:44 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 11:19:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:44 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 11:19:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:44 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 11:19:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:44 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 11:19:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:44 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 11:19:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:44 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 11:19:44]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:45 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 11:19:45]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:46 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 11:19:46]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:47 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 11:19:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:47 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 11:19:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:47 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 11:19:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:47 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 11:19:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:47 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 11:19:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:47 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 11:19:47]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:48 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 11:19:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:48 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 11:19:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:48 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 11:19:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:48 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 11:19:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:48 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 11:19:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:49 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 11:19:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:50 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 11:19:50]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:51 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 11:19:51]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:51 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 11:19:51]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:51 INFO CloudantRecommender: [Saved recommendationdb metadata record, {'_id': 'recommendation_metadata', 'latest_db': 'recommendationdb_1483636780'}]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:19:51 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483636780, 2017-01-05 11:19:51]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:28:49 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 11:28:49]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:32:16 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 11:32:16]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:35:33 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:35:33 INFO CloudantRecommender: [Starting train model: , 2017-01-05 11:35:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:20 INFO CloudantRecommender: [Finished train model: , 2017-01-05 11:37:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:20 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 11:37:20]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:23 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 11:37:23]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:24 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483636780]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:24 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483637844]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:26 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 11:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:27 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 11:37:27]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:27 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 11:37:27]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:27 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 11:37:27]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:27 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 11:37:27]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:28 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 11:37:28]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:29 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 11:37:29]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:30 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 11:37:30]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:31 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 11:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:32 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 11:37:32]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 11:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 11:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 11:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 11:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 11:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 11:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 11:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 11:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 11:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 11:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:34 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483637844]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_170634.log:17/01/05 11:37:34 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483637844, 2017-01-05 11:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:29:35 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 13:29:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:33:21 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 13:33:21]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:36:48 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:36:48 INFO CloudantRecommender: [Starting train model: , 2017-01-05 13:36:48]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:26 INFO CloudantRecommender: [Finished train model: , 2017-01-05 13:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:26 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 13:37:26]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:31 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 13:37:31]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:31 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483639447]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:31 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483643670]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:31 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483645051]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:33 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 13:37:33]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 13:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 13:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 13:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 13:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:34 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 13:37:34]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:35 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 13:37:35]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:36 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 13:37:36]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:37 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 13:37:37]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:38 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 13:37:38]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:39 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 13:37:39]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:40 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 13:37:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:40 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 13:37:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:40 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 13:37:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:40 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 13:37:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:40 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 13:37:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:40 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 13:37:40]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:41 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 13:37:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:41 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 13:37:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:41 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 13:37:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:41 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483645051]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:37:41 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483645051, 2017-01-05 13:37:41]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:50:18 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 13:50:18 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:51:57 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 13:51:57 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:53:31 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:53:31 INFO CloudantRecommender: [Starting train model: , 2017-01-05 13:53:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:10 INFO CloudantRecommender: [Finished train model: , 2017-01-05 13:54:10 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:10 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 13:54:10 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:17 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 13:54:17 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:17 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483645051]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:17 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483646057]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:19 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 13:54:19 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:19 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 13:54:19 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:19 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 13:54:19 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:19 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 13:54:19 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:19 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 13:54:19 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:20 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 13:54:20 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:21 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 13:54:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:22 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 13:54:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:23 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 13:54:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:24 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 13:54:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 13:54:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 13:54:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 13:54:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 13:54:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 13:54:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:25 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 13:54:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:26 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 13:54:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:26 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 13:54:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:26 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 13:54:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:26 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 13:54:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:26 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 13:54:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:26 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 13:54:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:27 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 13:54:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:27 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 13:54:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:27 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483646057]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 13:54:27 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483646057, 2017-01-05 13:54:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:10:55 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:10:55 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:12:42 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:12:42 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:14:22 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:14:22 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:14:22 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:31 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:15:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:31 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:15:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:39 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:15:39 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:39 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483646822]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:39 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483650263]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:40 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483650939]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:43 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:15:43 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:43 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:15:43 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:43 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:15:43 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:43 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:15:43 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:43 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:15:43 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:44 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:15:44 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:45 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:15:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:46 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:15:46 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:47 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:15:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:48 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:15:48 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:49 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:15:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:50 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:15:50 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:51 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:15:51 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:51 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:15:51 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:51 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483650939]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:15:51 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483650939, 2017-01-05 15:15:51 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_185244.log:17/01/05 15:19:45 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:19:45 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:25:00 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:25:00 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:32:49 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:32:49 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:34:01 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:34:01 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:34:01 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:34:52 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:34:52 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:34:52 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:34:52 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:21 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:35:21 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:21 INFO CloudantRecommender: [Deleted old recommendations db, recommendationdb_1483650939]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:21 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483652121]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:23 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:35:23 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:35:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:35:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:35:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:24 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:35:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:25 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:35:25 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:26 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:35:26 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:27 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:35:27 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:28 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:35:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:29 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:35:29 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:30 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:35:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:35:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:35:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:35:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:31 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483652121]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:35:31 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483652121, 2017-01-05 15:35:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:40:13 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-05 15:40:13 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:41:16 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-05 15:41:16 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:42:47 INFO CloudantRecommender: [Found, 1000009, records in Cloudant]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:42:47 INFO CloudantRecommender: [Starting train model: , 2017-01-05 15:42:47 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:24 INFO CloudantRecommender: [Finished train model: , 2017-01-05 15:43:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:24 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-05 15:43:24 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:28 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-05 15:43:28 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:29 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1483652608]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:30 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-05 15:43:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:30 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-05 15:43:30 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:31 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-05 15:43:31 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:32 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-05 15:43:32 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:33 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-05 15:43:33 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:34 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-05 15:43:34 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:35 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-05 15:43:35 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-05 15:43:36 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-05 15:43:36 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-05 15:43:36 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-05 15:43:36 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-05 15:43:36 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:36 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-05 15:43:36 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-05 15:43:37 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-05 15:43:37 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-05 15:43:37 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-05 15:43:37 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-05 15:43:37 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:37 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-05 15:43:37 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-05 15:43:38 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-05 15:43:38 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-05 15:43:38 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-05 15:43:38 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-05 15:43:38 CST]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Updated recommendationdb metadata record with latest_db, recommendationdb_1483652608]\r\n",
      "/gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/logs/notebook/kernel-pyspark-20170105_212445.log:17/01/05 15:43:38 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1483652608, 2017-01-05 15:43:38 CST]\r\n"
     ]
    }
   ],
   "source": [
    "# look for our log output in all kernel log files\n",
    "! grep 'CloudantRecommender' $HOME/logs/notebook/*pyspark* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}