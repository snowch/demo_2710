{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your cloudant credentials below, change notebook format to 'Code' and run the cell to save your credentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF > cloudant_credentials.json\n",
    "{\n",
    "  \"username\": \"changeme\",\n",
    "  \"password\": \"changeme\",\n",
    "  \"host\": \"changeme\",\n",
    "  \"port\": 443,\n",
    "  \"url\": \"changeme\"\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, ensure you have installed spark-cloudant 1.6.4 by running the notebook: **Install spark-cloudant 1.6.4 lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python -c 'import cloudant' || pip install cloudant --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility method for timestamps\n",
    "import time\n",
    "def ts():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility method for logging\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(\"CloudantRecommender\")\n",
    "\n",
    "def info(*args):\n",
    "    \n",
    "    # sends output to notebook\n",
    "    print(args)\n",
    "    \n",
    "    # sends output to kernel log file\n",
    "    LOGGER.info(args)\n",
    "    \n",
    "def error(*args):\n",
    "    \n",
    "    # sends output to notebook\n",
    "    print(args)\n",
    "    \n",
    "    # sends output to kernel log file\n",
    "    LOGGER.error(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility class for holding cloudant connection details\n",
    "import json\n",
    "\n",
    "def set_attr_if_exists(obj, data, k):\n",
    "    try:\n",
    "        setattr(obj, k, data[k])\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "class CloudantConfig:\n",
    "    def __init__(self, database, json_file=None, host=None, username=None, password=None):\n",
    "       \n",
    "        self.database = database\n",
    "        self.host = None\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "\n",
    "        with open(json_file) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "            \n",
    "            set_attr_if_exists(self, data, 'host')\n",
    "            set_attr_if_exists(self, data, 'username')\n",
    "            set_attr_if_exists(self, data, 'password')\n",
    "        \n",
    "        # override json attributes if provided\n",
    "        if host:     self.host = host\n",
    "        if username: self.username = username\n",
    "        if password: self.password = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourceDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"ratingdb\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# we use the cloudant python library to save the recommendations\n",
    "from cloudant.client import Cloudant\n",
    "from cloudant.adapters import Replay429Adapter\n",
    "\n",
    "class CloudantMovieRecommender:\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "    \n",
    "    def train(self, sourceDB):\n",
    "                      \n",
    "        info(\"Starting load from Cloudant: \", ts())\n",
    "\n",
    "        dfReader = sqlContext.read.format(\"com.cloudant.spark\")\n",
    "        dfReader.option(\"cloudant.host\", sourceDB.host)\n",
    "        \n",
    "        if sourceDB.username:\n",
    "            dfReader.option(\"cloudant.username\", sourceDB.username)\n",
    "            \n",
    "        if sourceDB.password:\n",
    "            dfReader.option(\"cloudant.password\", sourceDB.password)\n",
    "            \n",
    "        df = dfReader.load(sourceDB.database).cache()\n",
    "\n",
    "        info(\"Finished load from Cloudant: \", ts())\n",
    "        info(\"Found\", df.count(), \"records in Cloudant\")\n",
    "        \n",
    "        # convert cloudant docs into Rating objects\n",
    "        def make_rating(row):\n",
    "            (user_id, prod_id) = row[0].split('/')\n",
    "            user_id = int(user_id.replace('user_', ''))\n",
    "            prod_id = int(prod_id.replace('movie_', ''))\n",
    "\n",
    "            rating = float(row[2])\n",
    "            return Rating(user_id, prod_id, rating)\n",
    "        \n",
    "        ratings = df.map(make_rating)\n",
    "\n",
    "        rank = 50\n",
    "        numIterations = 20\n",
    "        lambdaParam = 0.1\n",
    "\n",
    "        info(\"Starting train model: \", ts())\n",
    "        self.model = ALS.train(ratings, rank, numIterations, lambdaParam)\n",
    "        info(\"Finished train model: \", ts())\n",
    "        \n",
    "    def get_top_recommendations(self):\n",
    "        info(\"Starting __get_top_recommendations: \", ts())\n",
    "        df = self.model.recommendProductsForUsers(10).toDF()\n",
    "        df.cache()\n",
    "        info(\"Finished __get_top_recommendations: \", ts())\n",
    "        return df\n",
    "        \n",
    "    def del_old_recommendationdbs(self, cloudant_client, db_name_prefix):\n",
    "        dbs_to_del = cloudant_client.all_dbs()\n",
    "\n",
    "        # only delete dbs we are using for recommendations\n",
    "        dbs_to_del = [db for db in dbs_to_del if db.startswith(db_name_prefix + '_') ]\n",
    "\n",
    "        # ensure the list is in timestamp order\n",
    "        dbs_to_del.sort()\n",
    "\n",
    "        # keeping the last 5 dbs and delete the rest\n",
    "        for db in dbs_to_del[:-5]:\n",
    "            cloudant_client.delete_database(db)\n",
    "            info(\"Deleted old recommendations db\", db)\n",
    "            \n",
    "    def update_meta_document(self, cloudant_client, meta_db_name, latest_db_name):\n",
    "        \n",
    "        meta_db = cloudant_client[meta_db_name]\n",
    "        \n",
    "        try:\n",
    "            # update doc if exists\n",
    "            meta_doc = meta_db['recommendation_metadata']\n",
    "            meta_doc['latest_db'] = latest_db_name\n",
    "            meta_doc['timestamp'] = ts()\n",
    "            meta_doc.save()\n",
    "            info(\"Updated recommendationdb metadata record with latest_db\", latest_db_name, meta_doc)\n",
    "        except KeyError:\n",
    "            # create a new doc\n",
    "            data = {\n",
    "                '_id': 'recommendation_metadata',\n",
    "                'latest_db': latest_db_name,\n",
    "                'timestamp': ts(),\n",
    "                }\n",
    "            meta_doc = meta_db.create_document(data)\n",
    "            meta_doc.save()\n",
    "            \n",
    "            if meta_doc.exists():\n",
    "                info(\"Saved recommendationdb metadata record\", str(data))\n",
    "                \n",
    "        # get Vt - see: http://stackoverflow.com/questions/41537470/als-model-how-to-generate-full-u-vt-v\n",
    "        pf = self.model.productFeatures().sortByKey()\n",
    "\n",
    "        pf_keys = json.dumps(pf.sortByKey().keys().collect())\n",
    "        pf_vals = json.dumps(pf.sortByKey().map(lambda x: list(x[1])).collect())               \n",
    "                \n",
    "        meta_doc.put_attachment(\n",
    "            attachment='product_feature_keys', \n",
    "            content_type='application/json', \n",
    "            data=pf_keys\n",
    "        )\n",
    "\n",
    "        meta_doc.put_attachment(\n",
    "            attachment='product_feature_vals', \n",
    "            content_type='application/json', \n",
    "            data=pf_vals\n",
    "        )\n",
    "    \n",
    "    def create_recommendationdb(self, cloudant_client):\n",
    "        # create a database for recommendations\n",
    "        import time\n",
    "        db_name = destDB.database + '_' + str(int(time.time()))\n",
    "        \n",
    "        db = cloudant_client.create_database(db_name)\n",
    "        info(\"Created new recommendations db\", db_name)\n",
    "        return db\n",
    "        \n",
    "    def save_recommendations(self, destDB):\n",
    "        df = movieRecommender.get_top_recommendations()\n",
    "        \n",
    "        cloudant_client = Cloudant(\n",
    "                                destDB.username,\n",
    "                                destDB.password,\n",
    "                                account=destDB.username, \n",
    "                                adapter=Replay429Adapter(retries=10, initialBackoff=1)\n",
    "                                )\n",
    "        cloudant_client.connect()\n",
    "        self.del_old_recommendationdbs(cloudant_client, destDB.database)\n",
    "        recommendations_db = self.create_recommendationdb(cloudant_client)\n",
    "\n",
    "        # reformat data for saving\n",
    "        docs = df.map(lambda x: {'_id':str(x[0]), 'recommendations':x[1]}).collect()\n",
    "        \n",
    "        # we could hit cloudant resource limits if trying to save entire doc\n",
    "        # so we save it in smaller sized chunks\n",
    "        \n",
    "        for i in range(0, len(docs), 100):\n",
    "            chunk = docs[i:i + 100]\n",
    "            recommendations_db.bulk_docs(chunk) # TODO check for errors saving the chunk\n",
    "            info(\"Saved recommendations chunk\", i, ts())\n",
    "        \n",
    "        self.update_meta_document(cloudant_client, destDB.database, recommendations_db.database_name)\n",
    "        \n",
    "        info(\"Saved recommendations to: \", recommendations_db.database_name, ts())\n",
    "\n",
    "        cloudant_client.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Starting load from Cloudant: ', '2017-01-11 06:59:57 CST')\n",
      "('Finished load from Cloudant: ', '2017-01-11 07:03:22 CST')\n",
      "('Found', 1000041, 'records in Cloudant')\n",
      "('Starting train model: ', '2017-01-11 07:05:57 CST')\n",
      "('Finished train model: ', '2017-01-11 07:06:33 CST')\n",
      "('Starting __get_top_recommendations: ', '2017-01-11 07:06:33 CST')\n",
      "('Finished __get_top_recommendations: ', '2017-01-11 07:06:38 CST')\n",
      "('Deleted old recommendations db', u'recommendationdb_1484135697')\n",
      "('Created new recommendations db', 'recommendationdb_1484139998')\n",
      "('Saved recommendations chunk', 0, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 100, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 200, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 300, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 400, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 500, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 600, '2017-01-11 07:06:39 CST')\n",
      "('Saved recommendations chunk', 700, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 800, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 900, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 1000, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 1100, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 1200, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 1300, '2017-01-11 07:06:40 CST')\n",
      "('Saved recommendations chunk', 1400, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 1500, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 1600, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 1700, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 1800, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 1900, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 2000, '2017-01-11 07:06:42 CST')\n",
      "('Saved recommendations chunk', 2100, '2017-01-11 07:06:43 CST')\n",
      "('Saved recommendations chunk', 2200, '2017-01-11 07:06:43 CST')\n",
      "('Saved recommendations chunk', 2300, '2017-01-11 07:06:43 CST')\n",
      "('Saved recommendations chunk', 2400, '2017-01-11 07:06:43 CST')\n",
      "('Saved recommendations chunk', 2500, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 2600, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 2700, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 2800, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 2900, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 3000, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 3100, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 3200, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 3300, '2017-01-11 07:06:45 CST')\n",
      "('Saved recommendations chunk', 3400, '2017-01-11 07:06:46 CST')\n",
      "('Saved recommendations chunk', 3500, '2017-01-11 07:06:46 CST')\n",
      "('Saved recommendations chunk', 3600, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 3700, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 3800, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 3900, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4000, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4100, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4200, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4300, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4400, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4500, '2017-01-11 07:06:48 CST')\n",
      "('Saved recommendations chunk', 4600, '2017-01-11 07:06:49 CST')\n",
      "('Saved recommendations chunk', 4700, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 4800, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 4900, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5000, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5100, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5200, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5300, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5400, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5500, '2017-01-11 07:06:51 CST')\n",
      "('Saved recommendations chunk', 5600, '2017-01-11 07:06:52 CST')\n",
      "('Saved recommendations chunk', 5700, '2017-01-11 07:06:52 CST')\n",
      "('Saved recommendations chunk', 5800, '2017-01-11 07:06:52 CST')\n",
      "('Saved recommendations chunk', 5900, '2017-01-11 07:06:52 CST')\n",
      "('Saved recommendations chunk', 6000, '2017-01-11 07:06:52 CST')\n",
      "('Updated recommendationdb metadata record with latest_db', 'recommendationdb_1484139998', {u'timestamp': '2017-01-11 07:06:52 CST', u'_rev': u'197-27f8ee875fe1abfef7581b6e924cdd08', u'_id': u'recommendation_metadata', u'latest_db': 'recommendationdb_1484139998', u'_attachments': {u'vtdata': {u'stub': True, u'length': 1976596, u'digest': u'md5-vE2r9jXIp3BwqwJpZ/IJZg==', u'revpos': 196, u'content_type': u'application/json'}}})\n",
      "('Saved recommendations to: ', 'recommendationdb_1484139998', '2017-01-11 07:06:55 CST')\n"
     ]
    }
   ],
   "source": [
    "sourceDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"ratingdb\"\n",
    "                    )\n",
    "\n",
    "destDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"recommendationdb\", \n",
    "                    )\n",
    "\n",
    "import traceback\n",
    "try:\n",
    "    movieRecommender = CloudantMovieRecommender(sc)\n",
    "    movieRecommender.train(sourceDB)\n",
    "    movieRecommender.save_recommendations(destDB)\n",
    "except Exception as e:\n",
    "    error(str(e), traceback.format_exc(), ts())\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For debugging issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dump the latest kernel log\n",
    "! cat $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look for our log output in the latest kernel log file\n",
    "! grep 'CloudantRecommender' $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look for our log output in all kernel log files\n",
    "! grep 'CloudantRecommender' $HOME/logs/notebook/*pyspark* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls $HOME/logs/notebook/*pyspark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}