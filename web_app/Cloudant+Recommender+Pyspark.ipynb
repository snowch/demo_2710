{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, ensure you have installed spark-cloudant 1.6.4 by running the notebook: **Install spark-cloudant 1.6.4 lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python -c 'import cloudant' || pip install cloudant --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility method for timestamps\n",
    "import time\n",
    "def ts():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility method for logging\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(\"CloudantRecommender\")\n",
    "\n",
    "def info(*args):\n",
    "    \n",
    "    # sends output to notebook\n",
    "    print(args)\n",
    "    \n",
    "    # sends output to kernel log file\n",
    "    LOGGER.info(args)\n",
    "    \n",
    "def error(*args):\n",
    "    \n",
    "    # sends output to notebook\n",
    "    print(args)\n",
    "    \n",
    "    # sends output to kernel log file\n",
    "    LOGGER.error(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility class for holding cloudant connection details\n",
    "import json\n",
    "\n",
    "def set_attr_if_exists(obj, data, k):\n",
    "    try:\n",
    "        setattr(obj, k, data[k])\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "class CloudantConfig:\n",
    "    def __init__(self, database, json_file=None, host=None, username=None, password=None):\n",
    "       \n",
    "        self.database = database\n",
    "        self.host = None\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "\n",
    "        with open(json_file) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "            \n",
    "            set_attr_if_exists(self, data, 'host')\n",
    "            set_attr_if_exists(self, data, 'username')\n",
    "            set_attr_if_exists(self, data, 'password')\n",
    "        \n",
    "        # override json attributes if provided\n",
    "        if host:     self.host = host\n",
    "        if username: self.username = username\n",
    "        if password: self.password = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourceDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"ratingdb\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# see http://stackoverflow.com/a/24375113/1033422\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "\n",
    "    def default(self, obj):\n",
    "        \"\"\"If input object is an ndarray it will be converted into a dict \n",
    "        holding dtype, shape and the data, base64 encoded.\n",
    "        \"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            if obj.flags['C_CONTIGUOUS']:\n",
    "                obj_data = obj.data\n",
    "            else:\n",
    "                cont_obj = np.ascontiguousarray(obj)\n",
    "                assert(cont_obj.flags['C_CONTIGUOUS'])\n",
    "                obj_data = cont_obj.data\n",
    "            data_b64 = base64.b64encode(obj_data)\n",
    "            return dict(__ndarray__=data_b64,\n",
    "                        dtype=str(obj.dtype),\n",
    "                        shape=obj.shape)\n",
    "        # Let the base class default method raise the TypeError\n",
    "        return json.JSONEncoder(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# we use the cloudant python library to save the recommendations\n",
    "from cloudant.client import Cloudant\n",
    "from cloudant.adapters import Replay429Adapter\n",
    "\n",
    "class CloudantMovieRecommender:\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "    \n",
    "    def train(self, sourceDB):\n",
    "                      \n",
    "        info(\"Starting load from Cloudant: \", ts())\n",
    "\n",
    "        dfReader = sqlContext.read.format(\"com.cloudant.spark\")\n",
    "        dfReader.option(\"cloudant.host\", sourceDB.host)\n",
    "        \n",
    "        if sourceDB.username:\n",
    "            dfReader.option(\"cloudant.username\", sourceDB.username)\n",
    "            \n",
    "        if sourceDB.password:\n",
    "            dfReader.option(\"cloudant.password\", sourceDB.password)\n",
    "            \n",
    "        df = dfReader.load(sourceDB.database).cache()\n",
    "\n",
    "        info(\"Finished load from Cloudant: \", ts())\n",
    "        info(\"Found\", df.count(), \"records in Cloudant\")\n",
    "        \n",
    "        # convert cloudant docs into Rating objects\n",
    "        def make_rating(row):\n",
    "            (user_id, prod_id) = row[0].split('/')\n",
    "            user_id = int(user_id.replace('user_', ''))\n",
    "            prod_id = int(prod_id.replace('movie_', ''))\n",
    "\n",
    "            rating = float(row[2])\n",
    "            return Rating(user_id, prod_id, rating)\n",
    "        \n",
    "        ratings = df.map(make_rating)\n",
    "\n",
    "        rank = 50\n",
    "        numIterations = 20\n",
    "        lambdaParam = 0.1\n",
    "\n",
    "        info(\"Starting train model: \", ts())\n",
    "        self.model = ALS.train(ratings, rank, numIterations, lambdaParam)\n",
    "        info(\"Finished train model: \", ts())\n",
    "        \n",
    "    def get_top_recommendations(self):\n",
    "        info(\"Starting __get_top_recommendations: \", ts())\n",
    "        df = self.model.recommendProductsForUsers(10).toDF()\n",
    "        df.cache()\n",
    "        info(\"Finished __get_top_recommendations: \", ts())\n",
    "        return df\n",
    "        \n",
    "    def del_old_recommendationdbs(self, cloudant_client, db_name_prefix):\n",
    "        dbs_to_del = cloudant_client.all_dbs()\n",
    "\n",
    "        # only delete dbs we are using for recommendations\n",
    "        dbs_to_del = [db for db in dbs_to_del if db.startswith(db_name_prefix + '_') ]\n",
    "\n",
    "        # ensure the list is in timestamp order\n",
    "        dbs_to_del.sort()\n",
    "\n",
    "        # keeping the last 5 dbs and delete the rest\n",
    "        for db in dbs_to_del[:-5]:\n",
    "            cloudant_client.delete_database(db)\n",
    "            info(\"Deleted old recommendations db\", db)\n",
    "            \n",
    "    def update_meta_document(self, cloudant_client, meta_db_name, latest_db_name):\n",
    "        \n",
    "        meta_db = cloudant_client[meta_db_name]\n",
    "        \n",
    "        # get Vt - see: http://stackoverflow.com/questions/41537470/als-model-how-to-generate-full-u-vt-v\n",
    "        pf = self.model.productFeatures()\n",
    "        \n",
    "        Vt = np.matrix(np.asarray(pf.values().collect()))\n",
    "        \n",
    "        try:\n",
    "            # update doc if exists\n",
    "            meta_doc = meta_db['recommendation_metadata']\n",
    "            meta_doc['latest_db'] = latest_db_name\n",
    "            meta_doc['timestamp'] = ts()\n",
    "            meta_doc.put_attachment(\n",
    "                attachment='vtdata', \n",
    "                content_type='application/json', \n",
    "                data=json.dumps(Vt, cls=NumpyEncoder)\n",
    "            )\n",
    "            meta_doc.save()\n",
    "            info(\"Updated recommendationdb metadata record with latest_db\", latest_db_name)\n",
    "        except KeyError:\n",
    "            # create a new doc\n",
    "            data = {\n",
    "                '_id': 'recommendation_metadata',\n",
    "                'latest_db': latest_db_name,\n",
    "                'timestamp': ts()\n",
    "                }\n",
    "            meta_doc = meta_db.create_document(data)\n",
    "            if meta_doc.exists():\n",
    "                info(\"Saved recommendationdb metadata record\", str(data))\n",
    "    \n",
    "    def create_recommendationdb(self, cloudant_client):\n",
    "        # create a database for recommendations\n",
    "        import time\n",
    "        db_name = destDB.database + '_' + str(int(time.time()))\n",
    "        \n",
    "        db = cloudant_client.create_database(db_name)\n",
    "        info(\"Created new recommendations db\", db_name)\n",
    "        return db\n",
    "        \n",
    "    def save_recommendations(self, destDB):\n",
    "        df = movieRecommender.get_top_recommendations()\n",
    "        \n",
    "        cloudant_client = Cloudant(\n",
    "                                destDB.username,\n",
    "                                destDB.password,\n",
    "                                account=destDB.username, \n",
    "                                adapter=Replay429Adapter(retries=10)\n",
    "                                )\n",
    "        cloudant_client.connect()\n",
    "        self.del_old_recommendationdbs(cloudant_client, destDB.database)\n",
    "        recommendations_db = self.create_recommendationdb(cloudant_client)\n",
    "\n",
    "        # reformat data for saving\n",
    "        docs = df.map(lambda x: {'_id':str(x[0]), 'recommendations':x[1]}).collect()\n",
    "        \n",
    "        # we could hit cloudant resource limits if trying to save entire doc\n",
    "        # so we save it in smaller sized chunks\n",
    "        \n",
    "        for i in range(0, len(docs), 100):\n",
    "            chunk = docs[i:i + 100]\n",
    "            recommendations_db.bulk_docs(chunk) # TODO check for errors saving the chunk\n",
    "            info(\"Saved recommendations chunk\", i, ts())\n",
    "        \n",
    "        self.update_meta_document(cloudant_client, destDB.database, recommendations_db.database_name)\n",
    "        \n",
    "        info(\"Saved recommendations to: \", recommendations_db.database_name, ts())\n",
    "\n",
    "        cloudant_client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your cloudant credentials below, change notebook format to 'Code' and run the cell to save your credentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF > cloudant_credentials.json\n",
    "{\n",
    "  \"username\": \"changeme\",\n",
    "  \"password\": \"changeme\",\n",
    "  \"host\": \"changeme\",\n",
    "  \"port\": 443,\n",
    "  \"url\": \"changeme\"\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Starting load from Cloudant: ', '2017-01-09 14:37:00 CST')\n",
      "('Finished load from Cloudant: ', '2017-01-09 14:41:04 CST')\n",
      "('Found', 1000037, 'records in Cloudant')\n",
      "('Starting train model: ', '2017-01-09 14:43:14 CST')\n",
      "('Finished train model: ', '2017-01-09 14:43:55 CST')\n",
      "('Starting __get_top_recommendations: ', '2017-01-09 14:43:55 CST')\n",
      "('Finished __get_top_recommendations: ', '2017-01-09 14:44:01 CST')\n",
      "('Deleted old recommendations db', u'recommendationdb_1483988102')\n",
      "('Created new recommendations db', 'recommendationdb_1483994641')\n",
      "('Saved recommendations chunk', 0, '2017-01-09 14:44:03 CST')\n",
      "('Saved recommendations chunk', 100, '2017-01-09 14:44:03 CST')\n",
      "('Saved recommendations chunk', 200, '2017-01-09 14:44:03 CST')\n",
      "('Saved recommendations chunk', 300, '2017-01-09 14:44:03 CST')\n",
      "('Saved recommendations chunk', 400, '2017-01-09 14:44:03 CST')\n",
      "('Saved recommendations chunk', 500, '2017-01-09 14:44:04 CST')\n",
      "('Saved recommendations chunk', 600, '2017-01-09 14:44:04 CST')\n",
      "('Saved recommendations chunk', 700, '2017-01-09 14:44:04 CST')\n",
      "('Saved recommendations chunk', 800, '2017-01-09 14:44:04 CST')\n",
      "('Saved recommendations chunk', 900, '2017-01-09 14:44:04 CST')\n",
      "('Saved recommendations chunk', 1000, '2017-01-09 14:44:04 CST')\n",
      "('Saved recommendations chunk', 1100, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1200, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1300, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1400, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1500, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1600, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1700, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1800, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 1900, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 2000, '2017-01-09 14:44:05 CST')\n",
      "('Saved recommendations chunk', 2100, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2200, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2300, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2400, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2500, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2600, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2700, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2800, '2017-01-09 14:44:06 CST')\n",
      "('Saved recommendations chunk', 2900, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3000, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3100, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3200, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3300, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3400, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3500, '2017-01-09 14:44:07 CST')\n",
      "('Saved recommendations chunk', 3600, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 3700, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 3800, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 3900, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 4000, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 4100, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 4200, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 4300, '2017-01-09 14:44:08 CST')\n",
      "('Saved recommendations chunk', 4400, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 4500, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 4600, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 4700, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 4800, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 4900, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 5000, '2017-01-09 14:44:09 CST')\n",
      "('Saved recommendations chunk', 5100, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5200, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5300, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5400, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5500, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5600, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5700, '2017-01-09 14:44:10 CST')\n",
      "('Saved recommendations chunk', 5800, '2017-01-09 14:44:11 CST')\n",
      "('Saved recommendations chunk', 5900, '2017-01-09 14:44:11 CST')\n",
      "('Saved recommendations chunk', 6000, '2017-01-09 14:44:11 CST')\n",
      "('Updated recommendationdb metadata record with latest_db', 'recommendationdb_1483994641')\n",
      "(\"global name 'vtfile' is not defined\", u'Traceback (most recent call last):\\n  File \"<ipython-input-8-17fbbf75ddb3>\", line 15, in <module>\\n    movieRecommender.save_recommendations(destDB)\\n  File \"<ipython-input-7-b7a039b05723>\", line 140, in save_recommendations\\n    self.update_meta_document(cloudant_client, destDB.database, recommendations_db.database_name)\\n  File \"<ipython-input-7-b7a039b05723>\", line 105, in update_meta_document\\n    vtfile.close()\\nNameError: global name \\'vtfile\\' is not defined\\n', '2017-01-09 14:44:12 CST')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'vtfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-17fbbf75ddb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: global name 'vtfile' is not defined"
     ]
    }
   ],
   "source": [
    "sourceDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"ratingdb\"\n",
    "                    )\n",
    "\n",
    "destDB = CloudantConfig(\n",
    "                    json_file='cloudant_credentials.json', \n",
    "                    database=\"recommendationdb\", \n",
    "                    )\n",
    "\n",
    "import traceback\n",
    "try:\n",
    "    movieRecommender = CloudantMovieRecommender(sc)\n",
    "    movieRecommender.train(sourceDB)\n",
    "    movieRecommender.save_recommendations(destDB)\n",
    "except Exception as e:\n",
    "    error(str(e), traceback.format_exc(), ts())\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For debugging issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump the latest kernel log\n",
    "! cat $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look for our log output in the latest kernel log file\n",
    "! grep 'CloudantRecommender' $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look for our log output in all kernel log files\n",
    "! grep 'CloudantRecommender' $HOME/logs/notebook/*pyspark* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls $HOME/logs/notebook/*pyspark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}