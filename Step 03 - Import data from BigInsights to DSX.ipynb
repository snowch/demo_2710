{"nbformat": 4, "cells": [{"source": "## Overview\n\nThis notebook retrieves movie rating data from BigInsights over webhdfs.  The data was copied to the cluster in the previous step.", "cell_type": "markdown", "metadata": {}}, {"source": "### Read the cluster connection information", "cell_type": "markdown", "metadata": {}}, {"source": "We retrieve the cluster hostname, username and password that were saved to DSX in Step 1.", "cell_type": "markdown", "metadata": {}}, {"source": "with open('credentials', 'r') as f:\n    (hostname, username, password) = f.readline().split(',')", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 1, "outputs": []}, {"source": "The next cell setups up a python object we can use to interact with our cluster. If you are using this notebook with an 'Enterprise' cluster, you will need to uncomment the line as shown.", "cell_type": "markdown", "metadata": {}}, {"source": "!pip install --user --quiet pywebhdfs\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nhdfs = PyWebHdfsClient( \n    base_uri_pattern=\"https://{0}:8443/gateway/default/webhdfs/v1\".format(hostname),\n    request_extra_opts={\n        'auth': (username, password),\n        # 'verify': False, # uncomment this for Enterprise clusters\n    }\n)", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 2, "outputs": []}, {"source": "### Load the movie rating data", "cell_type": "markdown", "metadata": {}}, {"source": "We can now load the rating data from webHDFS and save it onto DSX local file storage.", "cell_type": "markdown", "metadata": {}}, {"source": "First set the path to the file in HDFS", "cell_type": "markdown", "metadata": {}}, {"source": "ratings_path = '//user/{0}/ratings.dat'.format(username)\nprint(ratings_path)", "cell_type": "code", "metadata": {"collapsed": false, "scrolled": true}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "//user/demouser/ratings.dat\n"}]}, {"source": "Now retrieve the file contents into a variable - see NOTE 1 for a discussion on this approach", "cell_type": "markdown", "metadata": {}}, {"source": "ratings_data = hdfs.read_file(ratings_path)", "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": 4, "outputs": []}, {"source": "Save the data to a file on DSX", "cell_type": "markdown", "metadata": {}}, {"source": "with open('ratings.dat', 'w') as f:\n    f.write(ratings_data)", "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": 5, "outputs": []}, {"source": "Let's visually inspect the data to get a 'feel' for it:", "cell_type": "markdown", "metadata": {}}, {"source": "!head -3 ratings.dat\n!echo\n!tail -3 ratings.dat", "cell_type": "code", "metadata": {"collapsed": false}, "execution_count": 6, "outputs": [{"output_type": "stream", "name": "stdout", "text": "1::1193::5::978300760\n1::661::3::978302109\n1::914::3::978301968\n\n6040::562::5::956704746\n6040::1096::4::956715648\n6040::1097::4::956715569\n"}]}, {"source": "Note the format of the dataset: <br/>\n- No header record<br/>\n- The fields are - `UserID::MovieID::Rating::Timestamp`", "cell_type": "markdown", "metadata": {}}, {"source": "---\n## NOTE 1\n\nThe approach used in this notebook is a tactical solution:\n\n- The DSX storage space is limited to a few GB per user.\n- Lab services are open sourcing a spark webHDFS connect which will read webHDFS data directly into a spark dataframe.<br>See https://issues.apache.org/jira/browse/BAHIR-67 for more information.\n- Future offerings of DSX and BigInsights will have tighter integration of DSX and BigInsights spark.<br>See https://datascix.uservoice.com/forums/387207-general/suggestions/16274593-integrate-with-biginsights.\n- We are reading all of the data into memory in the notebook which will not scale.<br>There is a pull request on the pywebhdfs library to fix this: https://github.com/pywebhdfs/pywebhdfs/pull/46.\n- A similar approach could be coded by hand using python's requests library:\n\n```\nchunk_size = 200000000 # Read in 200 Mb chunks\nurl = \"https://{0}:8443/gateway/default/webhdfs/v1/{1}?op=OPEN\".format(host, webhdfs_filepath)\nr = requests.get(url, auth=(username, password), verify=True, allow_redirects=True, stream=True)\n\nchunk_num = 1\nwith open(local_filepath, 'wb') as f:\n    for chunk in r.iter_content(chunk_size):\n        if chunk: # filter out keep-alive new chunks\n           print('{0} writing chunk {1}'.format(datetime.datetime.now(), chunk_num))\n           f.write(chunk)\n           chunk_num = chunk_num + 1\n```", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "", "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}], "metadata": {"language_info": {"file_extension": ".py", "version": "2.7.11", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 2, "name": "ipython"}}, "kernelspec": {"language": "python", "display_name": "Python 2 with Spark 1.6", "name": "python2"}}, "nbformat_minor": 0}