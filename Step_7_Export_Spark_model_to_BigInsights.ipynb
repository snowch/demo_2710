{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 2", "name": "python2"}, "language_info": {"pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}, "name": "python", "version": "2.7.11", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python"}}, "nbformat_minor": 0, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Overview\n\nIn this notebook, we look at how the movie recommendation model that was built on DSX can be exported to BigInsights.\nWe have a lot of flexibility on BigInsights for using the model, e.g.\n\n- from Oozie to make batch predictions\n- from Spark Streaming to make realtime predictions\n\nWe will use ssh and scp python libraries to move data and code between BigInsights and DSX"}, {"metadata": {}, "cell_type": "markdown", "source": "### Setup SSH\n\nRead cluster credentials"}, {"execution_count": 43, "outputs": [], "metadata": {"collapsed": false}, "source": "with open('credentials', 'r') as f:\n    (hostname, username, password) = f.readline().split(',')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Install required ssh and scp libraries"}, {"execution_count": 44, "outputs": [], "metadata": {"collapsed": false}, "source": "!pip install --user --quiet paramiko\n!pip install --user --quiet scp", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "The ssh library will not work by default - we need to patch it"}, {"execution_count": 45, "outputs": [], "metadata": {"collapsed": true}, "source": "def patch_crypto_be_discovery():\n    # Monkey patches cryptography's backend detection.\n    from cryptography.hazmat import backends\n\n    try:\n        from cryptography.hazmat.backends.commoncrypto.backend import backend as be_cc\n    except ImportError:\n        be_cc = None\n\n    try:\n        from cryptography.hazmat.backends.openssl.backend import backend as be_ossl\n    except ImportError:\n        be_ossl = None\n\n    backends._available_backends_list = [ be for be in (be_cc, be_ossl) if be is not None ]\npatch_crypto_be_discovery()", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Setup a utility method to make running ssh commands easier"}, {"execution_count": 46, "outputs": [], "metadata": {"collapsed": false}, "source": "import paramiko\ns = paramiko.SSHClient()\ns.load_system_host_keys()\ns.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\ndef ssh_cmd(command):\n    s.connect(hostname, 22, username, password)\n    # kinit will fail on Basic clusters, but that can be ignored\n    s.exec_command('kinit -k -t {0}.keytab {0}@IBM.COM'.format(username))\n    (stdin, stdout, stderr) = s.exec_command(command)\n    for line in stdout.readlines():\n        print line.rstrip()\n    for line in stderr.readlines():\n        print line.rstrip()\n    s.close()", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Setup a utility method to make scp commands easier"}, {"execution_count": 47, "outputs": [], "metadata": {"collapsed": true}, "source": "from scp import SCPClient\n    \ndef scp_put(filenames):\n    s.connect(hostname, 22, username, password)\n    # kinit will fail on Basic clusters, but that can be ignored\n    s.exec_command('kinit -k -t {0}.keytab {0}@IBM.COM'.format(username))\n    with SCPClient(s.get_transport()) as scp:\n        scp.put(filenames)\n    scp.close()\n        \ndef scp_get(filenames):\n    s.connect(hostname, port, username, password)\n    with SCPClient(s.get_transport()) as scp:\n        scp.get(filenames)", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's verify ssh is working by listing the root folder contents of hdfs"}, {"execution_count": 48, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 9 items\ndrwxrwxr-x   - ams      hdfs             0 2016-10-15 15:27 /amshbase\ndrwxrwxrwx   - yarn     hadoop           0 2016-10-16 19:48 /app-logs\ndrwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /apps\ndrwxr-xr-x   - hdfs     hdfs             0 2016-07-05 07:07 /iop\ndrwxr-xr-x   - mapred   hdfs             0 2016-07-05 07:05 /mapred\ndrwxrwxrwx   - mapred   hadoop           0 2016-07-05 07:05 /mr-history\ndrwx------   - demouser biusers          0 2016-10-15 12:50 /securedir\ndrwxrwxrwx   - hdfs     hdfs             0 2016-10-15 12:49 /tmp\ndrwxr-xr-x   - hdfs     hdfs             0 2016-10-15 12:50 /user\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('hdfs dfs -ls /')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "The next command we are running on DSX.  We create a tar archive containing our model."}, {"execution_count": 93, "outputs": [], "metadata": {"collapsed": true}, "source": "!rm -f recommender_model.tgz\n!tar czf recommender_model.tgz recommender_model/", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "On BigInsights, use delete any models that were copied across to BigInsights on previous runs on the notebook."}, {"execution_count": 108, "outputs": [], "metadata": {"collapsed": false}, "source": "ssh_cmd('rm -rf ./recommender_model.tgz ./recommender_model')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Copy over our new model"}, {"execution_count": 109, "outputs": [], "metadata": {"collapsed": false}, "source": "scp_put('recommender_model.tgz')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Verify that it was copied ok"}, {"execution_count": 110, "outputs": [{"output_type": "stream", "name": "stdout", "text": "-rw-r--r--. 1 demouser biusers 2126955 Oct 17 21:52 ./recommender_model.tgz\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('ls -l ./recommender_model.tgz')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Unzip the model archive"}, {"execution_count": 111, "outputs": [], "metadata": {"collapsed": true}, "source": "ssh_cmd('tar xzf ./recommender_model.tgz')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Verify the unzipped model folders"}, {"execution_count": 112, "outputs": [{"output_type": "stream", "name": "stdout", "text": "./recommender_model\n./recommender_model/data\n./recommender_model/data/user\n./recommender_model/data/user/._SUCCESS.crc\n./recommender_model/data/user/.part-r-00000-22606c73-f35a-4143-993e-dbf47b7c0ce8.gz.parquet.crc\n./recommender_model/data/user/.part-r-00001-22606c73-f35a-4143-993e-dbf47b7c0ce8.gz.parquet.crc\n./recommender_model/data/user/_SUCCESS\n./recommender_model/data/user/_common_metadata\n./recommender_model/data/user/._common_metadata.crc\n./recommender_model/data/user/part-r-00000-22606c73-f35a-4143-993e-dbf47b7c0ce8.gz.parquet\n./recommender_model/data/user/part-r-00001-22606c73-f35a-4143-993e-dbf47b7c0ce8.gz.parquet\n./recommender_model/data/user/_metadata\n./recommender_model/data/user/._metadata.crc\n./recommender_model/data/product\n./recommender_model/data/product/.part-r-00000-8485d197-34ac-4732-8989-78658d33b29d.gz.parquet.crc\n./recommender_model/data/product/._SUCCESS.crc\n./recommender_model/data/product/_SUCCESS\n./recommender_model/data/product/part-r-00000-8485d197-34ac-4732-8989-78658d33b29d.gz.parquet\n./recommender_model/data/product/.part-r-00001-8485d197-34ac-4732-8989-78658d33b29d.gz.parquet.crc\n./recommender_model/data/product/_common_metadata\n./recommender_model/data/product/part-r-00001-8485d197-34ac-4732-8989-78658d33b29d.gz.parquet\n./recommender_model/data/product/._common_metadata.crc\n./recommender_model/data/product/_metadata\n./recommender_model/data/product/._metadata.crc\n./recommender_model/metadata\n./recommender_model/metadata/._SUCCESS.crc\n./recommender_model/metadata/.part-00000.crc\n./recommender_model/metadata/_SUCCESS\n./recommender_model/metadata/part-00000\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('find ./recommender_model')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Remove any models that were copied to BigInsights HDFS on previous runs on the notebook. And then copy the model from the BigInsights local file system to HDFS."}, {"execution_count": 113, "outputs": [], "metadata": {"collapsed": true}, "source": "model_path = 'hdfs:///user/{0}/recommender_model'.format(username)\noutput_path = 'hdfs:///user/{0}/rating'.format(username)", "cell_type": "code"}, {"execution_count": 114, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Deleted hdfs:///user/demouser/recommender_model\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('hdfs dfs -rm -r -skipTrash {0}'.format(model_path))\nssh_cmd('hdfs dfs -copyFromLocal ./recommender_model {0}'.format(model_path))", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Verify the model exists in HDFS"}, {"execution_count": 115, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 2 items\ndrwxr-xr-x   - demouser hdfs          0 2016-10-17 21:52 hdfs:///user/demouser/recommender_model/data\ndrwxr-xr-x   - demouser hdfs          0 2016-10-17 21:52 hdfs:///user/demouser/recommender_model/metadata\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('hdfs dfs -ls {0}'.format(model_path))", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Copy a scala spark class to the cluster for doing the predictions<br/>\nSee https://github.com/snowch/demo_2710/blob/master/scala_predictor/MovieRating.scala"}, {"execution_count": 116, "outputs": [{"output_type": "stream", "name": "stdout", "text": "-rw-r--r--. 1 demouser biusers 2200 Oct 17 21:52 movie-rating.jar\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('rm -f movie-rating.jar')\nssh_cmd('wget -q -O movie-rating.jar https://github.com/snowch/demo_2710/blob/master/scala_predictor/movie-rating_2.10-1.0.jar?raw=true')\nssh_cmd('ls -l movie-rating.jar')", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "The user_id and movie_id we want predictions for"}, {"execution_count": 117, "outputs": [], "metadata": {"collapsed": false}, "source": "user_id = 0\nmovie_id = 500", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Execute the spark class"}, {"execution_count": 118, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Deleted hdfs:///user/demouser/rating\n16/10/17 21:52:24 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n16/10/17 21:52:24 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n16/10/17 21:52:25 INFO TimelineClientImpl: Timeline service address: http://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8188/ws/v1/timeline/\n16/10/17 21:52:26 INFO RMProxy: Connecting to ResourceManager at bi-hadoop-prod-4261.bi.services.us-south.bluemix.net/172.16.115.1:8050\n16/10/17 21:52:26 INFO Client: Requesting a new application from cluster with 1 NodeManagers\n16/10/17 21:52:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n16/10/17 21:52:27 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n16/10/17 21:52:27 INFO Client: Setting up container launch context for our AM\n16/10/17 21:52:27 INFO Client: Setting up the launch environment for our AM container\n16/10/17 21:52:27 INFO Client: Preparing resources for our AM container\n16/10/17 21:52:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8020/iop/apps/4.2.0.0/spark/jars/spark-assembly.jar\n16/10/17 21:52:27 INFO Client: Uploading resource file:/home/demouser/movie-rating.jar -> hdfs://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8020/user/demouser/.sparkStaging/application_1476535729975_0039/movie-rating.jar\n16/10/17 21:52:28 INFO Client: Uploading resource file:/tmp/spark-0f39e5c0-08f9-4844-b58e-5adb99fd507b/__spark_conf__7880493707107160728.zip -> hdfs://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8020/user/demouser/.sparkStaging/application_1476535729975_0039/__spark_conf__7880493707107160728.zip\n16/10/17 21:52:28 WARN Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode\n16/10/17 21:52:28 INFO SecurityManager: Changing view acls to: demouser\n16/10/17 21:52:28 INFO SecurityManager: Changing modify acls to: demouser\n16/10/17 21:52:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(demouser); users with modify permissions: Set(demouser)\n16/10/17 21:52:28 INFO Client: Submitting application 39 to ResourceManager\n16/10/17 21:52:28 INFO YarnClientImpl: Submitted application application_1476535729975_0039\n16/10/17 21:52:29 INFO Client: Application report for application_1476535729975_0039 (state: ACCEPTED)\n16/10/17 21:52:29 INFO Client:\n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1476741148556\n\t final status: UNDEFINED\n\t tracking URL: http://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8088/proxy/application_1476535729975_0039/\n\t user: demouser\n16/10/17 21:52:30 INFO Client: Application report for application_1476535729975_0039 (state: ACCEPTED)\n16/10/17 21:52:31 INFO Client: Application report for application_1476535729975_0039 (state: ACCEPTED)\n16/10/17 21:52:32 INFO Client: Application report for application_1476535729975_0039 (state: ACCEPTED)\n16/10/17 21:52:33 INFO Client: Application report for application_1476535729975_0039 (state: ACCEPTED)\n16/10/17 21:52:34 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:34 INFO Client:\n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.16.115.2\n\t ApplicationMaster RPC port: 0\n\t queue: default\n\t start time: 1476741148556\n\t final status: UNDEFINED\n\t tracking URL: http://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8088/proxy/application_1476535729975_0039/\n\t user: demouser\n16/10/17 21:52:35 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:36 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:37 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:38 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:39 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:40 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:41 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:42 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:43 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:44 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:45 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:46 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:47 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:48 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:49 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:50 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:51 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:52 INFO Client: Application report for application_1476535729975_0039 (state: RUNNING)\n16/10/17 21:52:53 INFO Client: Application report for application_1476535729975_0039 (state: FINISHED)\n16/10/17 21:52:53 INFO Client:\n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.16.115.2\n\t ApplicationMaster RPC port: 0\n\t queue: default\n\t start time: 1476741148556\n\t final status: SUCCEEDED\n\t tracking URL: http://bi-hadoop-prod-4261.bi.services.us-south.bluemix.net:8088/proxy/application_1476535729975_0039/\n\t user: demouser\n16/10/17 21:52:53 INFO ShutdownHookManager: Shutdown hook called\n16/10/17 21:52:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-0f39e5c0-08f9-4844-b58e-5adb99fd507b\n"}], "metadata": {"collapsed": true}, "source": "# ensure the output folder is clean\nssh_cmd('hdfs dfs -rm -f -r -skipTrash {0}'.format(output_path))\nssh_cmd('spark-submit --class \"MovieRating\" --master yarn-cluster ./movie-rating.jar {0} {1} {2} {3}'.format(model_path, user_id, movie_id, output_path))", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Verify we have some ratings"}, {"execution_count": 119, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 3 items\n-rw-r--r--   3 demouser hdfs          0 2016-10-17 21:52 hdfs:///user/demouser/rating/_SUCCESS\n-rw-r--r--   3 demouser hdfs          0 2016-10-17 21:52 hdfs:///user/demouser/rating/part-00000\n-rw-r--r--   3 demouser hdfs         32 2016-10-17 21:52 hdfs:///user/demouser/rating/part-00001\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('hdfs dfs -ls {0}'.format(output_path))", "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's check the predicted rating"}, {"execution_count": 120, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Rating(0,500,5.519096770617339)\n"}], "metadata": {"collapsed": false}, "source": "ssh_cmd('hdfs dfs -cat {0}/*'.format(output_path))", "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "source": "", "cell_type": "code"}], "nbformat": 4}